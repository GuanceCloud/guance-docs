---
title     : 'iLogtail'
summary   : 'iLogtail collects log information'
__int_icon: 'icon/ilogtail'
dashboard :
  - desc  : 'Not available'
    path  : '-'
monitor   :
  - desc  : 'Not available'
    path  : '-'
---

<!-- markdownlint-disable MD025 -->
# `iLogtail`
<!-- markdownlint-enable -->

## Installation and Configuration {#config}

### Enable DataKit Collector

```shell
cd /usr/local/datakit/conf.d/log
cp logstreaming.conf.sample logstreaming.conf
sysemctl restart datakit
```

### `iLogtail` Configuration

#### Download `iLogtail`

```shell
wget https://ilogtail-community-edition.oss-cn-shanghai.aliyuncs.com/latest/ilogtail-latest.linux-amd64.tar.gz
```

#### Create Configuration File
After extraction, create the file `file_sample.yaml` in the directory `ilogtail-1.7.1/user_yaml_config.d`.

This configuration collects logs from the file `/usr/local/df-demo/log-demo/logs/log.log`.

```yaml
enable: true
inputs:
  - Type: file_log              
    LogPath: /usr/local/df-demo/log-demo/logs    # File path
    FilePattern: log.log     # File name   
flushers:
  - Type: flusher_stdout     
    OnlyStdout: true
  - Type: flusher_http
    RemoteURL: "http://localhost:9529/v1/write/logstreaming" 
```

#### Start `iLogtail`

```shell
cd ilogtail-1.7.1
./ilogtail
```

Manually input a log entry

```shell
echo "hello" >>  /usr/local/df-demo/log-demo/logs/log.log
```

At this point, Guance has received the logs pushed by `iLogtail`.

#### `Pipeline` Configuration

The logs used here are generated by a `JAVA` application from the file `/usr/local/df-demo/log-demo/logs/log.log`. Different log formats will result in different content values in the figure, and the `pipeline` configuration should be adjusted accordingly.

The format of logs output by `logback.xml` in `Java` is as follows, where the output content corresponds to the value of `content` in the figure.

```txt
%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{20} - [%method,%line] - [%X{dd.service}][%X{dd.trace_id}][%X{dd.span_id}] - %msg%n
```

Below is the `Pipeline` script that parses the complete log content, primarily extracting `host`, `host_ip`, `trace_id`, etc., from the log.

【`Log`】->【`Text Processing (Pipeline)`】-> Create a new `Pipeline`. Set the log source to "default", enter the parsing rules, and save.

```python
json_data = load_json(_)
contents = json_data["contents"]
tags = json_data["tags"]
content = contents["content"]
grok(content, "%{TIMESTAMP_ISO8601:time}%{SPACE}\\[%{NOTSPACE:thread_name}\\]%{SPACE}%{LOGLEVEL:status}%{SPACE}%{NOTSPACE:class_name}%{SPACE}%{SPACE}-%{SPACE}\\[%{NOTSPACE:method_name},%{NUMBER:line}\\]%{SPACE}-%{SPACE}\\[%{DATA:service_name}\\]\\[%{DATA:trace_id}\\]\\[%{DATA:span_id}\\]%{SPACE}-%{SPACE}%{GREEDYDATA:msg}")
add_key(message, content)
add_key(host_ip, tags["host.ip"])
add_key(host, tags["host.name"])
add_key(filepath, tags["log.file.path"])
```

Start the `JAVA` application to generate logs, which will automatically trigger `iLogtail` log collection.

Enter Guance, and you can see that the logs have been collected, and the `pipeline` is functioning properly.
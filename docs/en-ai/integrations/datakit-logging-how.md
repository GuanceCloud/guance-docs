---
skip: 'not-searchable-on-index-page'
title: 'DataKit Log Processing Overview'
---

This document introduces how DataKit processes logs. In [another document](datakit-logging.md), we explained how DataKit collects logs. These two documents can be read together to gain a comprehensive understanding of the entire log processing pipeline.

Core questions:

- Why is the configuration for log collection so complex?
- How is log data processed?

## Why Is Log Collection Configuration So Complex? {#why}

From [this document](datakit-logging.md), we understand that because logs come from various sources, the configuration methods are also diverse. We will review these here to help you better understand.

During the log collection process, DataKit has two main types of collection methods:

- Active Collection
    - Directly collect [disk file logs](logging.md)
    - Collect logs generated by [containers](container.md)

- Passive Collection
    - Inject logs into DataKit via [HTTP](logstreaming.md), [TCP/UDP](logging.md#socket), and [Websocket](logfwd.md)

In these different forms of log collection, they all need to solve the same core issue: **How does DataKit process these logs next?**

Breaking down this core issue further, it can be subdivided into several sub-questions:

- Determining what the `source` is: All subsequent log processing depends on this field (there is an additional `service` field, but if not specified, its value will be set to the same as the source).
- How to configure the Pipeline: Although not mandatory, it is widely used.
- Additional Tag Configuration: Also not mandatory, but sometimes it has special functions.
- How to split multi-line logs: Need to inform DataKit how the target logs are split into individual logs (by default, DataKit treats each line starting with non-whitespace characters as a new log).
- Whether there are specific ignore policies: Not all data collected by DataKit needs to be processed; under certain conditions, some data can be ignored (even though they meet the collection criteria).
- Other special configurations: Such as filtering color characters, text encoding handling, etc.

Currently, there are several ways to tell DataKit how to process collected logs:

- [Log Collector](logging.md) conf configuration

In the log collector, [configure](logging.md#config) the list of files to collect (or which TCP/UDP port to read log streams from). In the conf, you can configure settings such as source/Pipeline/multi-line splitting/additional tags.

If sending data to DataKit via TCP/UDP, only logging.conf can be used to configure subsequent log processing because protocols like TCP/UDP do not facilitate attaching additional descriptive information; they only transmit simple log stream data.

This form of log collection is the easiest to understand.

- Conf configuration in the [Container Collector](container.md)

Currently, the container collector's conf can only make very basic configurations for logs (based on container/Pod image names) and cannot configure subsequent log processing (such as Pipeline/source settings) because this conf targets **all logs on the current host**. In container environments, logs on a single host are varied, making it impractical to configure them individually here.

- Inform DataKit how to configure log processing through requests

By making HTTP requests to DataKitâ€™s [Log Streaming](logstreaming.md) service and including various request parameters, you can instruct DataKit on how to handle received log data.

- Annotate the objects being collected (e.g., containers/Pods) to inform DataKit how to process their logs

As mentioned earlier, configuring log collection solely through the container collector conf is too coarse-grained for detailed settings. However, you can [annotate containers/Pods](container-log.md#logging-with-annotation-or-label), and DataKit will **actively discover these annotations**, thereby knowing how to process logs from each container/Pod.

### Priority Explanation {#priority}

Generally, annotations on containers/Pods have the highest priority, overriding settings in conf/Env; Env settings have medium priority, overriding conf settings; and conf settings have the lowest priority, which can be overridden by Env or annotation settings at any time.

> Currently, there are no direct environment variables related to log collection/processing, but they may be added in the future.

For example, in container.conf, assume we exclude the image named 'my_test' from log collection:

```toml
container_exclude_log = ["image:my_test*"]
```

In this case, DataKit will not collect logs from all containers or Pods matching this image name. However, if the corresponding Pod has specific annotations:

```yaml
apiVersion: apps/v1
kind: Pod
metadata:
  name: test-app
  annotations:
    datakit/logs: |   # <----------
      [
        {
          "source": "my-testing-app",
          "pipeline": "test.p",
        }
      ]

spec:
   containers:
   - name : mytest
     image: my_test:1.2.3
```

Even though we excluded all images matching `my_test.*` in container.conf, because this Pod has a specific annotation (`datakit/logs`), DataKit will still collect logs from this Pod and can configure settings like Pipelines.

## How Is Log Data Processed? {#how-logging-processed}

In DataKit, log data currently goes through the following stages of processing (listed in order):

- Collection Stage

After reading (receiving) logs from external sources, the collection stage performs basic processing. This includes log segmentation (splitting large blocks of text into multiple independent raw logs), encoding/decoding (uniformly converting to UTF8 encoding), removing interfering color characters, etc.

- Single Log Parsing

If the corresponding log has Pipeline parsing configured, each log entry (including multi-line logs) will go through Pipeline parsing. Pipeline parsing mainly involves two steps:

1. Grok/JSON Parsing: Using Grok/JSON to parse raw logs into structured data.
1. Fine-tuning extracted fields: For example, [completing IP information](../pipeline/use-pipeline/pipeline-built-in-function.md#fn-geoip), [log obfuscation](../pipeline/use-pipeline/pipeline-built-in-function.md#fn-cover), etc.

- Blacklist (Filter)

[Filters are a set of filtering rules](../datakit/datakit-filter.md) that receive structured data and decide whether to discard it based on certain logical judgments. Filters are centrally distributed (DataKit actively pulls them) and take the following form:

``` not-set
{ source = 'datakit' AND bar IN [ 1, 2, 3] }
```

If central configuration includes a log blacklist, suppose out of 100 parsed logs, 10 meet the condition (i.e., source is `datakit`, and the `bar` field value appears in the list); these 10 logs will not be reported to Guance and will be silently discarded. The discard statistics can be viewed in the [DataKit Monitor](../datakit/datakit-monitor.md).

- Reporting to Guance

After going through these steps, log data is finally reported to Guance, where it can be viewed on the log viewing page.

Under normal circumstances, from log generation to visibility on the page, if collection is successful, there is about a 30-second delay. During this period, DataKit itself reports data every 10 seconds at most, and the center also undergoes a series of processing before final storage.

## Further Reading {#more-readings}

- [DataKit Log Collection Overview](datakit-logging.md)
- [How to Debug Pipelines](../pipeline/use-pipeline/pipeline-quick-start.md#debug)
- [Line Protocol Blacklist Filters](../datakit/datakit-filter.md)
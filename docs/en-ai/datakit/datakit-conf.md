# DataKit Main Configuration
---

The DataKit main configuration is used to configure the runtime behavior of DataKit itself.

<!-- markdownlint-disable MD046 -->
=== "Host Deployment"

    The directory is generally located at:
    
    - Linux/Mac: `/usr/local/datakit/conf.d/datakit.conf`
    - Windows: `C:\Program Files\datakit\conf.d\datakit.conf`

=== "Kubernetes"

    When installed via DaemonSet, although this file exists in the corresponding directory, **DataKit does not actually load the configuration here**. These configurations are generated by injecting environment variables in *datakit.yaml* [injected environment variables](datakit-daemonset-deploy.md#using-k8-env). All the configurations below can be found in the Kubernetes deployment documentation with [corresponding environment variable](datakit-daemonset-deploy.md#using-k8-env) configurations.
<!-- markdownlint-enable -->

## DataKit Main Configuration Example {#maincfg-example}

Below is an example of the DataKit main configuration. We can enable various features based on this example (current version 1.66.1):

<!-- markdownlint-disable MD046 -->
??? info "*datakit.conf*"

    ```toml linenums="1"
        
    
    ################################################
    # Global Configurations
    ################################################
    # Default enabled input list.
    default_enabled_inputs = [
      "cpu",
      "disk",
      "diskio",
      "host_processes",
      "hostobject",
      "mem",
      "net",
      "swap",
      "system",
    ]
    
    # enable_pprof: bool
    # If pprof is enabled, we can profile the running DataKit
    enable_pprof = true
    pprof_listen = "localhost:6060" # pprof listen
    
    # protect_mode: bool, default false
    # When protect_mode is enabled, we can set aggressive collection parameters which may cause DataKit
    # to collect data more frequently.
    protect_mode = true
    
    # The user name running DataKit. Generally for audit purposes. Default is root.
    datakit_user = "root"
    
    ################################################
    # ulimit: set max open-files limit(Linux only)
    ################################################
    ulimit = 64000
    
    ################################################
    # point_pool: use point pool for better memory usage
    ################################################
    [point_pool]
      enable = false
      reserved_capacity = 4096
    
    ################################################
    # DCA configure
    ################################################
    [dca]
      # Enable or disable DCA
      enable = false
    
      # DCA websocket server address
      websocket_server = "ws://localhost:8000/ws"
    
    ################################################
    # Upgrader 
    ################################################
    [dk_upgrader]
      # host address
      host = "0.0.0.0"
    
      # port number
      port = 9542 
    
    ################################################
    # Pipeline
    ################################################
    [pipeline]
      # IP database type, support iploc and geolite2
      ipdb_type = "iploc"
    
      # How often to sync remote pipeline
      remote_pull_interval = "1m"
    
      #
      # reftab configures
      #
      # Reftab remote HTTP URL(https/http)
      refer_table_url = ""
    
      # How often reftab syncs the remote
      refer_table_pull_interval = "5m"
    
      # use sqlite to store reftab data to release memory usage
      use_sqlite = false
      # or use pure memory to cache the reftab data
      sqlite_mem_mode = false
    
      # append run info
      disable_append_run_info = false
    
      # default pipeline
      [pipeline.default_pipeline]
        # logging = "<your_script.p>"
        # metric  = "<your_script.p>"
        # tracing = "<your_script.p>"
    
      # Offload data processing tasks to post-level data processors.
      [pipeline.offload]
        receiver = "datakit-http"
        addresses = [
          # "http://<ip>:<port>"
        ]
    
    ################################################
    # HTTP server(9529)
    ################################################
    [http_api]
    
      # HTTP server address
      listen = "localhost:9529"
    
      # Disable 404 page to hide detailed DataKit info
      disable_404page = false
    
      # only enable these APIs. If list empty, all APIs are enabled.
      public_apis = []
    
      # DataKit server-side timeout
      timeout = "30s"
      close_idle_connection = false
    
      # API rate limit(QPS)
      request_rate_limit = 20.0
    
      #
      # RUM related: we should port these configurations to RUM inputs(TODO)
      #
      # When serving RUM(/v1/write/rum), extract the IP address from this HTTP header
      rum_origin_ip_header = "X-Forwarded-For"
      # When serving RUM(/v1/write/rum), only accept requests from these app-id.
      # If the list is empty, all app's requests are accepted.
      rum_app_id_white_list = []
    
      # only these domains enable CORS. If list empty, all domains are enabled.
      allowed_cors_origins = []
    
      # Start DataKit web server with HTTPS
      [http_api.tls]
        # cert = "path/to/certificate/file"
        # privkey = "path/to/private_key/file"
    
    ################################################
    # io configurations
    ################################################
    [io]
      # How often DataKit flushes data to dataway.
      # DataKit will upload data points if cached(in memory) points
      #  reached(>=) the max_cache_count or the flush_interval triggered.
      max_cache_count = 1000
      flush_workers   = 0 # default to (cpu_core * 2)
      flush_interval  = "10s"
    
      # Queue size of feed.
      feed_chan_size = 1
    
      # Set blocking if queue is full.
      # NOTE: Global blocking mode may consume more memory on large metric points.
      global_blocking = false
    
      # Data point filter configurations.
      # NOTE: Most of the time, you should use web-side filter, it's a debug helper for developers.
      #[io.filters]
      #  logging = [
      #   "{ source = 'datakit' or f1 IN [ 1, 2, 3] }"
      #  ]
      #  metric = [
      #    "{ measurement IN ['datakit', 'disk'] }",
      #    "{ measurement CONTAIN ['host.*', 'swap'] }",
      #  ]
      #  object = [
      #    { class CONTAIN ['host_.*'] }",
      #  ]
      #  tracing = [
      #    "{ service = re("abc.*") AND some_tag CONTAIN ['def_.*'] }",
      #  ]
    
    [recorder]
      enabled = false
      #path = "/path/to/point-data/dir"
      encoding = "v2"  # use protobuf-json format
      duration = "30m" # record for 30 minutes
    
      # only record these inputs, if empty, record all
      inputs = [
        #"cpu",
        #"mem",
      ]
    
      # only record these categories, if empty, record all
      category = [
        #"logging",
        #"object",
      ]
    
    ################################################
    # Dataway configuration
    ################################################
    [dataway]
      # urls: Dataway URL list
      # NOTE: do not configure multiple URLs here, it's a deprecated feature.
      urls = [
        # "https://openway.guance.com?token=<YOUR-WORKSPACE-TOKEN>"
      ]
    
      # Dataway HTTP timeout
      timeout_v2 = "30s"
    
      # max_retry_count specifies at most how many times will be tried when dataway API fails(not 4xx),
      # default value(and minimal) is 1 and maximum value is 10.
      #
      # The default is set to 1 to make the API fail ASAP to release memory.
      max_retry_count = 1
    
      # The interval between two retry operations, valid time units are "ns", "us", "ms", "s", "m", "h"
      retry_delay = "1s"
    
      # HTTP Proxy
      # Format: "http(s)://IP:Port"
      http_proxy = ""
    
      max_idle_conns   = 0       # limit idle TCP connections for HTTP request to Dataway
      enable_httptrace = false   # enable trace HTTP metrics(connection/NDS/TLS and so on)
      idle_timeout     = "90s"   # not-set, default 90s
    
      # HTTP body content type, other candidates are(case insensitive):
      #  - v1: line-protocol
      #  - v2: protobuf
      content_encoding = "v2"
    
      # Enable GZip to upload point data.
      #
      # do NOT disable gzip or you will get large network payload.
      gzip = true
    
      max_raw_body_size = 1048576 # max body size(before gizp) in bytes
    
      # Customer tag or field keys that will be extracted from existing points
      # to build the X-Global-Tags HTTP header value.
      global_customer_keys = []
      enable_sinker        = false # disable sinker
    
      # use dataway as NTP server
      [dataway.ntp]
        interval = "5m"  # sync dataway time each 5min
    
        # if datakit local time and dataway time's ABS value reaches the diff,
        # datakit's soft time will update to the dataway time.
        # NOTE: diff MUST be larger than "1s"
        diff     = "30s" 
    
      # WAL queue for uploading points
      [dataway.wal]
        max_capacity_gb = 2.0 # 2GB reserved disk space for each category(M/L/O/T/...)
        #workers = 4          # flush workers on WAL(default to CPU limited cores)
        #mem_cap = 4          # in-memory queue capacity(default to CPU limited cores)
        #fail_cache_clean_interval = "30s" # duration for clean fail uploaded data
    
    
    ################################################
    # DataKit logging configuration
    ################################################
    [logging]
    
      # log path
      log = "/var/log/datakit/log"
    
      # HTTP access log
      gin_log = "/var/log/datakit/gin.log"
    
      # level level(info/debug)
      level = "info"
    
      # Disable log color
      disable_color = false
    
      # log rotate size(in MB)
      # DataKit will always keep at most n+1(n backup log and 1 writing log) splited log files on disk.
      rotate = 32
    
      # Upper limit count of backup log
      rotate_backups = 5
    
    ################################################
    # Global tags
    ################################################
    # We will try to add these tags to every collected data point if these
    # tags do not exist in original data.
    #
    # NOTE: we can get the real IP of current node, we just need
    # to set "$datakit_ip" or "__datakit_ip" here. Same for the hostname.
    [global_host_tags]
      ip   = "$datakit_ip"
      host = "$datakit_hostname"
    
    [election]
      # Enable election
      enable = false
    
      # Election whitelist
      # NOTE: Empty to disable whitelist
      node_whitelist = []
    
      # Election namespace.
      # NOTE: for single workspace, there can be multiple election namespaces.
      namespace = "default"
      
      # If enabled, every data point will add a tag with election_namespace = <your-election-namespace>
      enable_namespace_tag = false
    
      # Like global_host_tags, but only for data points that are remotely collected(such as MySQL/Nginx).
      [election.tags]
        #  project = "my-project"
        #  cluster = "my-cluster"
    
    ###################################################
    # Tricky: we can rename the default hostname here
    ###################################################
    [environments]
      ENV_HOSTNAME = ""
    
    ################################################
    # resource limit configurations
    ################################################
    [resource_limit]
    
      # enable or disable resource limit
      enable = true
    
      # Linux only, cgroup path
      path = "/datakit"
    
      # set max CPU usage(%, max 100.0, no matter how many CPU cores here)
      cpu_max = 30.0
    
      # set max memory usage(MB)
      mem_max_mb = 4096
    
    ################################################
    # git_repos configurations
    ################################################
    
    # We can host all input configurations on a git server
    [git_repos]
      # git pull interval
      pull_interval = "1m"
    
      # git repository settings
      [[git_repos.repo]]
        # enable the repository or not
        enable = false
    
        # the branch name to pull
        branch = "master"
    
        # git repository URL. There are 3 formats here:
        #   - HTTP(s): such as "https://github.datakit.com/path/to/datakit-conf.git"
        #   - Git: such as "git@github.com:path/to/datakit.git"
        #   - SSH: such as "ssh://git@github.com:9000/path/to/repository.git"
        url = ""
    
        # For formats Git and SSH, we need extra configurations:
        ssh_private_key_path = ""
        ssh_private_key_password = ""
    
    ################################################
    # crypto key or key filePath.
    ################################################
    [crypto]
      aes_key = ""
      aes_Key_file = ""
    
    [remote_job]
      enable=false
      envs = ["OSS_BUCKET_HOST=host","OSS_ACCESS_KEY_ID=key","OSS_ACCESS_KEY_SECRET=secret","OSS_BUCKET_NAME=bucket"]
      interval = "30s"
      java_home=""
    
    ```
<!-- markdownlint-enable -->

## HTTP Service Configuration {#config-http-server}

DataKit starts an HTTP service to receive external data or provide basic data services.

<!-- markdownlint-disable MD046 -->
=== "*datakit.conf*"

    ### Modify HTTP Service Address {#update-http-server-host}
    
    The default HTTP service address is `localhost:9529`. If port 9529 is occupied or you want to access DataKit's HTTP service externally (for example, to receive [RUM](../integrations/rum.md) or [Tracing](../integrations/datakit-tracing.md) data), you can change it to:
    
    ```toml
    [http_api]
       listen = "0.0.0.0:<other-port>"
       # Or use IPv6 address
       # listen = "[::]:<other-port>"
    ```

    Note, IPv6 support requires [DataKit upgrade to 1.5.7](changelog.md#cl-1.5.7-new).

    #### Use Unix Domain Socket {#uds}

    DataKit supports UNIX domain sockets access. To enable it, configure the `listen` field to **a non-existent file's full path**, using `datakit.sock` as an example, which can be any filename.
    ```toml
    [http_api]
       listen = "/tmp/datakit.sock"
    ```
    After configuring, you can test if the configuration is successful using the `curl` command: `sudo curl --no-buffer -XGET --unix-socket /tmp/datakit.sock http:/localhost/v1/ping`. More information about `curl` test commands can be found [here](https://superuser.com/a/925610){:target="_blank"}.
    
    ### HTTP Request Frequency Control {#set-http-api-limit}

    > [:octicons-tag-24: Version-1.62.0](changelog.md#cl-1.62.0) has already enabled this feature by default.
    
    Since DataKit needs to receive a large amount of external data writes, to avoid causing significant overhead on the node, you can modify the following HTTP configuration (disabled by default):
    
    ```toml
    [http_api]
      request_rate_limit = 20.0 # Limit the QPS of requests initiated by each client (IP + API route)
    ```

    ### Other Settings {#http-other-settings}

    ```toml
    [http_api]
        close_idle_connection = true # Close idle connections
        timeout = "30s"              # Set server-side HTTP timeout
    ```

=== "Kubernetes"

    Refer to [here](datakit-daemonset-deploy.md#env-http-api)
<!-- markdownlint-enable -->


### HTTP API Access Control {#public-apis}

[:octicons-tag-24: Version-1.64.0](changelog.md#cl-1.64.0)

For security reasons, DataKit restricts access to some of its own APIs by default (these APIs can only be accessed via localhost). If DataKit is deployed in a public network environment and needs to request these APIs through the public network (or from another machine within the local LAN), you can modify the following `public_apis` fields in *datakit.conf*:

```toml
[http_api]
  public_apis = [
    # Allow access to the /metrics interface
    "/metrics",
  ]
```

By default, only the Ping interface and basic data upload interfaces are enabled, while all other interfaces are disabled for external access. For collectors like trace collectors, once they are enabled, they can be accessed externally by default. For adding API whitelists in Kubernetes, refer to [here](datakit-daemonset-deploy.md#env-http-api).

## Global Tag Modification {#set-global-tag}

[:octicons-tag-24: Version-1.4.6](changelog.md#cl-1.4.6)

DataKit allows configuring global tags for all collected data, which are divided into two categories:

- Host-based Global Tags (GHT): Collected data is bound to the current host, such as CPU/memory metrics
- Election-based Global Tags (GET): Data comes from a common (remote) entity, such as MySQL/Redis, and these collections generally participate in elections, so these data do not carry host-related tags

```toml
[global_host_tags] # These are referred to as "Global Host Tags"
  ip   = "__datakit_ip"
  host = "__datakit_hostname"

[election]
  [election.tags] # These are referred to as "Global Election Tags"
    project = "my-project"
    cluster = "my-cluster"
```

When adding global tags, several points need attention:

1. These global tags can use a few wildcards supported by DataKit (both double underscores (`__`) prefix and `$` are acceptable):

    1. `__datakit_ip/$datakit_ip`: The tag value will be set to the first primary NIC IP obtained by DataKit.
    1. `__datakit_hostname/$datakit_hostname`: The tag value will be set to DataKit's hostname.

1. Due to [DataKit data transmission protocol limitations](apis.md#lineproto-limitation), do not include any metric (Field) fields in the global tags (Tag), otherwise, it may lead to data processing failure due to protocol violations. Refer to the specific collector's field list. Also, don't add too many tags, and both the Key and Value lengths of each tag have limits.
1. If the collected data already contains tags with the same name, DataKit will not append the configured global tags again.
1. Even if there are no configurations in GHT, DataKit will still add a `host=__datakit_hostname` tag because hostname is currently the default field for data correlation on the Guance platform. Logs/CPU/memory, etc., will all carry the `host` tag.
1. These two types of global tags (GHT/GET) can overlap, for example, both can set a `project = "my-project"` tag.
1. When elections are not enabled, GET inherits all tags from GHT (it has at least one `host` tag).
1. Election-based collectors default to appending GET, while non-election-based collectors default to appending GHT.

<!-- markdownlint-disable MD046 -->
???+ tip "How to distinguish between election and non-election collectors?"

    In the collector documentation, there are similar identifiers at the top, indicating the current collector's platform compatibility and collection characteristics:

    :fontawesome-brands-linux: :fontawesome-brands-windows: :fontawesome-brands-apple: :material-kubernetes: :material-docker:  · :fontawesome-solid-flag-checkered:

    If it has :fontawesome-solid-flag-checkered:, it means the current collector is an election-based collector.
<!-- markdownlint-enable -->

### Setting Global Tags for Remote Collection {#notice-global-tags}

Since DataKit defaults to appending the tag `host=<hostname of the DataKit host>` to all collected data, this default addition can sometimes be problematic.

Taking MySQL as an example, if MySQL is not on the same machine as DataKit but you hope this `host` tag reflects the actual hostname of the MySQL host (or another identifier for cloud databases) rather than the hostname of the DataKit host.

For this situation, there are two ways to bypass the global tags added by DataKit:

- In the specific collector configuration, there is usually a configuration like the following where you can add new tags. For example, if you do not want DataKit to add the tag `host=xxx`, you can override this tag here, taking MySQL as an example:

```toml
[[inputs.mysql.tags]]
  host = "real-mysql-host-name" 
```

- When pushing data to DataKit via [HTTP API](apis.md#api-v1-write), you can use the API parameter `ignore_global_tags` to ignore all global tags.

<!-- markdownlint-disable MD046 -->
???+ tip

    Since [1.4.20](changelog.md#cl-1.4.20), DataKit defaults to using the IP/host from the connection address of the collected service as the value of the `host` tag.
<!-- markdownlint-enable -->

## DataKit Runtime Log Configuration {#logging-config}

DataKit has two logs: one is the runtime log (*/var/log/datakit/log*), and the other is the HTTP Access log (*/var/log/datakit/gin.log*).

The default log level for DataKit is `info`. Editing `datakit.conf` allows modification of the log level and rotation size:

```toml
[logging]
  level = "debug" # Change info to debug
  rotate = 32     # Each log fragment is 32MB
```

- `level`: Setting it to `debug` allows seeing more logs (currently only supports `debug/info` levels).
- `rotate`: DataKit defaults to splitting logs into fragments, with each fragment being 32MB by default, totaling 6 fragments (1 current writing fragment plus 5 rotated fragments, the number of fragments is not configurable yet). If you find DataKit logs occupy too much disk space (up to 32 x 6 = 192MB), you can reduce the `rotate` size (e.g., change to 4, unit is MB). HTTP access logs also automatically split similarly.

## Advanced Configuration {#advance-config}

The following content involves advanced configurations. If you are not sure about the configuration, it is recommended to consult our technical experts.

### Point Cache {#point-pool}

[:octicons-tag-24: Version-1.28.0](changelog.md#cl-1.28.0)

> Point caching currently has additional performance issues and is not recommended.

To optimize DataKit's memory usage under high load, you can enable Point Pool to alleviate the issue:

```toml
# datakit.conf
[point_pool]
    enable = true
    reserved_capacity = 4096
```

At the same time, in the [DataKit configuration](datakit-conf.md#dataway-settings), you can enable `content_encoding = "v2"` transmission encoding ([:octicons-tag-24: Version-1.32.0](changelog.md#cl-1.32.0) has already enabled v2 by default), which has lower memory and CPU overhead compared to v1.

<!-- markdownlint-disable MD046 -->
???+ attention

    - Under low load (DataKit memory usage around 100MB), enabling point pool increases DataKit's memory usage. High load generally refers to scenarios where memory usage exceeds 2GB+. Enabling this can also improve DataKit's CPU consumption.

<!-- markdownlint-enable -->

### IO Module Tuning {#io-tuning}

[:octicons-tag-24: Version-1.4.8](changelog.md#cl-1.4.8) ·
[:octicons-beaker-24: Experimental](index.md#experimental)

<!-- markdownlint-disable MD046 -->
=== "*datakit.conf*"

    In some cases, the amount of data collected by DataKit on a single machine is very large. If the network bandwidth is limited, it may lead to interruptions or discarding of some data collection. You can adjust some parameters of the io module to mitigate this problem:

    ```toml
    [io]
      feed_chan_size  = 1     # Length of the data processing queue
      max_cache_count = 1000  # Threshold for batch sending data points, exceeding this value triggers sending
      flush_interval  = "10s" # Interval threshold for data sending, sends at least once every 10 seconds
      flush_workers   = 0     # Number of data upload workers (defaults to CPU cores * 2)
    ```

    Blocking mode reference [corresponding description in k8s](datakit-daemonset-deploy.md#env-io)

=== "Kubernetes"

    Refer to [here](datakit-daemonset-deploy.md#env-io)
<!-- markdownlint-enable -->

### Resource Limitation {#resource-limit}

Since the amount of data processed by DataKit cannot be estimated, if the resources consumed by DataKit are not physically restricted, it may consume a large amount of resources on the node. Here we can use Linux's cgroup and Windows's job object to limit it, and in *datakit.conf*, there are the following configurations:

```toml
[resource_limit]
  path = "/datakit" # Linux cgroup restriction directory, such as /sys/fs/cgroup/memory/datakit, /sys/fs/cgroup/cpu/datakit

  # Maximum allowed CPU usage (percentage)
  cpu_max = 20.0

  # Default allows 4GB of memory (memory + swap) usage
  # If set to 0 or negative, memory limitation is not enabled
  mem_max_mb = 4096 
```

If DataKit exceeds the memory limit, it will be forcibly killed by the operating system. You can see the following results through the command, and at this point, you need to [manually start the service](datakit-service-how-to.md#when-service-failed):

```shell
$ systemctl status datakit 
● datakit.service - Collects data and uploads it to DataFlux.
     Loaded: loaded (/etc/systemd/system/datakit.service; enabled; vendor preset: enabled)
     Active: activating (auto-restart) (Result: signal) since Fri 2022-02-30 16:39:25 CST; 1min 40s ago
    Process: 3474282 ExecStart=/usr/local/datakit/datakit (code=killed, signal=KILL)
   Main PID: 3474282 (code=killed, signal=KILL)
```

<!-- markdownlint-disable MD046 -->
???+ attention

    - Resource limitation is only enabled by default for [host installation](datakit-install.md).
    - Only supports CPU usage rate and memory usage control (memory + swap), and only supports Linux and Windows ([:octicons-tag-24: Version-1.15.0](changelog.md#cl-1.15.0)) operating systems.
    - CPU usage rate control is not supported on these Windows operating systems: Windows 7, Windows Server 2008 R2, Windows Server 2008, Windows Vista, Windows Server 2003, and Windows XP.
    - Non-root users changing resource limitation configurations must reinstall the service.
    - CPU core limit affects the worker count configuration of some sub-modules of DataKit (generally an integer multiple of CPU cores). For example, the number of data upload workers is CPU cores * 2. A single upload worker uses 10MB of memory by default for data sending, so if the number of CPU cores is opened up more, it will affect the overall memory usage of DataKit.

???+ tip

    DataKit has supported cgroup v2 since [1.5.8](changelog.md#cl-1.5.8). If unsure about the cgroup version, you can confirm it using the command `mount | grep cgroup`.
<!-- markdownlint-enable -->

### Election Configuration {#election}

Refer to [here](election.md#config)

### DataWay Parameter Configuration {#dataway-settings}

DataWay has the following configurations that can be modified, others are not recommended:

- `timeout`: Upload timeout to Guance, default is 30s
- `max_retry_count`: Set the retry count for DataWay sends (default 1 time, max 10 times) [:octicons-tag-24: Version-1.17.0](changelog.md#cl-1.17.0)
- `retry_delay`: Set the base step length for retry intervals, default is 200ms. The base step length means the first retry is 200ms, the second is 400ms, the third is 800ms, and so on (increasing exponentially $2^n$) [:octicons-tag-24: Version-1.17.0](changelog.md#cl-1.17.0)
- `max_raw_body_size`: Control the maximum size of a single upload package (before compression), in bytes [:octicons-tag-24: Version-1.17.1](changelog.md#cl-1.17.1)
- `content_encoding`: Choose between v1 or v2 [:octicons-tag-24: Version-1.17.1](changelog.md#cl-1.17.1)
    - v1 is line protocol (default v1)
    - v2 is Protobuf protocol, which performs better in all aspects compared to v1. After stable operation, v2 will be the default.

Configuration for deploying under Kubernetes, refer to [here](datakit-daemonset-deploy.md#env-dataway).

#### WAL Queue Configuration {#dataway-wal}

[:octicons-tag-24: Version-1.60.0](changelog.md#cl-1.60.0)

In `[dataway.wal]`, we can adjust the WAL queue configuration:

```toml
  [dataway.wal]
     max_capacity_gb = 2.0             # Reserve 2GB disk space for each category(M/L/O/T/...)
     workers = 0                       # Number of flush workers on WAL (defaults to CPU-limited cores)
     mem_cap = 0                       # In-memory queue capacity (defaults to CPU-limited cores)
     fail_cache_clean_interval = "30s" # Duration for cleaning failed uploaded data
```

Disk files are located in the *cache/dw-wal* directory under the DataKit installation directory:

```shell
/usr/local/datakit/cache/dw-wal/
├── custom_object
│   └── data
├── dialtesting
│   └── data
├── dynamic_dw
│   └── data
├── fc
│   └── data
├── keyevent
│   └── data
├── logging
│   ├── data
│   └── data.00000000000000000000000000000000
├── metric
│   └── data
├── network
│   └── data
├── object
│   └── data
├── profiling
│   └── data
├── rum
│   └── data
├── security
│   └── data
└── tracing
    └── data

13 directories, 14 files
```

Here, except for *fc* which is the failed retransmission queue, other directories correspond to different data types. When data upload fails, these data are cached in the *fc* directory, and DataKit will periodically upload them.

### Sinker Configuration {#dataway-sink}

Refer to [here](../deployment/dataway-sink.md)

### Using Git to Manage DataKit Configuration {#using-gitrepo}

Refer to [here](git-config-how-to.md)

### Local Setting of Default Pipeline Script {#pipeline-settings}

[:octicons-tag-24: Version-1.61.0](changelog.md#cl-1.61.0)

Supports setting the default Pipeline script locally. If it conflicts with the default script set remotely, the local setting takes precedence.

It can be configured in two ways:

- Host deployment, specify the default scripts for various categories in the main DataKit configuration file, as follows:

    ```toml
    # default pipeline
    [pipeline.default_pipeline]
        # logging = "<your_script.p>"
        # metric  = "<your_script.p>"
        # tracing = "<your_script.p>"
    ```

- Container deployment, use the environment variable `ENV_PIPELINE_DEFAULT_PIPELINE`, its value for example `{"logging":"abc.p","metric":"xyz.p"}`

### Setting the Maximum Value of Open File Descriptors {#enable-max-fd}

Under the Linux environment, you can configure the `ulimit` item in the main DataKit configuration file to set the maximum number of files DataKit can open, as follows:

```toml
ulimit = 64000
```

The default `ulimit` is set to 64000. In Kubernetes, you can set it via [setting `ENV_ULIMIT`](datakit-daemonset-deploy.md#env-others).

### :material-chat-question: Explanation of CPU Usage Rate in Resource Limitation {#cgroup-how}

CPU usage rate is percentage-based (maximum value 100.0). Taking an 8-core CPU as an example, if the limit `cpu_max` is set to 20.0 (i.e., 20%), then the maximum CPU consumption of DataKit will show approximately 160% in the top command.

### Collector Password Protection {#secrets_management}

[:octicons-tag-24: Version-1.31.0](changelog.md#cl-1.31.0)


If you wish to avoid storing passwords in plain text in configuration files, you can use this feature.

When DataKit loads collector configuration files at startup and encounters `ENC[]`, it replaces the text with the correct password after reading the file, environment variable, or AES decryption.

ENC currently supports three methods:

- File form (recommended):

    The password format in the configuration file is: ENC[file:///path/to/enc4dk]. Simply place the correct password in the corresponding file.

- AES encryption method.

    You need to configure the key in the main configuration file `datakit.conf`: `crypto_AES_key` or `crypto_AES_Key_filePath`, the key length is 16 bits.
    The format for filling in the password is: `ENC[aes://5w1UiRjWuVk53k96WfqEaGUYJ/Oje7zr8xmBeGa3ugI=]`

Next, let's illustrate how to configure and use these two methods using `mysql` as an example:

1 File form

First, place the plaintext password in the file `/usr/local/datakit/enc4mysql`, then modify the configuration file `mysql.conf`:

```toml
# Partial configuration
[[inputs.mysql]]
  host = "localhost"
  user = "datakit"
  pass = "ENC[file:///usr/local/datakit/enc4mysql]"
  port = 3306
  # sock = "<SOCK>"
  # charset = "utf8"
```

DK will read the password from `/usr/local/datakit/enc4mysql` and replace it, resulting in `pass = "Hello*******"`

2 AES encryption method

First, configure the key in `datakit.conf`:

```toml
# crypto key or key filePath.
[crypto]
  # Configure the key
  aes_key = "0123456789abcdef"
  # Or, place the key in a file and configure the file location here.
  aes_Key_file = "/usr/local/datakit/mykey"
```

`mysql.conf` configuration file:

```toml
pass = "ENC[aes://5w1UiRjWuVk53k96WfqEaGUYJ/Oje7zr8xmBeGa3ugI=]"
```

Note, the ciphertext obtained through AES encryption needs to be fully filled in. Below is a code example:
<!-- markdownlint-disable MD046 -->
=== "Golang"

    ```go
    // AESEncrypt encrypts.
    func AESEncrypt(key []byte, plaintext string) (string, error) {
        block, err := aes.NewCipher(key)
        if err != nil {
            return "", err
        }
    
        // PKCS7 padding
        padding := aes.BlockSize - len(plaintext)%aes.BlockSize
        padtext := bytes.Repeat([]byte{byte(padding)}, padding)
        plaintext += string(padtext)
        ciphertext := make([]byte, aes.BlockSize+len(plaintext))
        iv := ciphertext[:aes.BlockSize
# DataKit Main Configuration
---

DataKit main configuration is used to configure the runtime behavior of DataKit itself.

<!-- markdownlint-disable MD046 -->
=== "Host Deployment"

    The directory is usually located at:
    
    - Linux/Mac: `/usr/local/datakit/conf.d/datakit.conf`
    - Windows: `C:\Program Files\datakit\conf.d\datakit.conf`

=== "Kubernetes"

    When installed via DaemonSet, although this file exists in the corresponding directory, **DataKit does not actually load the configuration here**. These configurations are generated by injecting environment variables into *datakit.yaml* as described [here](datakit-daemonset-deploy.md#using-k8-env). All the configurations below can be found in the Kubernetes deployment documentation under [corresponding environment variables](datakit-daemonset-deploy.md#using-k8-env).
<!-- markdownlint-enable -->

## DataKit Main Configuration Example {#maincfg-example}

An example of the DataKit main configuration is provided below. We can enable various features based on this example (current version 1.68.1):

<!-- markdownlint-disable MD046 -->
??? info "*datakit.conf*"

    ```toml linenums="1"
        
    
    ################################################
    # Global Configurations
    ################################################
    # Default enabled input list.
    default_enabled_inputs = [
      "cpu",
      "disk",
      "diskio",
      "host_processes",
      "hostobject",
      "mem",
      "net",
      "swap",
      "system",
    ]
    
    # enable_pprof: bool
    # If pprof is enabled, we can profile the running datakit
    enable_pprof = true
    pprof_listen = "localhost:6060" # pprof listen
    
    # protect_mode: bool, default false
    # When protect_mode is enabled, we can set aggressive collection parameters, which may cause Datakit
    # to collect data more frequently.
    protect_mode = true
    
    # The user name running datakit. Generally for audit purposes. Default is root.
    datakit_user = "root"
    
    ################################################
    # ulimit: set max open-files limit (Linux only)
    ################################################
    ulimit = 64000
    
    ################################################
    # point_pool: use point pool for better memory usage
    ################################################
    [point_pool]
      enable = false
      reserved_capacity = 4096
    
    ################################################
    # DCA configure
    ################################################
    [dca]
      # Enable or disable DCA
      enable = false
    
      # DCA websocket server address
      websocket_server = "ws://localhost:8000/ws"
    
    ################################################
    # Upgrader 
    ################################################
    [dk_upgrader]
      # host address
      host = "0.0.0.0"
    
      # port number
      port = 9542 
    
    ################################################
    # Pipeline
    ################################################
    [pipeline]
      # IP database type, supports iploc and geolite2
      ipdb_type = "iploc"
    
      # How often to sync remote pipeline
      remote_pull_interval = "1m"
    
      #
      # reftab configures
      #
      # Reftab remote HTTP URL (https/http)
      refer_table_url = ""
    
      # How often reftab syncs with the remote
      refer_table_pull_interval = "5m"
    
      # Use sqlite to store reftab data to release memory usage
      use_sqlite = false
      # or use pure memory to cache the reftab data
      sqlite_mem_mode = false
    
      # append run info
      disable_append_run_info = false
    
      # default pipeline
      [pipeline.default_pipeline]
        # logging = "<your_script.p>"
        # metric  = "<your_script.p>"
        # tracing = "<your_script.p>"
    
      # Offload data processing tasks to post-level data processors.
      [pipeline.offload]
        receiver = "datakit-http"
        addresses = [
          # "http://<ip>:<port>"
        ]
    
    ################################################
    # HTTP server (9529)
    ################################################
    [http_api]
    
      # HTTP server address
      listen = "localhost:9529"
    
      # Disable 404 page to hide detailed Datakit info
      disable_404page = false
    
      # Only enable these APIs. If the list is empty, all APIs are enabled.
      public_apis = []
    
      # Datakit server-side timeout
      timeout = "30s"
      close_idle_connection = false
    
      # API rate limit (QPS)
      request_rate_limit = 20.0
    
      #
      # RUM related: we should port these configurations to RUM inputs (TODO)
      #
      # When serving RUM (/v1/write/rum), extract the IP address from this HTTP header
      rum_origin_ip_header = "X-Forwarded-For"
      # When serving RUM (/v1/write/rum), only accept requests from these app-id.
      # If the list is empty, all app's requests are accepted.
      rum_app_id_white_list = []
    
      # Only these domains enable CORS. If the list is empty, all domains are enabled.
      allowed_cors_origins = []
    
      # Start Datakit web server with HTTPS
      [http_api.tls]
        # cert = "path/to/certificate/file"
        # privkey = "path/to/private_key/file"
    
    ################################################
    # io configurations
    ################################################
    [io]
      # How often Datakit flushes data to dataway.
      # Datakit will upload data points if cached (in memory) points
      # reached (>=) the max_cache_count or the flush_interval is triggered.
      max_cache_count = 1000
      flush_workers   = 0 # defaults to (cpu_core * 2)
      flush_interval  = "10s"
    
      # Queue size of feed.
      feed_chan_size = 1
    
      # Set blocking if queue is full.
      # NOTE: Global blocking mode may consume more memory on large metric points.
      global_blocking = false
    
      # Data point filter configurations.
      # NOTE: Most of the time, you should use web-side filters; it's a debug helper for developers.
      #[io.filters]
      #  logging = [
      #   "{ source = 'datakit' or f1 IN [ 1, 2, 3] }"
      #  ]
      #  metric = [
      #    "{ measurement IN ['datakit', 'disk'] }",
      #    "{ measurement CONTAIN ['host.*', 'swap'] }",
      #  ]
      #  object = [
      #    { class CONTAIN ['host_.*'] }",
      #  ]
      #  tracing = [
      #    "{ service = re("abc.*") AND some_tag CONTAIN ['def_.*'] }",
      #  ]
    
    [recorder]
      enabled = false
      #path = "/path/to/point-data/dir"
      encoding = "v2"  # use protobuf-json format
      duration = "30m" # record for 30 minutes
    
      # Only record these inputs, if empty, record all
      inputs = [
        #"cpu",
        #"mem",
      ]
    
      # Only record these categories, if empty, record all
      category = [
        #"logging",
        #"object",
      ]
    
    ################################################
    # Dataway configuration
    ################################################
    [dataway]
      # urls: Dataway URL list
      # NOTE: do not configure multiple URLs here, it's a deprecated feature.
      urls = [
        # "https://openway.guance.com?token=<YOUR-WORKSPACE-TOKEN>"
      ]
    
      # Dataway HTTP timeout
      timeout_v2 = "30s"
    
      # max_retry_count specifies at most how many times will be tried when dataway API fails (not 4xx),
      # default value (and minimal) is 1 and maximum value is 10.
      #
      # The default set to 1 to make the API fail ASAP to release memory.
      max_retry_count = 1
    
      # The interval between two retry operations, valid time units are "ns", "us", "ms", "s", "m", "h"
      retry_delay = "1s"
    
      # HTTP Proxy
      # Format: "http(s)://IP:Port"
      http_proxy = ""
    
      max_idle_conns   = 0       # limit idle TCP connections for HTTP requests to Dataway
      enable_httptrace = false   # enable trace HTTP metrics (connection/NDS/TLS and so on)
      idle_timeout     = "90s"   # not-set, default 90s
    
      # HTTP body content type, other candidates are (case insensitive):
      #  - v1: line-protocol
      #  - v2: protobuf
      content_encoding = "v2"
    
      # Enable GZip to upload point data.
      #
      # do NOT disable gzip or you'll get large network payload.
      gzip = true
    
      max_raw_body_size = 1048576 # max body size (before gzip) in bytes
    
      # Customer tag or field keys that will be extracted from existing points
      # to build the X-Global-Tags HTTP header value.
      global_customer_keys = []
      enable_sinker        = false # disable sinker
    
      # use dataway as NTP server
      [dataway.ntp]
        interval = "5m"  # sync dataway time each 5min
    
        # if datakit local time and dataway time's ABS value reaches the diff,
        # datakit's soft time will update to the dataway time.
        # NOTE: diff MUST be larger than "1s"
        diff     = "30s" 
    
      # WAL queue for uploading points
      [dataway.wal]
        max_capacity_gb = 2.0 # 2GB reserved disk space for each category (M/L/O/T/...)
        #workers = 4          # flush workers on WAL (default to CPU limited cores)
        #mem_cap = 4          # in-memory queue capacity (default to CPU limited cores)
        #fail_cache_clean_interval = "30s" # duration for cleaning failed uploaded data
    
    
    ################################################
    # Datakit logging configuration
    ################################################
    [logging]
    
      # log path
      log = "/var/log/datakit/log"
    
      # HTTP access log
      gin_log = "/var/log/datakit/gin.log"
    
      # level (info/debug)
      level = "info"
    
      # Disable log color
      disable_color = false
    
      # log rotate size (in MB)
      # DataKit will always keep at most n+1 (n backup logs and 1 writing log) split log files on disk.
      rotate = 32
    
      # Upper limit count of backup logs
      rotate_backups = 5
    
    ################################################
    # Global tags
    ################################################
    # We will try to add these tags to every collected data point if these
    # tags do not exist in original data.
    #
    # NOTE: we can get the real IP of the current node, we just need
    # to set "$datakit_ip" or "__datakit_ip" here. Same for the hostname.
    [global_host_tags]
      ip   = "$datakit_ip"
      host = "$datakit_hostname"
    
    [election]
      # Enable election
      enable = false
    
      # Election whitelist
      # NOTE: Empty to disable whitelist
      node_whitelist = []
    
      # Election namespace.
      # NOTE: for single workspace, there can be multiple election namespaces.
      namespace = "default"
      
      # If enabled, every data point will add a tag with election_namespace = <your-election-namespace>
      enable_namespace_tag = false
    
      # Like global_host_tags, but only for data points that are remotely collected (such as MySQL/Nginx).
      [election.tags]
        #  project = "my-project"
        #  cluster = "my-cluster"
    
    ###################################################
    # Tricky: we can rename the default hostname here
    ###################################################
    [environments]
      ENV_HOSTNAME = ""
    
    ################################################
    # resource limit configurations
    ################################################
    [resource_limit]
    
      # enable or disable resource limit
      enable = true
    
      # Linux only, cgroup path
      path = "/datakit"
    
      # Limit CPU cores
      cpu_cores = 2.0
    
      # set max memory usage (MB)
      mem_max_mb = 4096
    
    ################################################
    # git_repos configurations
    ################################################
    
    # We can host all input configurations on a git server
    [git_repos]
      # git pull interval
      pull_interval = "1m"
    
      # git repository settings
      [[git_repos.repo]]
        # enable the repository or not
        enable = false
    
        # the branch name to pull
        branch = "master"
    
        # git repository URL. There are 3 formats here:
        #   - HTTP(s): such as "https://github.datakit.com/path/to/datakit-conf.git"
        #   - Git: such as "git@github.com:path/to/datakit.git"
        #   - SSH: such as "ssh://git@github.com:9000/path/to/repository.git"
        url = ""
    
        # For formats Git and SSH, we need extra configurations:
        ssh_private_key_path = ""
        ssh_private_key_password = ""
    
    ################################################
    # crypto key or key filePath.
    ################################################
    [crypto]
      aes_key = ""
      aes_Key_file = ""
    
    [remote_job]
      enable=false
      envs = ["OSS_BUCKET_HOST=host","OSS_ACCESS_KEY_ID=key","OSS_ACCESS_KEY_SECRET=secret","OSS_BUCKET_NAME=bucket"]
      interval = "30s"
      java_home=""
    
    ```
<!-- markdownlint-enable -->

## HTTP Service Configuration {#config-http-server}

DataKit starts an HTTP service to receive external data or provide basic data services.

<!-- markdownlint-disable MD046 -->
=== "*datakit.conf*"

    ### Modify HTTP Service Address {#update-http-server-host}
    
    The default HTTP service address is `localhost:9529`. If port 9529 is occupied or you want to access DataKit's HTTP service externally (e.g., to receive [RUM](../integrations/rum.md) or [Tracing](../integrations/datakit-tracing.md) data), you can modify it to:
    
    ```toml
    [http_api]
       listen = "0.0.0.0:<other-port>"
       # or use IPv6 address
       # listen = "[::]:<other-port>"
    ```

    Note that IPv6 support requires [DataKit upgrade to 1.5.7](changelog.md#cl-1.5.7-new).

    #### Using Unix Domain Socket {#uds}

    DataKit supports UNIX domain socket access. To enable it, set the `listen` field to a **non-existent file's full path**, using `datakit.sock` as an example, which can be any filename:
    ```toml
    [http_api]
       listen = "/tmp/datakit.sock"
    ```
    After configuration, you can test if it is successful using the `curl` command: `sudo curl --no-buffer -XGET --unix-socket /tmp/datakit.sock http:/localhost/v1/ping`. More information about testing commands can be found [here](https://superuser.com/a/925610){:target="_blank"}.
    
    ### HTTP Request Rate Limiting {#set-http-api-limit}

    > [:octicons-tag-24: Version-1.62.0](changelog.md#cl-1.62.0) has already enabled this feature by default.
    
    Since DataKit needs to receive a large amount of external data, to avoid causing significant overhead to the node, you can modify the following HTTP configuration (disabled by default):
    
    ```toml
    [http_api]
      request_rate_limit = 20.0 # Limit the QPS of requests initiated by each client (IP + API route)
    ```

    ### Other Settings {#http-other-settings}

    ```toml
    [http_api]
        close_idle_connection = true # Close idle connections
        timeout = "30s"              # Set server-side HTTP timeout
    ```

=== "Kubernetes"

    Refer to [here](datakit-daemonset-deploy.md#env-http-api)
<!-- markdownlint-enable -->


### HTTP API Access Control {#public-apis}

[:octicons-tag-24: Version-1.64.0](changelog.md#cl-1.64.0)

For security reasons, DataKit restricts access to certain APIs by default (these APIs can only be accessed via localhost). If DataKit is deployed in a public environment and you need to access these APIs through the public network (or from another machine within a local network), you can modify the `public_apis` field in *datakit.conf*:

```toml
[http_api]
  public_apis = [
    # Allow access to the /metrics endpoint
    "/metrics",
  ]
```

By default, only the Ping endpoint and basic data upload endpoints are enabled for external access. All other endpoints are prohibited from external access. For collectors like trace collectors, once they are enabled, they can be accessed externally by default. For adding API whitelists in Kubernetes, refer to [here](datakit-daemonset-deploy.md#env-http-api).

## Global Tag Modification {#set-global-tag}

[:octicons-tag-24: Version-1.4.6](changelog.md#cl-1.4.6)

DataKit allows configuring global tags for all collected data. Global tags are divided into two categories:

- Host-related global tags (GHT): Data collected is bound to the current host, such as CPU/memory metrics.
- Election-related global tags (GET): Data comes from a common (remote) entity, such as MySQL/Redis. These collectors generally participate in elections, hence these data do not carry tags related to the current host.

```toml
[global_host_tags] # Tags inside here are called "global host tags"
  ip   = "__datakit_ip"
  host = "__datakit_hostname"

[election]
  [election.tags] # Tags inside here are called "global election tags"
    project = "my-project"
    cluster = "my-cluster"
```

When adding global tags, several points to note:

1. These global tag values can use wildcards currently supported by DataKit (both double underscores (`__`) prefix and `$` are acceptable):

    1. `__datakit_ip/$datakit_ip`: The tag value will be set to the first primary NIC IP obtained by DataKit.
    1. `__datakit_hostname/$datakit_hostname`: The tag value will be set to the hostname of DataKit.

1. Due to [DataKit data transmission protocol limitations](apis.md#lineproto-limitation), do not include any metric (Field) fields in the global tags (Tag), otherwise, it will lead to data processing failures due to protocol violations. Refer to specific collector field lists for more details. Also, don't add too many tags, and each tag's Key and Value length is limited.
1. If the collected data already contains tags with the same name, DataKit will not append the configured global tags again.
1. Even if there is no configuration in GHT, DataKit still adds a `host=__datakit_hostname` tag. Because hostname is the default field for data association on the Guance platform, logs/CPU/memory, etc., will all carry the `host` tag.
1. These two types of global tags (GHT/GET) can overlap, for example, both can have a `project = "my-project"` tag.
1. When elections are not enabled, GET inherits all tags from GHT (it has at least one `host` tag).
1. Election-type collectors default to appending GET, while non-election-type collectors default to appending GHT.

<!-- markdownlint-disable MD046 -->
???+ tip "How to distinguish between election and non-election collectors?"

    In the collector documentation, there are similar identifiers at the top indicating the current collector's platform compatibility and collection characteristics:

    :fontawesome-brands-linux: :fontawesome-brands-windows: :fontawesome-brands-apple: :material-kubernetes: :material-docker:  · :fontawesome-solid-flag-checkered:

    If it has :fontawesome-solid-flag-checkered:, it means the current collector is an election-type collector.
<!-- markdownlint-enable -->

### Setting Global Tags for Remote Collection {#notice-global-tags}

Since DataKit automatically appends the `host=<hostname where DataKit is located>` tag to all collected data, this default `host` can sometimes cause confusion.

Taking MySQL as an example, if MySQL is not on the same machine as DataKit, but you hope this `host` tag reflects the actual hostname of the MySQL instance (or other identifier fields for cloud databases), rather than the hostname where DataKit resides.

For such cases, there are two ways to bypass DataKit's global tags:

- In specific collector configurations, there is generally a section like the following where you can add new tags. For example, if you do not want DataKit to add the `host=xxx` tag by default, you can override this tag here, using MySQL as an example:

```toml
[[inputs.mysql.tags]]
  host = "real-mysql-host-name" 
```

- When sending data to DataKit via [HTTP API](apis.md#api-v1-write), you can use the API parameter `ignore_global_tags` to ignore all global tags.

<!-- markdownlint-disable MD046 -->
???+ tip

    Since [1.4.20](changelog.md#cl-1.4.20), DataKit defaults to using the IP/host from the connection address of the collected service as the `host` tag value.
<!-- markdownlint-enable -->

## DataKit Log Configuration {#logging-config}

DataKit has two logs: its own operation log (*var/log/datakit/log*) and the HTTP Access log (*var/log/datakit/gin.log*).

The default log level for DataKit is `info`. Editing `datakit.conf`, you can change the log level and shard size:

```toml
[logging]
  level = "debug" # Change info to debug
  rotate = 32     # Each log shard is 32MB
```

- `level`: Setting it to `debug` will show more logs (currently supports `debug/info` levels).
- `rotate`: DataKit automatically shards logs by default, with a shard size of 32MB, totaling 6 shards (1 current writing shard plus 5 cut shards; shard count is not configurable yet). If you find DataKit logs consuming too much disk space (up to 32 x 6 = 192MB), you can reduce the `rotate` size (for example, change it to 4, unit is MB). HTTP access logs also auto-shard similarly.

## Advanced Configuration {#advance-config}

The following sections involve advanced configurations. If you are unsure about the configurations, it is recommended to consult our technical experts.

### Point Pool {#point-pool}

[:octicons-tag-24: Version-1.28.0](changelog.md#cl-1.28.0)

> Currently, the Point Pool has additional performance issues and is not recommended for use.

To optimize DataKit's memory usage under high load, you can enable the Point Pool to alleviate it:

```toml
# datakit.conf
[point_pool]
    enable = true
    reserved_capacity = 4096
```

Simultaneously, in [DataKit configuration](datakit-conf.md#dataway-settings), you can enable `content_encoding = "v2"` for transmission encoding ([:octicons-tag-24: Version-1.32.0](changelog.md#cl-1.32.0) has already enabled v2 by default), which offers lower memory and CPU overhead compared to v1.

<!-- markdownlint-disable MD046 -->
???+ attention

    - Under low load (DataKit memory usage around 100MB), enabling point pool increases DataKit's memory consumption. High load generally refers to scenarios with over 2GB memory usage. Enabling it can also improve DataKit's CPU consumption.

<!-- markdownlint-enable -->

### IO Module Tuning {#io-tuning}

[:octicons-tag-24: Version-1.4.8](changelog.md#cl-1.4.8) ·
[:octicons-beaker-24: Experimental](index.md#experimental)

<!-- markdownlint-disable MD046 -->
=== "*datakit.conf*"

    In some cases, the amount of data collected by a single DataKit instance is very large. If the network bandwidth is limited, it might lead to interruptions or discarding of some data collections. You can adjust some parameters of the IO module to mitigate this issue:

    ```toml
    [io]
      feed_chan_size  = 1     # Length of the data processing queue
      max_cache_count = 1000  # Threshold for batch sending data points, triggers sending when exceeded
      flush_interval  = "10s" # Interval threshold for data sending, sends at least once every 10s
      flush_workers   = 0     # Number of data upload workers (defaults to CPU core * 2)
    ```

    Blocking mode see [corresponding explanation in k8s](datakit-daemonset-deploy.md#env-io)

=== "Kubernetes"

    Refer to [here](datakit-daemonset-deploy.md#env-io)
<!-- markdownlint-enable -->

### Resource Limitation {#resource-limit}

Due to the unpredictable volume of data processed by DataKit, if physical limits are not placed on the resources consumed by DataKit, it may consume a large amount of resources on the node. Here, we can use Linux's cgroup and Windows's job object for restrictions, with the following configuration in *datakit.conf*:

```toml
[resource_limit]
  path = "/datakit" # Linux cgroup restriction directory, such as /sys/fs/cgroup/memory/datakit, /sys/fs/cgroup/cpu/datakit

  # Allowed CPU cores
  cpu_cores = 2.0

  # Default allows 4GB memory (memory + swap) usage
  # If set to 0 or negative, memory limitation is disabled
  mem_max_mb = 4096 
```

If DataKit exceeds the memory limit, it will be forcibly killed by the operating system. You can see the result using the following command, and then [manually start the service](datakit-service-how-to.md#when-service-failed):

```shell
$ systemctl status datakit 
● datakit.service - Collects data and uploads it to DataFlux.
     Loaded: loaded (/etc/systemd/system/datakit.service; enabled; vendor preset: enabled)
     Active: activating (auto-restart) (Result: signal) since Fri 2022-02-30 16:39:25 CST; 1min 40s ago
    Process: 3474282 ExecStart=/usr/local/datakit/datakit (code=killed, signal=KILL)
   Main PID: 3474282 (code=killed, signal=KILL)
```

<!-- markdownlint-disable MD046 -->
???+ attention

    - Resource limitation is enabled by default only during [host installation](datakit-install.md).
    - Supports CPU usage and memory usage control (memory + swap), and is only supported on Linux and Windows ([Version-1.15.0](changelog.md#cl-1.15.0)).
    - CPU usage control is not supported on these Windows systems: Windows 7, Windows Server 2008 R2, Windows Server 2008, Windows Vista, Windows Server 2003, and Windows XP.
    - Non-root users must reinstall the service when changing resource limitation configurations.
    - CPU core limits affect the worker configurations of some DataKit submodules (usually integer multiples of CPU cores). For example, data upload workers are CPU core * 2. A single upload worker uses 10MB of memory by default for data transmission, so a higher number of CPU cores can impact overall DataKit memory usage.
    - [Version-1.5.8](changelog.md#cl-1.5.8) began supporting cgroup v2. If uncertain about the cgroup version, confirm using the command `mount | grep cgroup`.
    - [Version-1.68.0](changelog.md#cl-1.68.0) supports configuring CPU core limits in *datakit.conf*, deprecating the previous percentage-based configuration method. Percentage-based configuration varies depending on the host's CPU core count, leading to inconsistent CPU quotas under the same collection pressure, potentially causing abnormal behavior. During upgrades, specify the `DK_LIMIT_CPUCORES` environment variable in the upgrade command. If not specified, it continues to use the previous percentage-based configuration. If DataKit is reinstalled, it directly adopts the CPU core quota method.
<!-- markdownlint-enable -->

### Election Configuration {#election}

Refer to [here](election.md#config)

### DataWay Parameter Configuration {#dataway-settings}

There are a few configurations available for DataWay, other parts are not recommended to be modified:

- `timeout`: Upload timeout to Guance, default 30s
- `max_retry_count`: Set the retry count for DataWay sending (default 1 time, max 10 times) [:octicons-tag-24: Version-1.17.0](changelog.md#cl-1.17.0)
- `retry_delay`: Set the base step length for retry intervals, default 1s. Base step length means the first retry after 1s, second retry after 2s, third retry after 4s, and so on (exponentially increasing by 2^n) [:octicons-tag-24: Version-1.17.0](changelog.md#cl-1.17.0)
- `max_raw_body_size`: Control the maximum size of a single upload package (before compression), in bytes [:octicons-tag-24: Version-1.17.1](changelog.md#cl-1.17.1)
- `content_encoding`: Choose between v1 or v2 [:octicons-tag-24: Version-1.17.1](changelog.md#cl-1.17.1)
    - v1 is line protocol (default v1)
    - v2 is Protobuf protocol, which performs better in all aspects compared to v1. After stable operation, v2 will be the default.

For configurations related to Kubernetes deployment, refer to [here](datakit-daemonset-deploy.md#env-dataway).

#### WAL Queue Configuration {#dataway-wal}

[:octicons-tag-24: Version-1.60.0](changelog.md#cl-1.60.0)

In `[dataway.wal]`, we can adjust the WAL queue configuration:

```toml
  [dataway.wal]
     max_capacity_gb = 2.0             # 2GB reserved disk space for each category (M/L/O/T/...)
     workers = 0                       # flush workers on WAL (default to CPU limited cores)
     mem_cap = 0                       # in-memory queue capacity (default to CPU limited cores)
     fail_cache_clean_interval = "30s" # duration for cleaning failed uploaded data
```

Disk files are located in the *cache/dw-wal* directory under the DataKit installation directory:

```shell
/usr/local/datakit/cache/dw-wal/
├── custom_object
│   └── data
├── dialtesting
│   └── data
├── dynamic_dw
│   └── data
├── fc
│   └── data
├── keyevent
│   └── data
├── logging
│   ├── data
│   └── data.00000000000000000000000000000000
├── metric
│   └── data
├── network
│   └── data
├── object
│   └── data
├── profiling
│   └── data
├── rum
│   └── data
├── security
│   └── data
└── tracing
    └── data

13 directories, 14 files
```

Here, except for *fc* being the failed retransmission queue, other directories correspond to different data types. When data upload fails, these data are cached in the *fc* directory, and DataKit periodically uploads them.

If the current host's disk performance is insufficient, you can try [using WAL with tmpfs](wal-tmpfs.md).

### Sinker Configuration {#dataway-sink}

Refer to [here](../deployment/dataway-sink.md)

### Managing DataKit Configuration with Git {#using-gitrepo}

Refer to [here](git-config-how-to.md)

### Setting Default Pipeline Scripts Locally {#pipeline-settings}

[:octicons-tag-24: Version-1.61.0](changelog.md#cl-1.61.0)

Supports setting default Pipeline scripts locally. If there is a conflict with remote default scripts, local settings take precedence.

Configuration can be done in two ways:

- Host deployment, specify default scripts for various categories in the main DataKit configuration file, as follows:

    ```toml
    # default pipeline
    [pipeline.default_pipeline]
        # logging = "<your_script.p>"
        # metric  = "<your_script.p>"
        # tracing = "<your_script.p>"
    ```

- Container deployment, use the environment variable `ENV_PIPELINE_DEFAULT_PIPELINE`, with a value like `{"logging":"abc.p","metric":"xyz.p"}`

### Setting Maximum File Descriptor Limit {#enable-max-fd}

In Linux environments, you can configure the `ulimit` item in the main DataKit configuration file to set the maximum number of files DataKit can open, as follows:

```toml
ulimit = 64000
```

The default `ulimit` is set to 64000. In Kubernetes, set it via [setting `ENV_ULIMIT`](datakit-daemonset-deploy.md#env-others).

### :material-chat-question: Explanation of CPU Usage Limits {#cgroup-how}

CPU usage is in percentage terms (maximum value 100.0). For example, on an 8-core CPU, if the limit `cpu_max` is set to 20.0 (i.e., 20%), DataKit's maximum CPU consumption will be displayed as approximately 160% in the top command.

### Collector Password Protection {#secrets_management}

[:octicons-tag-24: Version-1.31.0](changelog.md#cl-1.31.0)


If you wish to avoid storing passwords in plain text in configuration files, you can use this feature.

When DataKit loads collector configuration files and encounters `ENC[]`, it replaces the text with the correct password after reading it from a file, environment variable, or AES decryption.

ENC currently supports three methods:

- File form (recommended):

    Password format in the configuration file: ENC[file:///path/to/enc4dk], fill in the correct password in the corresponding file.

- AES encryption method.

    Configure the secret key in the main configuration file `datakit.conf`: `crypto_AES_key` or `crypto_AES_Key_filePath`, with a key length of 16 characters.
    Fill in the password as: `ENC[aes://5w1UiRjWuVk53k96WfqEaGUYJ/Oje7zr8xmBeGa3ugI=]`


Next, taking `mysql` as an example, let's explain how to configure and use the two methods:

1 File Form

First, place the plaintext password in the file `/usr/local/datakit/enc4mysql`, then modify the configuration file `mysql.conf`:

```toml
# Partial configuration
[[inputs.mysql]]
  host = "localhost"
  user = "datakit"
  pass = "ENC[file:///usr/local/datakit/enc4mysql]"
  port = 3306
  # sock = "<SOCK>"
  # charset = "utf8"
```

DataKit reads the password from `/usr/local/datakit/enc4mysql` and replaces the password, resulting in `pass = "Hello*******"`

2 AES Encryption Method

First, configure the secret key in `datakit.conf`:

```toml
# crypto key or key filePath.
[crypto]
  # Configure the secret key
  aes_key = "0123456789abcdef"
  # Or, place the secret key in a file and specify the file location here.
  aes_Key_file = "/usr/local/datakit/mykey"
```

`mysql.conf` configuration file:

```toml
pass = "ENC[aes://5w1UiRjWuVk53k96WfqEaGUYJ/Oje7zr8xmBeGa3ugI=]"
```

Note, the ciphertext obtained through AES encryption must be fully entered. Below is the code sample:
<!-- markdownlint-disable MD046 -->
=== "Golang"

    ```go
    // AESEncrypt  encrypts.
    func AESEncrypt(key []byte, plaintext string) (string, error) {
        block, err := aes.NewCipher(key)
        if err != nil {
            return "", err
        }
    
        // PKCS7 padding
        padding := aes```go
        padding := aes.BlockSize - len(plaintext)%aes.BlockSize
        padtext := bytes.Repeat([]byte{byte(padding)}, padding)
        plaintext += string(padtext)
        ciphertext := make([]byte, aes.BlockSize+len(plaintext))
        iv := ciphertext[:aes.BlockSize]
        if _, err := io.ReadFull(rand.Reader, iv); err != nil {
            return "", err
        }
        mode := cipher.NewCBCEncrypter(block, iv)
        mode.CryptBlocks(ciphertext[aes.BlockSize:], []byte(plaintext))
    
        return base64.StdEncoding.EncodeToString(ciphertext), nil
    }
    
    // AESDecrypt decrypts AES.
    func AESDecrypt(key []byte, cryptoText string) (string, error) {
        ciphertext, err := base64.StdEncoding.DecodeString(cryptoText)
        if err != nil {
            return "", err
        }
    
        block, err := aes.NewCipher(key)
        if err != nil {
            return "", err
        }
    
        if len(ciphertext) < aes.BlockSize {
            return "", fmt.Errorf("ciphertext too short")
        }
    
        iv := ciphertext[:aes.BlockSize]
        ciphertext = ciphertext[aes.BlockSize:]
    
        mode := cipher.NewCBCDecrypter(block, iv)
        mode.CryptBlocks(ciphertext, ciphertext)
    
        // Remove PKCS7 padding
        padding := int(ciphertext[len(ciphertext)-1])
        if padding > aes.BlockSize {
            return "", fmt.Errorf("invalid padding")
        }
        ciphertext = ciphertext[:len(ciphertext)-padding]
    
        return string(ciphertext), nil
    }
    ```

=== "Java"

    ```java
    import javax.crypto.Cipher;
    import javax.crypto.spec.IvParameterSpec;
    import javax.crypto.spec.SecretKeySpec;
    import java.security.SecureRandom;
    import java.util.Base64;
    
    public class AESUtils {
        public static String AESEncrypt(byte[] key, String plaintext) throws Exception {
            javax.crypto.Cipher cipher = Cipher.getInstance("AES/CBC/PKCS5Padding");
            SecretKeySpec secretKeySpec = new SecretKeySpec(key, "AES");
    
            SecureRandom random = new SecureRandom();
            byte[] iv = new byte[16];
            random.nextBytes(iv);
            IvParameterSpec ivParameterSpec = new IvParameterSpec(iv);
            cipher.init(Cipher.ENCRYPT_MODE, secretKeySpec, ivParameterSpec);
            byte[] encrypted = cipher.doFinal(plaintext.getBytes());
            byte[] ivAndEncrypted = new byte[iv.length + encrypted.length];
            System.arraycopy(iv, 0, ivAndEncrypted, 0, iv.length);
            System.arraycopy(encrypted, 0, ivAndEncrypted, iv.length, encrypted.length);
    
            return Base64.getEncoder().encodeToString(ivAndEncrypted);
        }
    
        public static String AESDecrypt(byte[] key, String cryptoText) throws Exception {
            byte[] ciphertext = Base64.getDecoder().decode(cryptoText);
    
            SecretKeySpec secretKeySpec = new SecretKeySpec(key, "AES");
    
            if (ciphertext.length < 16) {
                throw new Exception("ciphertext too short");
            }
    
            byte[] iv = new byte[16];
            System.arraycopy(ciphertext, 0, iv, 0, 16);
            byte[] encrypted = new byte[ciphertext.length - 16];
            System.arraycopy(ciphertext, 16, encrypted, 0, ciphertext.length - 16);
    
            Cipher cipher = Cipher.getInstance("AES/CBC/PKCS5Padding");
            IvParameterSpec ivParameterSpec = new IvParameterSpec(iv);
            cipher.init(Cipher.DECRYPT_MODE, secretKeySpec, ivParameterSpec);
    
            byte[] decrypted = cipher.doFinal(encrypted);
    
            return new String(decrypted);
        }
    }
    public static void main(String[] args) {
        try {
            String key = "0123456789abcdef"; // 16, 24, or 32 bytes AES key
            String plaintext = "HelloAES9*&.";
            byte[] keyBytes = key.getBytes("UTF-8");

            String encrypted = AESEncrypt(keyBytes, plaintext);
            System.out.println("Encrypted text: " + encrypted);

            String decrypt = AESDecrypt(keyBytes, encrypted);
            System.out.println("Decoded text: " + decrypt);
        } catch (Exception e) {
            System.out.println(e);
            e.printStackTrace();
        }
    }
    ```
<!-- markdownlint-enable -->

In K8S environments, you can add private keys via environment variables: `ENV_CRYPTO_AES_KEY` and `ENV_CRYPTO_AES_KEY_FILEPATH`. Refer to [DaemonSet Installation - Others](datakit-daemonset-deploy.md#env-others).

### Remote Jobs {#remote-job}

---

[:octicons-tag-24: Version-1.63.0](changelog.md#cl-1.63.0)

---

DataKit receives tasks from the center and executes them. Currently, it supports the `JVM dump` feature.

This function executes the `jmap` command, generates a dump file, and uploads it to `OSS`, `AWS S3 Bucket`, or `Huawei Cloud OBS`.

After installing DK, two files will be generated in the installation directory under `template/service-task`: `jvm_dump_host_script.py` and `jvm_dump_k8s_script.py`. The former is for host mode scripts, and the latter is for k8s environments.

After DK starts, it periodically executes the script. If the script is modified, it will be overwritten after a DK restart.

In host environments, Python 3 and packages are required. If not installed, you need to install them:

```shell
# With Python 3 environment
pip install requests
# Or
pip3 install requests

# If uploading to Huawei Cloud OBS requires installing the library:
pip install esdk-obs-python --trusted-host pypi.org

# If uploading to AWS S3 requires installing boto3:
pip install boto3
```

You can control the upload destination through environment variables. Below is the configuration explanation; configurations for k8s environments are similar:

```toml
# Upload to OSS
[remote_job]
  enable = true
  envs = [
      "REMOTE=oss",
      "OSS_BUCKET_HOST=host","OSS_ACCESS_KEY_ID=key","OSS_ACCESS_KEY_SECRET=secret","OSS_BUCKET_NAME=bucket",
    ]
  interval = "30s"

# Or upload to AWS:
[remote_job]
  enable = true
  envs = [
      "REMOTE=aws",
      "AWS_BUCKET_NAME=bucket","AWS_ACCESS_KEY_ID=AK","AWS_SECRET_ACCESS_KEY=SK","AWS_DEFAULT_REGION=us-west-2",
    ]
  interval = "30s"
  
# Or upload to OBS:
[remote_job]
  enable = true
  envs = [
      "REMOTE=obs",
      "OBS_BUCKET_NAME=bucket","OBS_ACCESS_KEY_ID=AK","OBS_SECRET_ACCESS_KEY=SK","OBS_SERVER=https://xxx.myhuaweicloud.com"
    ]
  interval = "30s"    
```

In k8s environments, Kubernetes API calls require RBAC (Role-Based Access Control).

Configuration related:

<!-- markdownlint-disable MD046 -->
=== "Host Deployment"

    The directory is generally located at:
    
    - Linux/Mac: `/usr/local/datakit/conf.d/datakit.conf`
    - Windows: `C:\Program Files\datakit\conf.d\datakit.conf`

    Modify the configuration, adding the following if not already present:
    ```toml
    [remote_job]
      enable=true
      envs=["REMOTE=oss","OSS_BUCKET_HOST=<bucket_host>","OSS_ACCESS_KEY_ID=<key>","OSS_ACCESS_KEY_SECRET=<secret_key>","OSS_BUCKET_NAME=<name>"]
      interval="100s"
      java_home=""
    ```

=== "Kubernetes"

    Modify the DataKit YAML file and add RBAC permissions

    ```yaml

    ---
    
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: datakit
    rules:
    - apiGroups: ["rbac.authorization.k8s.io"]
      resources: ["clusterroles"]
      verbs: ["get", "list", "watch"]
    - apiGroups: [""]
      resources: ["nodes", "nodes/stats", "nodes/metrics", "namespaces", "pods", "pods/log", "events", "services", "endpoints", "persistentvolumes", "persistentvolumeclaims", "pods/exec"]
      verbs: ["get", "list", "watch", "create"]
    - apiGroups: ["apps"]
      resources: ["deployments", "daemonsets", "statefulsets", "replicasets"]
      verbs: ["get", "list", "watch"]
    - apiGroups: ["batch"]
      resources: ["jobs", "cronjobs"]
      verbs: ["get", "list", "watch"]
    - apiGroups: ["guance.com"]
      resources: ["datakits"]
      verbs: ["get","list"]
    - apiGroups: ["monitoring.coreos.com"]
      resources: ["podmonitors", "servicemonitors"]
      verbs: ["get", "list"]
    - apiGroups: ["metrics.k8s.io"]
      resources: ["pods", "nodes"]
      verbs: ["get", "list"]
    - nonResourceURLs: ["/metrics"]
      verbs: ["get"]
    
    ---
    ```

    In the above configuration, add "pod/exec". Keep other parts consistent with the YAML.

    Add remote_job environment variables:

    ```yaml
    - name: ENV_REMOTE_JOB_ENABLE
      value: 'true'
    - name: ENV_REMOTE_JOB_ENVS
      value: >-
        REMOTE=oss,OSS_BUCKET_HOST=<bucket_host>,OSS_ACCESS_KEY_ID=<key>,OSS_ACCESS_KEY_SECRET=<secret_key>,OSS_BUCKET_NAME=<name>
    - name: ENV_REMOTE_JOB_JAVA_HOME
    - name: ENV_REMOTE_JOB_INTERVAL
      value: 100s

    ```

<!-- markdownlint-enable -->

Configuration Explanation:

1. `enable ENV_REMOTE_JOB_ENABLE remote_job` enables the feature.
2. `envs ENV_REMOTE_JOB_ENVS` includes `host`, `access key`, `secret key`, `bucket` information, sending the obtained JVM dump file to OSS. Similarly for AWS and OBS, just change the environment variables accordingly.
3. `interval ENV_REMOTE_JOB_INTERVAL` sets the time interval for DataKit to actively call the interface to get the latest task.
4. `java_home ENV_REMOTE_JOB_JAVA_HOME` automatically retrieves from the environment variable (`$JAVA_HOME`) in the host environment, so it usually doesn't need to be configured.

> Note, the used Agent:`dd-java-agent.jar` version should not be lower than `v1.4.0-guance`

## Further Reading {#more-reading}

- [DataKit Host Installation](datakit-install.md)
- [DataKit DaemonSet Installation](datakit-daemonset-deploy.md)
- [DataKit Line Protocol Filters](datakit-filter.md)
```
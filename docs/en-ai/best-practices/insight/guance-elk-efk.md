# Guance VS ELK, EFK

---

## Overview of ELK, EFK, and Guance

As software systems become increasingly complex, logs are typically generated by servers and output to different files. Common log types include system logs, application logs, and security logs. These logs are stored on various machines in a distributed manner. When a system failure occurs, engineers often need to log into each server and use Linux scripting tools like grep, sed, awk, etc., to find the cause of the failure. Without a logging system, identifying the server handling the request first requires locating the server, and if multiple instances are deployed on that server, one must search through the log directories of each application instance. Each application instance also sets up log rotation policies (e.g., generating a new file daily) and compression archiving strategies. This series of procedures can make it difficult to diagnose and quickly identify the root cause of issues.

When deploying to the cloud, logging into each node to check logs from various modules is generally impractical. Not only is it inefficient, but due to security reasons, engineers cannot directly access physical nodes. Moreover, modern large-scale software systems typically use clustered deployment, meaning each service has multiple identical PODs providing services externally. Each container generates its own logs, making it challenging to determine which POD produced a particular log, thus complicating the viewing of distributed logs.

Therefore, centralizing these logs and providing centralized search functionality not only improves diagnostic efficiency but also provides a comprehensive understanding of the system, avoiding the reactive approach of addressing issues after they occur.

## ELK

So, what exactly is ELK? "ELK" is an acronym for three open-source projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a search and analytics engine. Logstash is a server-side data processing pipeline that can simultaneously collect data from multiple sources, transform the data, and send it to repositories like Elasticsearch. Kibana allows users to visualize data in Elasticsearch using graphs and charts.

### Elasticsearch

Elasticsearch is a JSON-based distributed search and analytics engine. It can be accessed via RESTful web service interfaces and uses JSON documents to store data. Built on the Java programming language, Elasticsearch can run on different platforms, enabling users to search large volumes of data very quickly.

#### Key Features

- Distributed real-time document storage with every field indexed and searchable
- Distributed real-time analytical search engine
- Scalable to hundreds of servers, handling PB-level structured or unstructured data

![image.png](../images/guance-elk-efk-1.png)

### Logstash

Logstash is an open-source streaming ETL engine for data streams, allowing data pipelines to be set up within minutes. It is horizontally scalable, resilient, and features adaptive buffering. It has an ecosystem of over 200 integrated plugins and processors, monitored and managed using the Elastic Stack.

#### Key Features

- Can access almost any type of data
- Integrates with multiple external applications
- Supports elastic scalability

#### Components of Logstash

- **Inputs:** Inputs provide rules for receiving data, such as collecting file content.
- **Filters:** Filters primarily handle filtering transmitted data, such as using grok rules for data filtering.
- **Outputs:** Outputs mainly define how received data should be output according to specified patterns, such as sending data to Elasticsearch.

![image.png](../images/guance-elk-efk-2.png)

### Kibana

Kibana is an open-source data analysis and visualization platform and is part of the Elastic Stack. Designed to work with Elasticsearch, you can use Kibana to search, view, and interact with data indexed in Elasticsearch. You can easily create diverse visualizations and presentations of data using charts, tables, and maps.

Kibana makes big data accessible and understandable. Its browser-based interface simplifies the process of quickly creating and sharing dynamic dashboards to track real-time changes in Elasticsearch data.

## EFK

EFK is not a single piece of software but a solution set. EFK stands for Elasticsearch, Fluentd, Kibana or Elasticsearch, Filebeat, Kibana. Elasticsearch handles log analysis and storage, Fluentd and Filebeat manage log collection, and Kibana is responsible for interface display. They work together seamlessly, efficiently meeting many scenarios' requirements and forming a mainstream log analysis system solution.

### Fluentd

Fluentd is an open-source data collector designed specifically for handling data streams, using JSON as the data format. It employs a plugin-based architecture, offering high scalability and availability while ensuring reliable message forwarding. Users can send various data sources to Fluentd, which then forwards the information to different destinations based on configuration, such as files, SaaS platforms, databases, or even another Fluentd instance.

#### Key Features

- Easy installation
- Low resource consumption
- Semi-structured data logging
- Flexible plugin mechanism
- Reliable buffering
- Log forwarding

#### Components of Fluentd

Fluentd's Input/Buffer/Output components are similar to Flume's Source/Channel/Sink.

- **Input:** Receives or actively fetches data. Supports syslog, http, file tail, etc.
- **Buffer:** Ensures performance and reliability of data acquisition. Different types of buffers like file or memory can be configured.
- **Output:** Responsible for sending data to the destination, e.g., files, AWS S3, or other Fluentd instances.

![image.png](../images/guance-elk-efk-3.png)

### Filebeat

Filebeat is a lightweight log collector implemented in Golang and is part of the Elasticsearch stack. Essentially an agent, it can be installed on various nodes, reading logs from specified locations and reporting them to designated destinations.

Filebeat is highly reliable, ensuring logs are reported at least once, considering various issues in log collection, such as resuming from breakpoints, file renaming, and truncated logs.

Filebeat does not depend on Elasticsearch and can operate independently. We can use Filebeat alone for log reporting and collection. Filebeat includes common Output components like Kafka, Elasticsearch, Redis, and can also output to console and file for debugging purposes. Using existing Output components, logs can be reported. Additionally, custom Output components can be created to forward logs to desired destinations.

Filebeat is part of elastic/beats, along with HeartBeat, PacketBeat, all built on the libbeat framework.

#### Components of Filebeat

- **Harvester:** The harvester reads the contents of a single file. It reads each file and sends the content to the output. Each file starts a harvester, which opens and closes files, keeping file descriptors open during runtime. If a file is deleted or renamed while being read, Filebeat continues reading the file.

- **Prospector:** The prospector manages harvesters and finds all sources to read. If the input type is logs, the prospector looks for all matching files and starts a harvester for each file. Each prospector runs in its own Go goroutine.

> Note: Filebeat prospector can only read local files and has no functionality to connect to remote hosts to read stored files or logs. Due to the limited scope of Filebeat, this article does not extensively compare Filebeat.

![image.png](../images/guance-elk-efk-4.png)

## Guance

### DataKit

DataKit is a fundamental data collection tool running on user-local machines, primarily used to collect various metrics and logs from system operations. It aggregates this data for [Guance](https://guance.com), where users can view and analyze their metrics and logs. DataKit is a critical data collection component in Guance, with all data in Guance originating from DataKit.

1. DataKit collects various metrics periodically and sends the data to DataWay via HTTP(s) at fixed intervals and quantities. Each DataKit is configured with a corresponding token to identify different users.

2. After receiving data, DataWay forwards it to Guance. The forwarded data includes an API signature.

3. Upon receiving valid data, Guance writes the data to different storages based on the data type.

For data collection tasks, some data loss is generally acceptable (since data is intermittently collected, data within the interval can be considered lost). The entire data transmission chain has the following loss protection measures:

1. If DataKit fails to send data to DataWay due to network issues, it caches up to 1000 data points. When the cache exceeds this limit, it gets cleared.

2. If DataWay fails to send data to Guance or cannot keep up with the traffic, it persists the data to disk. Once traffic decreases or the network recovers, DataWay sends the cached data to Guance. Delayed data does not affect timeliness as timestamps are attached to cached data.

On DataWay, to protect disks, the maximum disk usage can be configured to prevent overuse. For excess data, DataWay discards it. However, this capacity is usually set relatively high.

![image.png](../images/guance-elk-efk-21.png)

#### Components of DataKit

From top to bottom, DataKit's internal structure mainly consists of three layers:

- **Top Layer:** Includes entry modules and common modules
   - Configuration loading module: In addition to its main configuration (`conf.d/datakit.conf`), DataKit's individual collectors are separately configured. Combining them would result in a very large configuration file, making editing inconvenient.
   - Service management module: Manages the entire DataKit service.
   - Toolchain module: Besides data collection, DataKit provides various peripheral functions, such as viewing documentation, restarting services, updating, etc.
   - Pipeline module: Uses Grok scripts in log processing to split logs, converting unstructured log data into structured data. Similar processing can be applied to non-log data.
   - Election module: When deploying numerous DataKits, configurations can be made uniform and deployed automatically. The election module ensures that only one DataKit collects certain data (to avoid duplication and reduce pressure on the source).
   - Documentation module: Documentation is included with DataKit installation. Users can access the documentation list via `http://localhost:9529/man` or browse it via command line.

- **Transmission Layer:** Handles almost all data input and output.
   - HTTP service module: Supports third-party data integration, such as Telegraf/Prometheus. Currently, this data is accessed via HTTP.
   - IO module: Collects data from each plugin and sends it to IO module for unified data construction, processing, and transmission. IO module sends data to DataWay at regular intervals.

- **Collection Layer:** Responsible for collecting various data types.
   - Active collectors: Collect data at fixed frequencies, e.g., CPU, network card traffic, cloud dial testing, etc.
   - Passive collectors: Typically collect data from external inputs, such as RUM, Tracing. They run outside DataKit and upload standardized data to Guance via DataKit's data upload API.

![image.png](../images/guance-elk-efk-5.png)

### Guance Platform

Based on powerful data collection capabilities, "Guance" builds full-chain observability from infrastructure, containers, middleware, databases, message queues, application traces, front-end visits, system security, and network performance. With correct DataKit configuration, users can quickly achieve complete observability for their projects. Based on Line Protocol and Guance's scenario-building capabilities, users can customize and integrate required metrics, enhancing observability.

Guance, as a comprehensive observability product, has significant technical barriers. Compared to open-source solutions, Guance emphasizes reducing user learning costs and improving usability. Therefore, from DataKit installation to all configurable features, Guance aims to simplify configuration difficulty, aligning with most programmers and operations engineers' habits. The UI is user-friendly and professional, helping users quickly understand the product and its value.

## Platform Comparison

One of Logstash's original advantages was its support for Windows because it was written in JRuby;

Fluentd recently added Windows support, no longer relying on Linux-centric event libraries. You can also use the in_windows_eventlog plugin to track Windows event logs;

DataKit, provided officially by Guance, supports multiple data sources, integrates various data inputs, and supports Windows, Linux, Mac operating systems, ARM, X86 architectures, and achieves full-platform compatibility for log collection.

### Logstash

Linux and Windows

### Fluentd

Linux and Windows

### DataKit

Supports all platforms and provides a visual interface for managing configurations, significantly lowering the learning curve for installation and complex configurations.

## Event Routing Comparison

For event routing configuration, Fluentd's method is more declarative, while Logstash's method is procedural. Developers trained in procedural programming may find Logstash's configuration easier to start with. Fluentd's tag-based routing allows clear expression of complex routes. However, Guance can achieve event alerts and data browsing without relying on other products, ensuring data security and providing excellent user experience.

## Logstash Event Routing

Logstash routes all data to a single stream and uses if-then statements to send them to the desired destination. Here’s an example of sending error events in production to PagerDuty:

```go
output {
if [loglevel] == "ERROR" and [deployment] == "production" {
pagerduty {
...
}
}
}
```

## Fluentd Event Routing

Fluentd relies on tags to route events. Each Fluentd event has a tag indicating its destination. If you want to send error events in production to PagerDuty, the configuration would look like this:

```go
<source>
  @type forward
</source>

<filter app.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
  </record>
</filter>

<match app.**>
  @type file
  # ...
</match>
```

## DataKit Proxy

DataKit, as a key component of Guance, sends data directly to the Guance cloud platform for analysis without needing to route events to other tools for analysis or caching. To ensure data security and address DataKit deployment in internal networks without Internet access, DataKit's proxy configuration is simple, requiring only enabling the proxy option. A rich set of features can be experienced with minimal configuration.

```go
[[inputs.proxy]]
  ## default bind ip address
  bind = "0.0.0.0"
  ## default bind port
  port = 9530
```

## Plugin Ecosystem Comparison

Logstash, Fluentd, and DataKit all have rich plugin ecosystems covering many input systems (files and TCP/UDP, etc.), filters (field splitting and filtering).

### [Logstash Plugins](https://www.elastic.co/guide/en/logstash/current/input-plugins.html)

Logstash manages all its plugins on GitHub, including 200+ input, filter, and output plugins maintained by users, lacking official maintenance.

![image.png](../images/guance-elk-efk-7.png)

### [Fluentd Plugins](https://www.fluentd.org/plugins/all)

Fluentd includes eight types of plugins—input, parser, filter, output, formatter, storage, service discovery, and buffer—totaling over 500 plugins. Only ten are officially maintained, with the rest supported by users.

### ![image.png](../images/guance-elk-efk-6.png)

### [DataKit Plugins](/integrations)

DataKit offers powerful built-in features—dynamic Grok query debugging, fast data querying using proprietary DQL, real-time input observation, edge computing, and visual client configuration. It supports 200+ officially maintained data source integrations and technology stacks, compatible with Telegraf, Beats, Logstash, Fluentd, etc. Users can visually manage plugins and agents and monitor data collection in real-time.

![image.png](../images/guance-elk-efk-22.png)

## Queue Comparison

Logstash lacks persistent internal message queues and depends on external queues like Redis for persistence. Fluentd has configurable buffer systems that can be in-memory or on-disk but are complex to configure. DataKit has a built-in caching mechanism that can be adjusted by changing simple parameters.

### Logstash Queue

Due to the lack of a persistent internal message queue, Logstash relies on external Redis queues for persistence.

### [Fluentd Queue](https://docs.fluentd.org/buffer/file)

Fluentd has built-in reliability but complex configuration, increasing user learning costs.

### DataKit Queue

DataKit has a built-in caching mechanism. When DataKit fails to send data to DataWay due to network issues, it caches up to 1000 data points to prevent data loss. Cache size can be controlled via configuration, with simple setup and low learning cost.

## Log Parsing Comparison

Log parsing is a core technology used in security teams, IT development teams, and business teams. Security teams extract log data to detect unknown security events and trace known events. Regulatory compliance is also a crucial requirement. IT teams use logs to discover and analyze known issues, focusing on system monitoring and APM (which includes all monitoring items of interest to developers). Business teams focus on risk control, operations promotion, user profiling, and website profiling. Log analysis transforms log information into valuable insights, reflecting the company's technical strength.

Logstash includes common parsers like Grok for text parsing, mutate for field transformations, drop for deleting events, clone for duplicating events, and geoip for adding geolocation data. Fluentd parses logs by filtering events based on field values, enriching events with new fields, and protecting privacy and compliance by deleting or masking certain fields. It has fewer plugins, only five: record_transformer, filter_stdout, filter_grep, parser, and filter_geoip. DataKit's log parsing includes Pipeline for splitting unstructured text data or extracting parts from structured text (like JSON), using glob rules for specifying log files, automatic discovery and file filtering, an interactive Grok matching tool to lower the Grok usage barrier, and support for numerous script functions to enhance data formatting flexibility.

### [Logstash Log Parsing](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html)
Grok is currently the best way in Logstash to parse unstructured log data into structured and queryable content. Logstash has 120 built-in Grok templates but lacks official support, including template performance tuning, which users must explore on their own.

![image.png](../images/guance-elk-efk-8.png)

### Fluentd Log Parsing

Fluentd's log parsing is similar to Logstash but more flexible in configuration. However, it lacks Grok templates and only provides configuration examples, requiring users to configure parsing based on documentation. This increases the learning curve and lacks technical support for configuration issues.

![image.png](../images/guance-elk-efk-9.png)

Using a sample server environment, take Nginx's access log as an example. A typical log entry is 365 bytes, structured into 14 fields:

![image.png](../images/guance-elk-efk-10.png)

In subsequent tests, the log will be repeatedly written to files under different pressures, with the time field of each log entry set to the current system time, while the other 13 fields remain the same.

Compared to real-world scenarios, there is no difference in log parsing in simulations. One distinction is that higher data compression rates reduce network write traffic.

### Logstash

Logstash version 7.1.0, using Grok to parse logs and write to Kafka (built-in plugin with gzip compression).

Log parsing configuration:
```
grok { 
patterns_dir=> "/home/admin/workspace/survey/logstash/patterns"
 
match=>{ "message"=>"%{IPORHOST:ip} %{USERNAME:rt} -
\"%{WORD:method} %{DATA:url}\" %{NUMBER:status} %{NUMBER:size} \"%{DATA:ref}\" \"%{DATA:agent}\" \"%{DATA:cookie_unb}\" \"%{DATA:cookie_cookie2}\" \"%{DATA:monitor_traceid}\" %{WORD:cell} %{WORD:ups} %{BASE10NUM:remote_port}" }
 
remove_field=>[ "message"]
}

```
Test results:

| **Write TPS** | **Write Traffic (KB/s)** | **CPU Usage (%)** | **Memory Usage (MB)** |
| --- | --- | --- | --- |
| 500 | 178.89 | 25.3 | 432 |
| 1000 | 346.65 | 46.9 | 476 |
| 5000 | 1882.23 | 231.1 | 489 |
| 10000 | 3564.45 | 511.2 | 512 |

### Fluentd

td-agent version 4.1.0, using regular expressions to parse logs and write to Kafka (third-party plugin fluent-plugin-kafka with gzip compression).

Log parsing configuration:

```
<source>
type tail
format /^(? <ip>\S+)\s(?<rt>\d+)\s-\s\[(?<time>[^\]]*)\]\s"(?<url>[^\"]+)"\s(?<status>\d+)\s(?<size>\d+)\s"(?<ref>[^\"]+)"\s"(?<agent>[^\"]+)"\s"(?<cookie_unb>\d+)"\s"(?<cookie_cookie2>\w+)"\s"(?
<monitor_traceid>\w+)"\s(?<cell>\w+)\s(?<ups>\w+)\s(?<remote_port>\d+).*$/
time_format %d/%b/%Y:%H:%M:%S %z
path /home/admin/workspace/temp/mock_log/access.log 
pos_file /home/admin/workspace/temp/mock_log/nginx_access.pos
tag nginx.access 
</source>

```
Test results:

| **Write TPS** | **Write Traffic (KB/s)** | **CPU Usage (%)** | **Memory Usage (MB)** |
| --- | --- | --- | --- |
| 500 | 174.272 | 13.8 | 58 |
| 1000 | 336.85 | 24.4 | 61 |
| 5000 | 1771.43 | 95.3 | 103 |
| 10000 | 3522.45 | 140.2 | 140 |

### DataKit

**DataKit-**1.1.8-rc3, using Pipeline to split unstructured text data.
```
# access log
grok(_, "%{NOTSPACE:ip} %{NOTSPACE:rt} - "%{NOTSPACE:method} %{NOTSPACE:url}\" %{NOTSPACE:status} %{NOTSPACE:size} \"%{NOTSPACE:ref}\" \"%{NOTSPACE:agent}\" \"%{NOTSPACE:cookie_unb}\" \"%{NOTSPACE:cookie_cookie2}\" \"%{NOTSPACE:monitor_traceid}\" %{NOTSPACE:cell} %{NOTSPACE:ups} %{NOTSPACE:remote_port}"）

cast(status_code, "int")
cast(bytes, "int")

default_time(time)
```
Test results:

| **Write TPS** | **Write Traffic (KB/s)** | **CPU Usage (%)** | **Memory Usage (MB)** |
| --- | --- | --- | --- |
| 500 | 178.24 | 8.5 | 41 |
| 1000 | 356.45 | 13.8 | 45 |
| 5000 | 1782.23 | 71.1 | 76 |
| 10000 | 3522.45 | 101.2 | 88 |

## Log Collection Architecture Comparison
### ELK Solution
#### Solution One
![image.png](../images/guance-elk-efk-11.png)

This is the simplest ELK architecture. Advantages include ease of setup and quick learning. Disadvantages include high resource consumption by Logstash, high CPU and memory usage, and potential data loss without a message queue. Users need to be proficient in Logstash, Elasticsearch, and Kibana to solve complex business problems and optimize cluster performance.

This architecture involves Logstash distributed across nodes collecting logs and data, analyzing and filtering them before sending to Elasticsearch for storage. Elasticsearch compresses and stores data in shards and provides APIs for queries. Users can configure Kibana Web for intuitive log queries and report generation.

#### Solution Two
![image.png](../images/guance-elk-efk-12.png)

This is a more mature ELK architecture. Advantages include Kafka's ability to store data temporarily if the Logstash cluster fails, preventing data loss. Disadvantages include complexity and difficulty in setting up and maintaining Kafka and possibly Zookeeper clusters. Users need to be proficient in Logstash, Elasticsearch, Kafka, and Kibana to solve complex business problems and optimize cluster performance.

This architecture introduces a message queue mechanism. Logstash Agents on various nodes send data/logs to Kafka (or Redis) before passing them indirectly to Logstash. Logstash filters and analyzes the data before sending it to Elasticsearch for storage. Finally, Kibana presents the logs and data to users. Kafka ensures data persistence even if the Logstash server fails.

### EFK Solution

#### Solution One

![image.png](../images/guance-elk-efk-13.png)

This is a more flexible EFK architecture. Advantages include lower resource consumption compared to Logstash and better scalability. Disadvantages include the need for a large Logstash cluster to handle log processing and proficiency in Logstash, Elasticsearch, and Kibana.

This architecture replaces Logstash with Filebeats for log collection and can configure Logstash and Elasticsearch clusters for large-scale operation and maintenance log data monitoring and querying.

#### Solution Two

![image.png](../images/guance-elk-efk-14.png)

Building on ELK, this solution uses Filebeat for log collection. Advantages include not needing a Java environment on each server since Filebeat has no dependencies. Disadvantages include complexity and the need to maintain Kafka and possibly Zookeeper clusters. Users need to be proficient in FileBeats, Logstash, Elasticsearch, Kafka, and Kibana.

This architecture defines a log_topic field in Filebeat's input to categorize log files from specified paths. Logs are sent to Kafka, which receives all logs from Filebeat clients and forwards them based on type (e.g., nginx, php, system). Kafka creates different topics based on the defined log types. Logstash receives messages from Kafka and writes logs to Elasticsearch based on Kafka topics. Kibana matches Elasticsearch indexes for log analysis, retrieval, and visualization.

#### Solution Three
![image.png](../images/guance-elk-efk-15.png)

This solution uses Fluentd for log collection. Advantages include lower resource consumption compared to Logstash. Disadvantages include complex configuration and higher learning curve. Users need to be proficient in Fluentd, Elasticsearch, and Kibana.

This architecture uses Fluentd to collect program logs, store them in the Elasticsearch cluster, and finally associate them with Kibana for log querying.

### Guance Architecture
![image.png](../images/guance-elk-efk-16.png)

DataKit is the foundational data collection tool, collecting various metrics and logs from system operations. Data is aggregated via Dataway to Guance, where users can view and analyze their data. DataKit is a critical component of Guance, with all data originating from DataKit.

DataKit deployment and configuration are simple and visual, supporting log data and APM data, infrastructure, containers, middleware, network performance, etc. DataKit does not require additional components like Elasticsearch or Kafka for functionality, allowing users to focus on business optimization. DataKit minimizes learning costs and complex configurations, solving complex business problems with simple setup. ELK and EFK have high overall operational costs, especially for Elasticsearch clusters. Using Guance eliminates these concerns, allowing users to concentrate on business development.

## Hardware Cost Comparison

Cost is a critical factor. We compare ELK, EFK, and Guance using cloud services.

### ELK Costs

Elastic components are open-source, with hardware costs being the primary expense. For collecting logs from 10 servers, each generating 1GB of logs daily.

LogStash Cluster + Kafka Cluster + Elasticsearch Cluster + Kibana

   - LogStash Cluster

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 1 x 2 cores, 4GB RAM | Monthly fee: 216.7 CNY/Month | 216.7 |
| Storage | 50GB | ESSD: 0.5 CNY/GB | 25 |
| Total | | | 241.7 |

   - Kafka Cluster

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 3 x 4 cores, 16GB RAM | Monthly fee: 788 CNY/Month | 2364 |
| Storage | 200GB | ESSD: 0.5 CNY/GB | 300 |
| Total | | | 2664 |

   - Elasticsearch Cluster

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 3 x 2 cores, 8GB RAM | Monthly fee: 383 CNY/Month | 1149 |
| Storage | 500GB | ESSD: 0.5 CNY/GB | 750 |
| Total | | | 1899 |

   - Kibana Node

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 1 x 1 core, 2GB RAM | Monthly fee: 104 CNY/Month | 104 |
| Storage | 50GB | ESSD: 0.5 CNY/GB | 25 |
| Total | | | 129 |

Total monthly cost for LogStash + Kafka + Elasticsearch + Kibana: 5175.4 CNY.

Total monthly cost for a simpler architecture without Kafka: 2511.4 CNY.

### EFK Costs

Elastic components are open-source, with hardware costs being the primary expense. For collecting logs from 10 servers, each generating 1GB of logs daily.

Fluentd + Elasticsearch Cluster + Kibana 

   - Fluentd 

Fluentd can be deployed without a separate cluster, so we only consider Elasticsearch + Kibana.

   - Elasticsearch Cluster

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 3 x 2 cores, 8GB RAM | Monthly fee: 383 CNY/Month | 1149 |
| Storage | 500GB | ESSD: 0.5 CNY/GB | 750 |
| Total | | | 1899 |

   - Kibana Node

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 1 x 1 core, 2GB RAM | Monthly fee: 104 CNY/Month | 104 |
| Storage | 50GB | ESSD: 0.5 CNY/GB | 25 |
| Total | | | 129 |

Total monthly cost for Fluentd + Elasticsearch + Kibana: 2028 CNY.

### Guance Costs

Guance charges based on storage usage, not product fees. DataKit collector count, log data volume, backup log data volume, daily task scheduling times, session counts for user access monitoring, trace counts for APM, etc., are priced accordingly. For collecting logs from 10 servers, each generating 1GB of logs daily.

| **Billing Item / Plan** | **Free Plan** | **Agile Plan** |
| --- | --- | --- |
| DataKit Count | Unlimited | 5 CNY/day |
| Time Series | Total time series < 500 | Single DataKit time series < 500, DataKit cost = DataKit count × base price<br />Single DataKit time series > 500, DataKit count calculated as:<br />- DataKit count = total workspace time series / 500 (round up)<br />- DataKit cost = DataKit count × base price |
| Log Data Volume | 2 million entries | 0.5 CNY/day (per 1 million entries) |
| Backup Log Data Volume | None | 0.2 CNY/day (per 1 million entries) |
| Trace Counts | 10,000 | 1 CNY/day (per 1 million entries) |
| Session Counts / PV Counts | 100 sessions | 1 CNY/day (per 100 sessions or per 1,000 PVs) |
| Cloud Dial Testing API Task Counts | 5 | 1 CNY/day (per 1,000 times) |
| Cloud Dial Testing Browser Task Counts | | 15 CNY/day (per 1,000 times) |
| Task Scheduling Counts | 5,000 | 1 CNY/day (per 10,000 times) |
| SMS Sending Counts | None | 0.1 CNY/day (per time) |

10 servers install DataKit, collecting 1GB of logs daily per server at 4K per entry.

| **Billing Item** | **Value** | **Unit Price** | **Cost (CNY)** |
| --- | --- | --- | --- |
| Server | 10 DataKits | Monthly fee: 150 CNY/Month | 1500 |
| Storage | 1GB daily | 0.5 CNY/day (per 1 million entries) | 325 |
| Total | | | 1825 |

Total monthly cost for Guance: 1825 CNY.

## Operations Cost Comparison

Speaking of operations costs, we know that maintaining cluster integrity is essential. Let's analyze the cost differences between different solutions.

### ELK Operations Costs

Since Elastic components are open-source, users need to build and manage their own clusters. For complex ELK architectures, including LogStash Cluster + Kafka Cluster + Elasticsearch Cluster + Kibana Node, if the business log volume is large and the computation logic is complex, the scale and configuration requirements for LogStash, Kafka, and Elasticsearch clusters are high. Additionally, skilled personnel are needed to handle various issues and perform performance optimizations. Maintaining optimal resource utilization and scaling the clusters also requires expertise.

### EFK Operations Costs

Similar to ELK, EFK is based on open-source components and requires users to build and manage clusters. EFK uses Fluentd for data collection, reducing the need for a LogStash cluster and resource consumption. However, it still requires dynamic expansion of the Elasticsearch cluster based on business scale, facing similar challenges as ELK in terms of large-scale cluster management and optimization. Fluentd configuration is more complex than LogStash, relying on user experience for configuration and optimization, which can impact server performance if not done correctly. Ensuring stable online operations demands higher skill levels from operations personnel.

### Guance Operations Costs

Guance is a SaaS-based observability platform. Users only need to deploy a DataKit on the servers they want to collect data from and use the visual management feature to configure the collection end remotely. Guance provides optimal log parsing templates to minimize server load and maximize performance. Users can focus on optimizing and expanding their business without worrying about collection end and log parsing cluster optimization. Guance offers complete observability from infrastructure, containers, middleware, databases, message queues, application traces, front-end visits, system security, and network performance. Users do not need to spend time studying or modifying immature open-source products, achieving zero operations costs and focusing entirely on business development.

## Learning Cost Comparison

The learning cost is an essential part of implementing or using a log analysis system. Let's see how easy it is to get started with different solutions.

### ELK Learning Costs

Since Elastic components are open-source, users need to build their own clusters. To analyze logs using ELK, users must learn about the ELK environment setup and component configuration, such as understanding Elasticsearch basics, mastering index operations, planning the Elasticsearch cluster, and optimizing performance for open-source versions of LogStash and Elasticsearch.

### EFK Learning Costs

Like ELK, EFK also requires learning these aspects. Fluentd is more flexible than LogStash, leading to higher learning costs. Performance optimization is more dependent on user experience. Unlike LogStash, which provides 120+ Grok templates, Fluentd requires more learning from users to effectively utilize it.

### Guance Learning Costs

For users, only DataKit needs to be deployed in the environment for data collection. Official configuration references and usage guides (covering 200+ technology stacks) are available. Users can perform log analysis, business observability, or trace tracking by learning the relevant modules of the Guance platform and enabling DataKit collection items. Avoiding the need to build open-source clusters for log analysis, users can focus more on business issues rather than spending time learning various technologies to ensure the operation of open-source clusters.

## User Experience Comparison

Comparing user experiences is important. How do different solutions differ when achieving the same functionality?

### ELK User Experience

To perform log analysis with ELK, users must set up LogStash, Kafka, Elasticsearch clusters, and Kibana display nodes. To collect and parse specific component data, users must check if the existing 120+ parsing templates meet their needs. If not, they must test to achieve data collection and parsing. Debugging performance consumption or slow parsing can be tedious. If log increments are too high, users face Elasticsearch query index optimization and frequent cluster expansion. To display specific business data, users must learn Kibana's KQL for data queries and displays. To implement real-time tracking and alerts, additional open-source components may be required. During the process, 80% of the time is spent dealing
# Best Practices for Full-Process Observability in E-commerce Orders

---

## Introduction

To meet the needs of rapid iteration and traffic surges, e-commerce systems often use microservices for development and deployment. Performance bottlenecks in a particular microservice can directly impact the customer shopping experience, especially when payment anomalies or order cancellations occur. To observe the entire order chain, it is helpful to track real-time paid orders, abnormal orders, and canceled orders using these metrics to analyze business bottlenecks. This best practice is based on a distributed e-commerce platform using Java, combined with Guance to monitor the number of successfully paid orders and analyze the reasons for unsuccessful payments from an order perspective.

## Prerequisites
### Install Datakit

- [Install Datakit](/datakit/datakit-install/)
## Data Ingestion

Order data is ingested into Guance via logs. Microservices output log files to cloud servers, and DataKit is installed on these servers to enable log collection by specifying the path to the log files. To parse out the order ID, user, and order status from the log files, you need to write a Pipeline script to split the log entries.

### Enable Input

1. Enable ddtrace

```shell
cd /usr/local/datakit/conf.d/ddtrace
cp ddtrace.conf.sample ddtrace.conf  
```

2. Write Pipeline Script

```shell
cd /usr/local/datakit/pipeline
vi log_book_order.p
```

In this script, **%{DATA:username}** represents the user, **%{DATA:order_no}** represents the order ID, and **%{DATA:order_status}** represents the order status.

```toml
#2021-12-22 10:09:53.443 [http-nio-7001-exec-7] INFO  c.d.s.b.s.i.OrderServiceImpl - [createOrder,164] - ecs009-book-order 7547183777837932733 2227975860088333788 test d6a3337d-ff82-4b00-9b4d-c07fb00c0cfb - User:test has placed an order, Order ID: d6a3337d-ff82-4b00-9b4d-c07fb00c0cfb


grok(_, "%{TIMESTAMP_ISO8601:time} %{NOTSPACE:thread_name} %{LOGLEVEL:status}%{SPACE}%{NOTSPACE:class_name} - \\[%{NOTSPACE:method_name},%{NUMBER:line}\\] - %{DATA:service1} %{DATA:trace_id} %{DATA:span_id} %{DATA:username} %{DATA:order_no} %{DATA:order_status} - %{GREEDYDATA:msg}")

default_time(time)

```

3. Enable Logging Plugin and Copy Sample File

```shell
cd /usr/local/datakit/conf.d/log
cp logging.conf.sample log_book_order.conf
```

Modify the `log_book_order.conf` file to specify the log files and the Pipeline script created in the previous step. Set the source to `log_book_order` for easy use in views.

```toml
[[inputs.logging]]
  ## required
  logfiles = [
    "/usr/local/df-demo/book-shop/logs/order/log.log"    
  ]

  ## glob filter
  ignore = [""]

  ## your logging source, if it's empty, use 'default'
  source = "log_book_order"

  ## add service tag, if it's empty, use $source.

  service = "book-store"

  ## grok pipeline script path
  pipeline = "log_book_order.p"

  ## optional status:
  ##   "emerg","alert","critical","error","warning","info","debug","OK"
  ignore_status = []

  ## optional encodings:
  ##    "utf-8", "utf-16le", "utf-16le", "gbk", "gb18030" or ""
  character_encoding = ""

  ## The pattern should be a regexp. Note the use of '''this regexp'''
  ## regexp link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
  multiline_match = '''^\d{4}-\d{2}-\d{2}'''

  ## removes ANSI escape codes from text strings
  remove_ansi_escape_codes = false

  [inputs.logging.tags]
    app = "book-shop"
  # some_tag = "some_value"
  # more_tag = "some_other_value"

```

4. Restart DataKit 

```shell
systemctl restart datakit
```
### E-commerce Data Ingestion

Project source code: [book-store](https://github.com/devdcores/BookStoreApp-Distributed-Application).

The logs processed by the Pipeline are generated by microservices, so you need to ensure that the user, order ID, and order status are included in the logs. In this example, Logback is used as the logging tool. To output business data through Logback, you need to use the MDC mechanism, i.e., put the user, order ID, and order status into MDC before logging, and then output them in the PATTERN section of `logback-spring.xml`. You need to modify the `bookstore-order-service` microservice code.

1. Create an Aspect

Create an aspect to add `userName`, `orderNo`, and `orderStatus` to MDC, and remove them after the request ends.

```java
@Component
@Aspect
public class LogAop {
    private static final String USER_NAME = "userName";
    private static final String ORDER_NO = "orderNo";
    private static final String ORDER_STATUS = "orderStatus";

    @Pointcut("execution(public * com.devd.spring.bookstoreorderservice.controller..*.*(..))")
    public void controllerCall() {
    }

    @Before("controllerCall()")
    public void logInfoBefore(JoinPoint jp) throws UnsupportedEncodingException {
        MDC.put(USER_NAME, "");
        MDC.put(ORDER_NO, "");
        MDC.put(ORDER_STATUS, "");
    }

    @AfterReturning(returning = "req", pointcut = "controllerCall()")
    public void logInfoAfter(JoinPoint jp, Object req) throws Exception {
        MDC.remove(USER_NAME);
        MDC.remove(ORDER_NO);
        MDC.remove(ORDER_STATUS);
    }

}
```

2. Write Order Data to Logs

Output logs after a successful order placement.

![1641289006.png](../images/order-1.png)

3. Configure `logback-spring.xml`

```xml
<property name="CONSOLE_LOG_PATTERN" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{20} - [%method,%line] - %X{dd.service} %X{dd.trace_id} %X{dd.span_id} %X{userName} %X{orderNo} %X{orderStatus} - %msg%n" />
```

### Build and Deploy

- Build the frontend, which will generate a `build` directory

```shell
cd bookstore-frontend-react-app
yarn build
```

- Build the backend, generating JAR files:

`bookstore-account-service-0.0.1-SNAPSHOT.jar`,
`bookstore-payment-service-0.0.1-SNAPSHOT.jar`,
`bookstore-api-gateway-service-0.0.1-SNAPSHOT.jar`,
`bookstore-billing-service-0.0.1-SNAPSHOT.jar`,
`bookstore-catalog-service-0.0.1-SNAPSHOT.jar`,
`bookstore-eureka-discovery-service-0.0.1-SNAPSHOT.jar`,
`bookstore-order-service-0.0.1-SNAPSHOT.jar`

```shell
mvn clean install -DskipTests
```

### Enable RUM

- Log in to [Guance](https://console.guance.com/)

Click on 【User Analysis】 -> 【New Application】, enter `book-shop`, select Web, and copy the JS snippet.

![image.png](../images/order-2.png)

- Copy the `build` directory to the server

Open `index.html`, paste the copied JS snippet into the `<head>` section, and modify the `datakitOrigin` value to the address of the DataKit deployed on the current cloud server, and set `allowedDDtracingOrgins` to the Gateway address.

![image.png](../images/order-3.png)

- Install Nginx and deploy the web project

```
server {
        listen       80;
        location / {            
            proxy_set_header   Host    $host:$server_port;
            proxy_set_header   X-Real-IP   $remote_addr;
            proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
			      root   /usr/local/df-demo/book-shop/build;
            index  index.html index.htm;
        }
}

```

### Enable APM

To collect Trace data with Guance, use `/usr/local/datakit/data/dd-java-agent.jar`.

```shell
java -jar bookstore-eureka-discovery-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-gateway \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-api-gateway-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-account \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-account-service-0.0.1-SNAPSHOT.jar  
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-order \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-order-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-billing \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-billing-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-payment \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-payment-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-catalog \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-catalog-service-0.0.1-SNAPSHOT.jar 
```

### Order Monitoring View

Log in to [Guance](https://console.guance.com/), go to 【Scenarios】 -> 【New Dashboard】 -> 【New Blank Dashboard】, enter "Order Monitoring View", and click 【Confirm】.

![1641360132(1).png](../images/order-4.png)

Click on the dashboard created in the previous step, click 【Edit】, drag a "Time Series Chart", set the title to "Number of Orders Placed", choose "Log" as the data source, then select `log_book_order`, which is the source value in `log_book_order.conf`. Select `order_no`, and choose "Count_distinct_by" as the aggregation method. Set the filter condition to "order_status" with the value "Placed". Finally, click "+" -> Transformation Function -> Cumsum to convert the order count into a cumulative total over time.

![1642059188(1).png](../images/order-5.png)

Drag another "Time Series Chart", set the title to "Number of Paid Orders", choose "Log" as the data source, then select `log_book_order`, which is the source value in `log_book_order.conf`. Select `order_no`, and choose "Count_distinct_by" as the aggregation method. Set the filter condition to "order_status" with the value "Paid". Finally, click "+" -> Transformation Function -> Cumsum to convert the order count into a cumulative total over time.

![1642059204(1).png](../images/order-6.png)

Drag another "Time Series Chart", set the title to "Number of Abnormal Orders", choose "Log" as the data source, then select `log_book_order`, which is the source value in `log_book_order.conf`. Select `order_no`, and choose "Count_distinct_by" as the aggregation method. Set the filter condition to "order_status" with the value "Payment Anomaly". Finally, click "+" -> Transformation Function -> Cumsum to convert the order count into a cumulative total over time.

![1642059222(1).png](../images/order-7.png)
# OpenTelemetry Observability

---

## Challenges in Building Observability
> 1. How to link front-end and back-end tracing?
>
> 2. How to associate trace data with corresponding logs and metrics?

1. OpenTelemetry provides SDKs for different programming languages. Front-end tracing is mainly implemented using `opentelemetry-js`, while back-end tracing is supported by implementations for languages like Java, Go, Python, etc. Trace information from different languages is uniformly reported to the opentelemetry-collector (referred to as `otel-collector`).

2. Taking Java as an example, opentelemetry-java (referred to as "Agent") is injected into applications via `javaagent`. After the application generates trace information, setting MDC allows the traceId and spanId to be passed as parameters to logs. This way, logs will include traceId and spanId when output.

Mapping Diagnostic Context (MDC) is 
> a tool used to distinguish interleaved log outputs from different sources.â€” [log4j MDC Documentation](http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/MDC.html)

It contains thread-local context information that is later copied into each log event captured by the logging library.

The OTel Java Agent injects several pieces of information about the current span into each log record's MDC copy:

> - `trace_id`- The current trace ID (same as `Span.current().getSpanContext().getTraceId()`).
> - `span_id`- The current span ID (same as `Span.current().getSpanContext().getSpanId()`).
> - `trace_flags`- The current trace flags, formatted according to the W3C Trace Context format (same as `Span.current().getSpanContext().getTraceFlags().asHex()`).

These three pieces of information can be included in log statements generated by the logging library by specifying them in the pattern/format.

Tip: For Spring Boot configurations using logback, you can add MDC to log lines by overriding only the following content in `logging.pattern.level`:

```properties
logging.pattern.level = trace_id=%mdc{trace_id} span_id=%mdc{span_id} trace_flags=%mdc{trace_flags} %5p
```

This way, any service or tool parsing application logs can correlate traces/spans with log statements.

3. OpenTelemetry also supports metric collection, exporting metrics through otel-collector to exporters such as Prometheus for visualization via Grafana. The otlpExporter supports metric export, and metrics can be associated with logs and traces using the `tag` `server.name`.

> The initial goal of OpenTelemetry was to unify data formats, indicating that for a long time, OpenTelemetry did not plan to focus on observability products. Instead, it aimed to serve as a data intermediary or use its data standards to constrain observability products.

## End-to-End Full Trace Observability Using OpenTelemetry

Below are three approaches to building end-to-end full trace observability using OpenTelemetry:

### [1. Traditional Monitoring Integration](./opentelemetry-elk.md)

Mainly involves using otel-collector to push logs, metrics, and traces to ELK, Prometheus, and related APM vendors such as Jaeger.

### [2. Grafana Ecosystem](./opentelemetry-grafana.md)

In recent years, Grafana has ventured into the observability domain, establishing Grafana Cloud and Grafana Labs, and offering its own observability solutions. Grafana Tempo is an open-source, easy-to-use, and scalable distributed tracing backend. Tempo is cost-effective, requiring only object storage to run, and integrates deeply with Grafana, Prometheus, and Loki. Tempo can directly receive trace data from OpenTelemetry, while Loki collects log data from OpenTelemetry, and Prometheus continues to receive metric data.

> Although these two approaches solve data format issues, they can only be considered technical solutions rather than products. They are essentially stitched together from various open-source tools. When encountering business problems, users still need to access different tools to analyze issues. Logs, metrics, and traces are not well integrated, which does not reduce operational and communication costs for operations and development personnel. A unified platform for log, metric, and trace data analysis is crucial. While Grafana is making efforts in this direction, it has not fully solved data silos. Different structured data still uses different query languages, and although Grafana has achieved log data correlation with trace data, trace data cannot be reverse-correlated with log data. The Grafana team still needs to work on mutual correlation and analysis between data types.

### [3. Guance - Commercial Observability Product](./opentelemetry-guance.md)

[Guance](https://www.guance.com) is a unified platform for collecting and managing metrics, logs, APM, RUM, infrastructure, containers, middleware, and network performance data. Using Guance provides comprehensive visibility into applications beyond just log tracing.

![image.png](../images/opentelemetry_observable_guance.png)

DataKit is the gateway for Guance. To send data to Guance, DataKit must be correctly configured, providing the following advantages:

> 1. In host environments, each host has a DataKit instance. Data is first sent to the local DataKit, cached, pre-processed, and then reported, mitigating network fluctuations while adding edge processing capabilities and reducing backend data processing pressure.
> 2. In k8 environments, each node has a DataKit daemonset. By leveraging k8s local traffic mechanisms, data from pods on each node is first sent to the local node's DataKit, mitigating network fluctuations and adding pod and node labels to APM data, facilitating location in distributed environments.

DataKit's design philosophy is inspired by OpenTelemetry, supporting the oltp protocol, allowing direct data reporting to DataKit without passing through the collector or setting the collector's exporter to oltp (DataKit).

### Solution Comparison
| **Scenario** | **Self-built Open Source Products** | **Using Guance** |
| --- | --- | --- |
| **Building Cloud-Native Monitoring Systems** | Requires at least 3 months of investment from a specialized technical team, just to start | Ready-to-use within 30 minutes |
| **Cost Investment** | Even a simple open-source monitoring product requires hardware investment exceeding $20,000/year, and cloud-native observability platforms require at least $100,000/year fixed investment (cloud hardware estimate) | Pay-as-you-go, based on actual business needs, with overall costs 50% lower than using open-source products |
| **System Maintenance and Management** | Requires continuous attention and investment from specialized engineers, increasing complexity with multiple open-source products | No need for concern, focusing on business issues |
| **Number of Probes Installed on Servers** | Each open-source software requires its own probe, consuming significant server performance | One probe, running entirely in binary mode, with minimal CPU and memory usage |
| **Value Delivered** | Depends solely on the company's engineers' capabilities and their proficiency with open-source products | Comprehensive data platform for full observability, enabling engineers to solve problems using data |
| **Root Cause Analysis for Performance and Failures** | Relies on the team's capabilities | Rapid root cause identification based on data analysis |
| **Security** | Various mixed open-source software challenges the engineers' comprehensive skills | Comprehensive security scanning and testing, customer-side code open-sourced, and timely product updates ensure security |
| **Scalability and Services** | Requires building an SRE engineer team | Provides professional services, equivalent to having an external SRE support team |
| **Training and Support** | Hiring external trainers | Continuous online training and support |
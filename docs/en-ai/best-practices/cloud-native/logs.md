# Guance Log Collection and Analysis Best Practices

---

## Log Collection

First, what are logs? Logs are text data generated by programs that follow a certain format (usually including timestamps).<br />Logs are typically generated by servers and output to different files, usually including system logs, application logs, security logs. These logs are stored on different machines. When the system fails, engineers need to log in to various servers and use Linux scripting tools such as grep / sed / awk to search the logs for causes of failure. In the absence of a logging system, the first step is to locate the server handling the request. If this server has multiple instances deployed, it will be necessary to look for log files under each application instance's log directory. Each application instance also sets up log rotation policies (such as generating one file per day or generating a new file when the log file reaches a given size), as well as log compression and archiving policies.<br />This series of processes can make it quite challenging for us to troubleshoot and promptly identify the cause of failure. Therefore, if we can centrally manage these logs and provide centralized search functionality, it not only improves diagnostic efficiency but also gives a comprehensive understanding of the system status, avoiding the passive situation of firefighting after an incident.<br />Thus, log data plays a very important role in the following areas:

- Data Search: By searching log information, locate corresponding bugs and find solutions;
- Service Diagnosis: By statistically analyzing log information, understand the server load and service operation status;
- Data Analysis: Further data analysis can be performed.

### Collecting File Logs

This article uses Nginx log collection as an example. Enter the `conf.d/log` directory under the DataKit installation directory, copy `logging.conf.sample` and rename it to `logging.conf`. The example is as follows:

```yaml
[[inputs.logging]]
  # List of log files, absolute paths can be specified, supports using glob rules for batch designation
  # It is recommended to use absolute paths
  logfiles = [
    "/var/log/nginx/access.log",                         
    "/var/log/nginx/error.log",                      
  ]
  
  # File path filtering, using glob rules, files matching any filter condition will not be collected
  ignore = [""]
  
  # Data source, if empty, 'default' will be used by default
  source = ""
  
  # New tag, if empty, $source will be used by default
  service = ""
  
  # Pipeline script path, if empty, $source.p will be used, if $source.p does not exist, pipeline will not be used
  pipeline = "nginx.p"
  
  # Filter corresponding statuses:
  #   `emerg`,`alert`,`critical`,`error`,`warning`,`info`,`debug`,`OK`
  ignore_status = []
  
  # Choose encoding, incorrect encoding may prevent data from being viewed. Default is empty:
  #    `utf-8`, `utf-16le`, `utf-16le`, `gbk`, `gb18030` or ""
  character_encoding = ""
  
  ## Set regular expressions, for example ^\d{4}-\d{2}-\d{2} matches YYYY-MM-DD time format at the beginning of line
  ## Data matching this regex will be considered valid data; otherwise, it will be appended to the end of the previous valid data
  ## Use three single quotes '''this-regexp''' to avoid escaping
  ## Regular expression link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
  # multiline_match = '''^\S'''

  ## Whether to remove ANSI escape codes, such as text colors in standard output
  remove_ansi_escape_codes = false
  
  # Custom tags
  [inputs.logging.tags]
   app = oa
```

Custom tags configuration allows filling in any key-value pairs.
● After configuration, all metrics will have the tag app = oa, enabling quick queries
● Related documentation < [DataFlux Tag Application Best Practices](../insight/tag.md)> 
Restart Datakit

```bash
systemctl restart datakit
```

#### Collecting Multi-line Logs

By identifying the characteristics of the first line of multi-line logs, you can determine whether a line of logs is a new log entry. If it does not match this characteristic, we assume that the current line is just an addition to the previous multi-line log.
For example, logs are usually written without indentation, but some log texts are indented, like stack traces during program crashes. For such log texts, they are considered multi-line logs. In DataKit, we use regular expressions to identify multi-line log features. Lines that match the regex are considered the start of a new log entry, and all subsequent non-matching lines are treated as additions to this new log until another line matches the regex.
To enable multi-line log collection, modify the following configuration in `logging.conf`:

```yaml
match = '''enter the specific regular expression here''' # Note, the regex should be enclosed with three single quotes on both sides
```

> Refer to the regular expression style used in the log collector [here](https://golang.org/pkg/regexp/syntax/#hdr-Syntax)

Here’s an example using Python logs:

```
2020-10-23 06:41:56,688 INFO demo.py 1.0
2020-10-23 06:54:20,164 ERROR /usr/local/lib/python3.6/dist-packages/flask/app.py Exception on /0 [GET]
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
ZeroDivisionError: division by zero
2020-10-23 06:41:56,688 INFO demo.py 5.0
```

Match configuration set to ^\d{4}-\d{2}-\d{2}.* (i.e., matching lines starting with a date like 2020-10-23)

#### Filtering Special Byte Codes in Logs

Logs may contain unreadable byte codes (like terminal color outputs), which can be filtered out by setting `remove_ansi_escape_codes` to `true` in `logging.conf`.
> Enabling this feature will slightly increase processing time

#### Collecting Remote Log Files

On Linux systems, you can use [NFS](https://linuxize.com/post/how-to-mount-an-nfs-share-in-linux/) to mount the log file paths from the host machine where the logs reside to the host machine where DataKit is installed. The Logging collector can then collect logs by configuring the corresponding log paths.

### Collecting Streaming Logs

This article uses collecting Fluentd logs as an example.
> Example Fluentd version is td-agent-4.2.x, configurations may vary across different versions.

#### DataKit Configuration

When collecting streaming logs, DataKit starts an HTTP Server to receive log text data and report it to Guance. The HTTP URL is fixed as `/v1/write/logstreaming`, i.e., `http://Datakit_IP:PORT/v1/write/logstreaming`
> Note: If DataKit is deployed as a DaemonSet in Kubernetes, you can access it via Service, with the address being `http://datakit-service.datakit:9529`

Enter the `conf.d/log` directory under the DataKit installation directory, copy `logstreaming.conf.sample` and rename it to `logstreaming.conf`. An example is as follows:

```yaml
[inputs.logstreaming]
  ignore_url_tags = true
```

Restart DataKit

```bash
systemctl restart datakit
```

##### Parameter Support

Logstreaming supports adding parameters in the HTTP URL to manipulate log data. The parameter list is as follows:

- `type`: Data format, currently only supports `influxdb`. 
   - When type is influxdb (`/v1/write/logstreaming?type=influxdb`), it indicates that the data itself is in line protocol format (default precision is s), only built-in Tags will be added, no other operations will be performed
   - When this value is empty, the data will be processed with line splitting and pipeline etc.
- `source`: Identifies the data source, i.e., the measurement in the line protocol. For example, nginx or redis (`/v1/write/logstreaming?source=nginx`)
   - When `type` is `influxdb`, this value is invalid
   - Defaults to `default`
- `service`: Adds a service label field, for example (`/v1/write/logstreaming?service=nginx_service`)
   - Defaults to the value of the source parameter.
- `pipeline`: Specifies the name of the pipeline to be used by the data, for example `nginx.p` (`/v1/write/logstreaming?pipeline=nginx.p`)

#### Fluentd Configuration

Using Fluentd to collect Nginx logs and forward them to the upper-level server's Plugin configuration as an example, instead of directly sending them to the server for processing, we want to process them directly and send them to DataKit for reporting to the Guance platform for analysis.

```yaml
##pc端日志收集
<source>
  @type tail
  format ltsv
  path /var/log/nginx/access.log
  pos_file /var/log/buffer/posfile/access.log.pos
  tag nginx
  time_key time
  time_format %d/%b/%Y:%H:%M:%S %z
</source>
 
## 收集的数据由tcp协议转发到多个server的49875端口

<match nginx>
 type forward
  <server>
   name es01
   host es01
   port 49875
   weight 60
  </server>
  <server>
   name es02
   host es02
   port 49875
   weight 60
  </server>
</match>
```
Modify the Output of Match to Http type and point the Endpoint to the DataKit address with Logstreaming enabled to complete the collection

```yaml
##pc端日志收集
<source>
  @type tail
  format ltsv
  path /var/log/nginx/access.log
  pos_file /var/log/buffer/posfile/access.log.pos
  tag nginx
  time_key time
  time_format %d/%b/%Y:%H:%M:%S %z
</source>
 
##collecting data forwarded via http protocol to local DataKit

## nginx output

<match nginx>
  @type http
  endpoint http://127.0.0.1:9529/v1/write/logstreaming?source=nginx_td&pipeline=nginx.p
  open_timeout 2
  <format>
    @type json
  </format>
</match>
```

After modifying the configuration, restart td-agent to complete data reporting

![image](../images/logs/1.png)

**You can verify the reported data through** [DQL](../../dql/define.md):

```bash
dql > L::nginx_td LIMIT 1
-----------------[ r1.nginx_td.s1 ]-----------------
    __docid 'L_c6et7vk5jjqulpr6osa0'
create_time 1637733374609
    date_ns 96184
       host 'df-solution-ecs-018'
    message '{"120.253.192.179 - - [24/Nov/2021":"13:55:10 +0800] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\" \"-\""}'
     source 'nginx_td'
       time 2021-11-24 13:56:06 +0800 CST
---------
1 rows, 1 series, cost 2ms
```

## Log Parsing (Pipeline)

Logs generated by general systems or services are often long strings with fields separated by spaces. Typically, logs are obtained as whole strings, and if each field meaning is split out for analysis, the presented data becomes clearer and more convenient for visualization.<br />Pipeline is an important component of Guance used for text data processing. Its main function is to convert text string formats into structured data, working in conjunction with Grok regular expressions.

### Grok Pattern Classification

In DataKit, Grok patterns can be categorized into two types: global patterns and local patterns. Patterns in the `pattern` directory are global patterns, available to all Pipeline scripts. Patterns added via the `add_pattern()` function within Pipeline scripts are local patterns, effective only for the current Pipeline script.<br />When DataKit's built-in patterns do not meet all user needs, users can add pattern files in the Pipeline directory to expand. If the custom pattern is global, create a new file in the `pattern` directory and add the pattern there. Do not add or modify existing built-in pattern files because DataKit startup process will overwrite these files.

#### Adding Local Patterns

Grok essentially predefines some regular expressions for text matching and extraction, naming these predefined regular expressions for ease of use and nested referencing to extend countless new patterns. For example, DataKit has the following three built-in patterns:

```python
_second (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)    # Matches seconds, _second is the pattern name
_minute (?:[0-5][0-9])                            # Matches minutes, _minute is the pattern name
_hour (?:2[0123]|[01]?[0-9])                      # Matches hours, _hour is the pattern name
```

Based on the above three built-in patterns, you can extend your own built-in pattern named `time`:

```python
# Add time to a file in the pattern directory, this pattern is a global pattern, usable anywhere
time ([^0-9]?)%{hour:hour}:%{minute:minute}(?::%{second:second})([^0-9]?)

# You can also add it to the pipeline file via add_pattern(), making it a local pattern, usable only in the current pipeline script
add_pattern(time, "([^0-9]?)%{HOUR:hour}:%{MINUTE:minute}(?::%{SECOND:second})([^0-9]?)")

# Extract the time field from the original input using grok. Assuming the input is 12:30:59, it extracts {"hour": 12, "minute": 30, "second": 59}
grok(_, %{time})
```

> Note:
> - Same pattern names prioritize script-level definitions (i.e., local patterns override global patterns)
> - In the pipeline script, `add_pattern()` must be called before `grok()` function, otherwise the first data extraction will fail.

### Configuring Nginx Log Parsing
#### Writing Pipeline Files
Write Pipeline files in the `<DataKit installation directory>/pipeline` directory, with the filename `nginx.p`.
```bash
add_pattern("date2", "%{YEAR}[./]%{MONTHNUM}[./]%{MONTHDAY} %{TIME}")

grok(_, "%{IPORHOST:client_ip} %{NOTSPACE:http_ident} %{NOTSPACE:http_auth} \\[%{HTTPDATE:time}\\] \"%{DATA:http_method} %{GREEDYDATA:http_url} HTTP/%{NUMBER:http_version}\" %{INT:status_code} %{INT:bytes}")

# Access log
add_pattern("access_common", "%{IPORHOST:client_ip} %{NOTSPACE:http_ident} %{NOTSPACE:http_auth} \\[%{HTTPDATE:time}\\] \"%{DATA:http_method} %{GREEDYDATA:http_url} HTTP/%{NUMBER:http_version}\" %{INT:status_code} %{INT:bytes}")
grok(_, '%{access_common} "%{NOTSPACE:referrer}" "%{GREEDYDATA:agent}')
user_agent(agent)

# Error log
grok(_, "%{date2:time} \\[%{LOGLEVEL:status}\\] %{GREEDYDATA:msg}, client: %{IPORHOST:client_ip}, server: %{IPORHOST:server}, request: \"%{DATA:http_method} %{GREEDYDATA:http_url} HTTP/%{NUMBER:http_version}\", (upstream: \"%{GREEDYDATA:upstream}\", )?host: \"%{IPORHOST:ip_or_host}\"")
grok(_, "%{date2:time} \\[%{LOGLEVEL:status}\\] %{GREEDYDATA:msg}, client: %{IPORHOST:client_ip}, server: %{IPORHOST:server}, request: \"%{GREEDYDATA:http_method} %{GREEDYDATA:http_url} HTTP/%{NUMBER:http_version}\", host: \"%{IPORHOST:ip_or_host}\"")
grok(_,"%{date2:time} \\[%{LOGLEVEL:status}\\] %{GREEDYDATA:msg}")

group_in(status, ["warn", "notice"], "warning")
group_in(status, ["error", "crit", "alert", "emerg"], "error")

cast(status_code, "int")
cast(bytes, "int")

group_between(status_code, [200,299], "OK", status)
group_between(status_code, [300,399], "notice", status)
group_between(status_code, [400,499], "warning", status)
group_between(status_code, [500,599], "error", status)


nullif(http_ident, "-")
nullif(http_auth, "-")
nullif(upstream, "")
default_time(time)
```

> Note, during the parsing process, avoid [possible conflicts with tag keys (Pipeline Field Naming Considerations)](../../datakit/datakit-pl-how-to.md#5cf855c0)

#### Debugging Pipeline Files

Since there are numerous Grok Patterns, manual matching can be cumbersome. DataKit provides an interactive command-line tool `grokq` (Grok Query):

```shell
datakit --grokq
grokq > Mon Jan 25 19:41:17 CST 2021   # Enter the text you wish to match here
        2 %{DATESTAMP_OTHER: ?}        # The tool suggests corresponding matches, the closer to the top, the more precise (higher weight). The number before indicates the weight.
        0 %{GREEDYDATA: ?}

grokq > 2021-01-25T18:37:22.016+0800
        4 %{TIMESTAMP_ISO8601: ?}      # Here, ? indicates you need to name the matched text with a field
        0 %{NOTSPACE: ?}
        0 %{PROG: ?}
        0 %{SYSLOGPROG: ?}
        0 %{GREEDYDATA: ?}             # Patterns like GREEDYDATA have broad ranges, thus lower weights
                                       # Higher weight means higher precision

grokq > Q                              # Q or exit to quit
Bye!
```

After writing the Pipeline file using DataKit's provided command-line tool `grokq`, specify the Pipeline script name (using --pl, Pipeline script must be placed in <DataKit installation directory>/pipeline), enter a piece of text (--txt) to check if extraction was successful

```shell
# Successful extraction example
datakit --pl nginx.p --txt '172.17.0.1 - - [06/Jan/2017:16:16:37 +0000] "GET /datadoghq/company?test=var1%20Pl HTTP/1.1" 401 612 "http://www.perdu.com/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" "-"'
Extracted data(cost: 5.279203ms):  # Indicates successful parsing
{
  "agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\" \"-\"",
  "browser": "Chrome",
  "browserVer": "55.0.2883.87",
  "bytes": 612,
  "client_ip": "172.17.0.1",
  "engine": "AppleWebKit",
  "engineVer": "537.36",
  "http_method": "GET",
  "http_url": "/datadoghq/company?test=var1%20Pl",
  "http_version": "1.1",
  "isBot": false,
  "isMobile": false,
  "message": "172.17.0.1 - - [06/Jan/2017:16:16:37 +0000] \"GET /datadoghq/company?test=var1%20Pl HTTP/1.1\" 401 612 \"http://www.perdu.com/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\" \"-\"",
  "os": "Linux x86_64",
  "referrer": "http://www.perdu.com/",
  "status": "warning",
  "status_code": 401,
  "time": 1483719397000000000,
  "ua": "X11"
}

# Failed extraction example
datakit --pl nginx.p --txt '172.17.0.1 - - [06/Jan/2017:16:16:37 +0000] "GET /datadoghq/company?test=var1%20Pl HTTP/1.1" 401 612 "http://www.perdu.com/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" "-"'
No data extracted from pipeline
```

#### Configuring Collector to Apply Pipeline Script

##### Text Log Collection Configuration with Pipeline Script

Taking Nginx log collection as an example, in the Logging collector, configure the Pipeline field. Note that here you configure the name of the Pipeline script, not the path. All referenced Pipeline scripts must be stored in the <DataKit installation directory/pipeline> directory:

```yaml
[inputs.logging]]
  # List of log files, can specify absolute paths, supports using glob rules for batch designation
  # Recommended to use absolute paths
  logfiles = [
    "/var/log/nginx/access.log",                         
    "/var/log/nginx/error.log",                      
  ]
  
  # File path filtering, using glob rules, files matching any filter condition will not be collected
  ignore = [""]
  
  # Data source, if empty, defaults to 'default'
  source = ""
  
  # New tag, if empty, defaults to $source
  service = ""
  
  # Pipeline script path, if empty, uses $source.p, if $source.p does not exist, pipeline is not used
  pipeline = "nginx.p"
  
  # Filter corresponding statuses:
  #   `emerg`,`alert`,`critical`,`error`,`warning`,`info`,`debug`,`OK`
  ignore_status = []
  
  # Choose encoding, incorrect encoding may prevent data from being viewed. Default is empty:
  #    `utf-8`, `utf-16le`, `utf-16le`, `gbk`, `gb18030` or ""
  character_encoding = ""
  
  ## Set regular expressions, for example ^\d{4}-\d{2}-\d{2} matches YYYY-MM-DD time format at the beginning of line
  ## Data matching this regex will be considered valid data; otherwise, it will be appended to the end of the previous valid data
  ## Use three single quotes '''this-regexp''' to avoid escaping
  ## Regular expression link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
  # multiline_match = '''^\S'''

  ## Whether to remove ANSI escape codes, such as text colors in standard output
  remove_ansi_escape_codes = false
  
  # Custom tags
  [inputs.logging.tags]
   app = oa
```

Restart Datakit to parse the corresponding logs.

```shell
systemctl restart datakit
```

##### Stream Log Collection Configuration with Pipeline Script

Taking Fluentd log collection as an example, modify the Output of Match to Http type and point the Endpoint to the DataKit address with Logstreaming enabled and configure the Pipeline script name to complete the collection.

```yaml
##pc端日志收集
<source>
  @type tail
  format ltsv
  path /var/log/nginx/access.log
  pos_file /var/log/buffer/posfile/access.log.pos
  tag nginx
  time_key time
  time_format %d/%b/%Y:%H:%M:%S %z
</source>
 
##collecting data forwarded via http protocol to local DataKit
## nginx output
<match nginx>
  @type http
  endpoint http://127.0.0.1:9529/v1/write/logstreaming?source=nginx_td&pipeline=nginx.p
  open_timeout 2
  <format>
    @type json
  </format>
</match>
```
After modifying the configuration, restart td-agent to complete data reporting

## Log Collection Performance Optimization

### Why Is My Pipeline Running Very Slowly?

Performance issues are often discussed. Users typically notice that using Grok expressions slows down the speed of Pipeline log processing. Grok patterns are based on regular expressions, possibly due to overly broad Grok variables used while writing Pipelines or performing full-line matching leading to slow processing speeds.

### Be Mindful of Expressions That Match Twice

We've seen many Grok patterns encounter problems when processing multiple application logs from a single gateway, such as Syslog. Imagine a scenario where we record three application logs using the format `"common_header: payload"`
```
Application 1: '8.8.8.8 process-name[666]: a b 1 2 a lot of text at the end'
Application 2: '8.8.8.8 process-name[667]: a 1 2 3 a lot of text near the end;4'
Application 3: '8.8.8.8 process-name[421]: a completely different format | 1111'
```
Typically, we handle all three logs in a single Pipeline
```
grok(_ , "%{IPORHOST:clientip} %{DATA:process_name}\[%{NUMBER:process_id}\]: %{WORD:word_1} %{WORD:word_2} %{NUMBER:number_1} %{NUMBER:number_2} %{DATA:data}")
grok(_ , "%{IPORHOST:clientip} %{DATA:process_name}\[%{NUMBER:process_id}\]: %{WORD:word_1} %{NUMBER:number_1} %{NUMBER:number_2} %{NUMBER:number_3} %{DATA:data};%{NUMBER:number_4}")
grok(_ , "%{IPORHOST:clientip} %{DATA:process_name}\[%{NUMBER:process_id}\]: %{DATA:data} | %{NUMBER:number}")
```
However, even if the log matches normally, Grok still tries to match incoming logs in sequence. It breaks the loop once it encounters a successful match. This requires us to judge how to place the patterns appropriately, otherwise, it will try one by one, especially for different formats. A common optimization solution is to use layered matching to optimize the Pipeline
```
add_pattern("message", "%{IPORHOST:clientip} %{DATA:process_name}\[%{NUMBER:process_id}\]: %{GREEDYDATA:message}")

grok(_, "%{message} %{WORD:word_1} %{WORD:word_2} %{NUMBER:number_1} %{NUMBER:number_2} %{GREEDYDATA:data}")
grok(_, "%{message} %{WORD:word_1} %{NUMBER:number_1} %{NUMBER:number_2} %{NUMBER:number_3} %{DATA:data};%{NUMBER:number_4}")
grok(_, "%{message} %{DATA:data} | %{NUMBER:number}")

```

### Be Mindful of High-Cost Grok Expressions

Consider the following Nginx log line

```
172.17.0.1 - - [06/Jan/2017:16:16:37 +0000] "GET /datadoghq/company?test=var1%20Pl HTTP/1.1" 401 612 "http://www.perdu.com/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" "-"
```
We usually handle this using flexible Grok expressions in a Pipeline
```
grok(_, "%{IPORHOST:client_ip} %{NOTSPACE:http_ident} %{NOTSPACE:http_auth} \\[%{HTTPDATE:time}\\] \"%{DATA:http_method} %{GREEDYDATA:http_url} HTTP/%{NUMBER:http_version}\" %{INT:status_code} %{INT:bytes}")

cast(status_code, "int")
cast(bytes, "int")
```

Here `%{IPORHOST:client_ip} --> 172.17.0.1` incurs significant performance overhead because Grok expressions are converted to regular expressions, and the more scenarios covered by a Grok expression, the worse its performance might be. Let's examine the complex regular expression underlying `%{IPORHOST:client_ip}`

```
IPORHOST (?:%{IP}|%{HOSTNAME})
HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
IP (?:%{IPV6}|%{IPV4})
IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
IPV4 (?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])
```

We can see that a short Grok expression can contain so many complex regular expressions. Clearly, when handling large volumes of logs, using such complex Grok expressions can significantly impact performance. How can we optimize it?

```
grok(_, "%{NOTSPACE:client_ip} %{NOTSPACE:http_ident} %{NOTSPACE:http_auth} \\[%{HTTPDATE:time}\\] \"%{DATA:http_method} %{GREEDYDATA:http_url} HTTP/%{NUMBER:http_version}\" %{INT:status_code} %{INT:bytes}")

cast(status_code, "int")
cast(bytes, "int")

default_time(time)
```

Focus on performance, prefer using `%{NOTSPACE:}`, since Grok expressions are converted to regular expressions, the more scenarios covered by a Grok expression, the worse its performance might be. Conversely, simple variables like `%{NOTSPACE:}` (non-space) have high performance, so choose `%{NOTSPACE:}` to enhance Pipeline performance when you can confirm the data is non-space and adjacent to whitespace.

### Better Utilization of Tools for Writing Pipelines

#### DataKit - Interactive Command-Line Tool grokq

Due to the numerous Grok Patterns, manual matching can be cumbersome. DataKit provides an interactive command-line tool `grokq` (Grok Query):

```shell
datakit --grokq
grokq > Mon Jan 25 19:41:17 CST 2021   # Enter the text you wish to match here
        2 %{DATESTAMP_OTHER: ?}        # The tool suggests corresponding matches, the closer to the top, the more precise (higher weight). The number before indicates the weight.
        0 %{GREEDYDATA: ?}

grokq > 2021-01-25T18:37:22.016+0800
        4 %{TIMESTAMP_ISO8601: ?}      # Here, ? indicates you need to name the matched text with a field
        0 %{NOTSPACE: ?}
        0 %{PROG: ?}
        0 %{SYSLOGPROG: ?}
        0 %{GREEDYDATA: ?}             # Patterns like GREEDYDATA have broad ranges, thus lower weights
                                       # Higher weight means higher precision

grokq > Q                              # Q or exit to quit
Bye!
```

#### DataKit - Pipeline Script Testing

After writing the Pipeline file using DataKit
# Pipeline Data Processing for Various Categories

[:octicons-beaker-24: Experimental](index.md#experimental)

---

Since DataKit 1.4.0, the built-in Pipeline feature can directly manipulate data collected by DataKit, supporting all [data types](../../datakit/apis.md#category).

<!-- markdownlint-disable MD046 -->
???+ attention

    - The Pipeline applies to all data and is currently in the experimental phase. There is no guarantee that incompatible changes will not be made to the mechanism or behavior later.
    - Even data reported via the [DataKit API](../../datakit/apis.md) supports Pipeline processing.
    - Using Pipeline to process existing collected data (especially non-log data) may destroy the existing data structure, leading to abnormal behavior in Guance.
    - Before applying a Pipeline, please confirm whether the data processing meets expectations using the [Pipeline debugging tool](pipeline-quick-start.md#debug).
<!-- markdownlint-enable -->

The Pipeline can perform the following operations on data collected by DataKit:

- Add, delete, modify the values or data types of fields and tags
- Convert fields to tags
- Modify the name of a measurement
- Mark the current data for discard ([drop()](pipeline-built-in-function.md#fn-drop))
- Terminate the execution of the Pipeline script ([exit()](pipeline-built-in-function.md#fn-exit))
- ...

## Input Data Structure {#input-data-struct}

All categories of data are encapsulated into a Point structure before being processed by the Pipeline script. Its structure can be viewed as:

``` not-set
struct Point {
   Name:      str          # Equivalent to the name of the Metrics (time series) data's measurement set, Logging (log) data's source,
                              # Network data's source, Object/CustomObject (object) data's class ...
   Tags:      map[str]str  # Stores all tags of the data; for non-time series category data, the boundary between tags and fields is blurry
   Fields:    map[str]any  # Stores all fields of the data (referred to as metrics for time series data)
   Time:      int64        # Represents the timestamp of the data, typically interpreted as the timestamp when the data was generated, in nanoseconds
   DropFlag:  bool         # Indicates whether this data should be discarded
}
```

For example, an nginx log entry collected by the log collector would be represented as follows when input into the Pipeline script:

``` not-set
Point {
    Name: "nginx"
    Tags: map[str]str {
        "host": "your_hostname"
    },
    Fields: map[str]any {
        "message": "127.0.0.1 - - [12/Jan/2023:11:51:38 +0800] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.81.0\""
    },
    Time: 1673495498000123456,
    DropFlag: false,
}
```

Note:

- The `name` can be modified using the function `set_measurement()`.

- In the point's tags/fields map, **no key will appear simultaneously in both tags and fields**;

- You can read the corresponding key's value from the point's tags/fields map using custom identifiers or the `get_key()` function within the Pipeline; modifying keys in Tags or Fields requires other built-in functions like `add_key`; `_` can be considered an alias for the key `message`.

- After the script runs, if there is a key named `time` in the point's tags/fields map, it will be deleted; if its value is of type int64, it will be assigned to the point's time and then deleted. If `time` is a string, you can attempt to use the `default_time()` function to convert it to int64.

- You can mark an input Point for discard using the `drop()` function, ensuring that the data will not be uploaded after the script executes.

## Storage, Indexing, and Matching of Pipeline Scripts {#script-store-index-match}

### Script Storage and Indexing {#store-and-index}

Currently, Pipeline scripts are divided into four namespaces based on their source, with decreasing indexing priority as shown in the table below:

| Namespace | Directory | Supported Data Types | Description |
| --- | --- | --- | --- |
| `remote`  | *[DataKit installation directory]/pipeline_remote*                  | CO, E, L, M, N, O, P, R, S, T | Scripts managed by the Guance console            |
| `confd`   | *[DataKit installation directory]/pipeline_confd*                    | CO, E, L, M, N, O, P, R, S, T | Scripts managed by Confd                |
| `gitrepo` | *[DataKit installation directory]/pipeline_gitrepos/[repo-name]*  | CO, E, L, M, N, O, P, R, S, T | Scripts managed by Git                  |
| `default` | *[DataKit installation directory]/pipeline*                         | CO, E, L, M, N, O, P, R, S, T | Scripts generated by DataKit or written by users |

Note:

- Do not modify the automatically generated default scripts under the *pipeline* directory; any modifications will be overwritten upon DataKit startup.
- It is recommended to add local scripts corresponding to specific data types under the *pipeline/[category]/* directory.
- Except for the *pipeline* directory, do not make any modifications to other script directories (*remote*, *confd*, *gitrepo*).

When DataKit selects the corresponding Pipeline, the indexing priority of scripts within these four namespaces decreases in order. For example, when needing *metric/cpu.p*, DataKit searches in the following order:

1. `pipeline_remote/metric/cpu.p`
2. `pipeline_confd/metric/cpu.p`
3. `gitrepo/<repo-name>/metric/cpu.p`
4. `pipeline/metric/cpu.p`

> Note: `<repo-name>` depends on your git repository name.

We create separate indexes for each data category's scripts. This feature does not allow cross-namespace script references via the `use()` function. The implementation of Pipeline script storage and indexing is illustrated in the diagram below, where higher-priority namespace scripts shadow lower-priority ones during index creation:

![script-index](img/pipeline-script-index.drawio.png)

All four sources of Pipeline directories store Pipeline scripts in the following manner:

``` not-set
├── pattern   <-- Directory specifically for custom patterns
├── apache.p
├── consul.p
├── sqlserver.p        <--- All top-level directories' Pipelines default to logs for backward compatibility
├── tomcat.p
├── other.p
├── custom_object      <--- Directory specifically for custom object pipelines
│   └── some-object.p
├── keyevent           <--- Directory specifically for event pipelines
│   └── some-event.p
├── logging            <--- Directory specifically for log pipelines
│   └── nginx.p
├── metric             <--- Directory specifically for time series metric pipelines
│   └── cpu.p
├── network            <--- Directory specifically for network metric pipelines
│   └── ebpf.p
├── object             <--- Directory specifically for object pipelines
│   └── HOST.p
├── rum                <--- Directory specifically for RUM pipelines
│   └── error.p
├── security           <--- Directory specifically for Security Check pipelines
│   └── scheck.p
└── tracing            <--- Directory specifically for APM pipelines
    └── service_a.p
```

### Data and Script Matching Strategy {#match}

The matching strategy between data and script names consists of four rules, evaluated from highest (Rule 4) to lowest (Rule 1) priority. If a higher-priority rule is satisfied, lower-priority rules are not evaluated:

1. Generate a data characteristic string based on the input data, append the Pipeline script file extension `.p`, and search for the corresponding category script.
2. Use the default script set in the Guance console for all data of that category.
3. Use the data-to-script mapping defined in the Guance console.
4. Use the script specified in the collector configuration file.

All data and script matching strategies depend on the data characteristic string, which varies by data category:

1. Generating the data characteristic string using specific point tags/fields:
   - For APM Tracing and Profiling data:
       - Use the value of `service` in tags/fields to generate the data characteristic string. For example, if DataKit collects data with `service` value `service-a`, it generates `service-a`, corresponding to the script name `service-a.p`, which is then searched in the *Tracing/Profiling* script index.
   - For Security Check data:
       - Use the value of `category` in tags/fields to generate the data characteristic string. For example, if DataKit receives Security data with `category` value `system`, it generates `system`, corresponding to the script name `system.p`.

2. Generating the data characteristic string using specific point tags/fields and point name:
   - For RUM data:
     - Use the value of `app_id` in tags/fields and the point name to generate the data characteristic string. For example, with a point name of `action`, it generates `<app_id>_action`, corresponding to the script name `<app_id>_action.p`.

3. Generating the data characteristic string using the point name:
   - For Logging/Metric/Network/Object/... and other categories:
     - Use the point name to generate the data characteristic string. For example, for the time series measurement `cpu`, it generates `cpu`, corresponding to the script `cpu.p`. For object data with class `HOST`, it generates `HOST`, corresponding to the script `HOST.p`.

## Pipeline Processing Examples {#examples}

> Example scripts are for reference only; please write according to your needs.

### Processing Time Series Data {#M}

The following example demonstrates how to modify tags and fields using Pipeline. Through DQL, we can see the fields of a CPU measurement set as follows:

```shell
dql > M::cpu{host='u'} LIMIT 1
-----------------[ r1.cpu.s1 ]-----------------
core_temperature 76
             cpu 'cpu-total'
            host 'u'
            time 2022-04-25 12:32:55 +0800 CST
     usage_guest 0
usage_guest_nice 0
      usage_idle 81.399796
    usage_iowait 0.624681
       usage_irq 0
      usage_nice 1.695563
   usage_softirq 0.191229
     usage_steal 0
    usage_system 5.239674
     usage_total 18.600204
      usage_user 10.849057
---------
```

Write the following Pipeline script:

```python
# file pipeline/metric/cpu.p

set_tag(script, "metric::cpu.p")
set_tag(host2, host)
usage_guest = 100.1
```

After restarting DataKit, new data is collected, and through DQL, we can get the modified CPU measurement set as follows:

```shell
dql > M::cpu{host='u'}[20s] LIMIT 1
-----------------[ r1.cpu.s1 ]-----------------
core_temperature 54.250000
             cpu 'cpu-total'
            host 'u'
           host2 'u'                        <--- New tag
          script 'metric::cpu.p'            <--- New tag
            time 2022-05-31 12:49:15 +0800 CST
     usage_guest 100.100000                 <--- Modified field value
usage_guest_nice 0
      usage_idle 94.251269
    usage_iowait 0.012690
       usage_irq 0
      usage_nice 0
   usage_softirq 0.012690
     usage_steal 0
    usage_system 2.106599
     usage_total 5.748731
      usage_user 3.616751
---------
```

### Processing Object Data {#O}

The following Pipeline example demonstrates how to discard (filter) data. Taking the Nginx process on the current host as an example, the Nginx process list is as follows:

```shell
$ ps axuwf | grep  nginx
root        1278  0.0  0.0  55288  1496 ?        Ss   10:10   0:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;
www-data    1279  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1280  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1281  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1282  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1283  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1284  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1286  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1287  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
```

Through DQL, we can see the fields of a specific process measurement set as follows:

```shell
dql > O::host_processes:(host, class, process_name, cmdline, pid) {host='u', pid=1278}
-----------------[ r1.host_processes.s1 ]-----------------
       class 'host_processes'
     cmdline 'nginx: master process /usr/sbin/nginx -g daemon on; master_process on;'
        host 'u'
         pid 1278
process_name 'nginx'
        time 2022-05-31 14:19:15 +0800 CST
---------
```

Write the following Pipeline script:

```python
if process_name == "nginx" {
    drop()  # The drop() function marks this data for discard and continues running the pl
    exit()  # The exit() function terminates Pipeline execution
}
```

After restarting DataKit, the corresponding Nginx process objects will no longer be collected (the central object has an expiration policy, so wait 5-10 minutes for the original Nginx object to expire automatically).
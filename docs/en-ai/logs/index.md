---
icon: zy/logs
---
# Logs
---

<video controls="controls" poster="https://<<< custom_key.static_domain >>>/dataflux/help/video/log.png" >
      <source id="mp4" src="https://<<< custom_key.static_domain >>>/dataflux/help/video/log.mp4" type="video/mp4">
</video>

In modern infrastructure, thousands of log events may be generated every minute. These logs follow specific formats, typically including timestamps and are generated by servers. They are output to different files such as system logs, application logs, and security logs. However, these logs are currently stored in a decentralized manner across various servers, leading to the need to log into each server separately to review logs when a system failure occurs, in order to determine the cause of the failure. This process increases the complexity of troubleshooting.

Faced with such a large volume of data, you need to decide which logs should be sent to a log management solution and which should be archived. Filtering logs before sending them might result in missing critical information or inadvertently deleting valuable data.

To improve the efficiency of fault diagnosis and gain a comprehensive understanding of the system status, avoiding passive responses in emergencies, it is crucial to centralize log management and provide centralized search and correlation analysis functions.

<<< custom_key.brand_name >>>, through its powerful log collection capabilities, allows you to unify log data reporting to <<< custom_key.brand_name >>> workspaces. This way, you can perform centralized storage, auditing, monitoring, alerting, analysis, and export operations on collected log data, thereby simplifying the log management process.

Through this method, <<< custom_key.brand_name >>> helps you avoid issues that may arise from filtering logs before sending them, ensuring that all critical information is properly processed and analyzed.


## Features


<div class="grid cards" markdown>

- [:material-clipboard-text-search:{ .lg .middle } __Query & Analysis__](explorer.md)

    ---
    
    Automatically identify log status, quickly filter and correlate logs, aggregate similar text, helping to swiftly discover and analyze anomalies, accelerating troubleshooting

- [:material-book-arrow-down-outline:{ .lg .middle } __Pipelines__](../pipeline/index.md)

    ---

    Parse the textual content of logs, converting them into structured data, including extracting timestamps, statuses, and specific fields as labels

- [:fontawesome-brands-atlassian:{ .lg .middle } __Generate Metrics__](generate-metrics.md)

    ---

    Generate new metric data based on existing data within the current workspace, enabling the design and implementation of new technical metrics as needed

- [:material-calendar-text-outline:{ .lg .middle } __Log Indexing__](./multi-index/index.md)

    ---

    Archive log data that meets certain criteria in different indexes and select data storage policies for log indexes

- [:material-filter-multiple:{ .lg .middle } __Log Blacklist__](../management/overall-blacklist.md)  

    ---

    Customize filtering rules for log collection; log data that meets the criteria will not be reported to <<< custom_key.brand_name >>>, helping to save on log storage costs

- [:material-clipboard-check-multiple-outline:{ .lg .middle } __Data Forwarding__](../management/backup/index.md)
    
    ---

    Save logs, traces, and user access data to <<< custom_key.brand_name >>> object storage or forward them to external storage, managing forwarded data flexibly

- [:material-database-check:{ .lg .middle } __Data Access__](../management/logdata-access.md)

    ---

    Control access to log data more precisely by setting role-based access permissions and data masking rules, while handling sensitive information appropriately
      
</div>
apiVersion: v1
kind: Namespace
metadata:
  name: datakit

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: datakit
rules:
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterroles"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "namespaces", "pods", "pods/log", "events", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "statefulsets", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: [ "get", "list", "watch"]
- apiGroups: ["guance.com"]
  resources: ["datakits"]
  verbs: ["get","list"]
- apiGroups: ["monitoring.coreos.com"]
  resources: ["podmonitors", "servicemonitors"]
  verbs: ["get", "list"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: datakits.guance.com
spec:
  group: guance.com
  versions:
    - name: v1beta1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                instances:
                  type: array
                  items:
                    type: object
                    properties:
                      k8sNamespace:
                        type: string
                      k8sDeployment:
                        type: string
                      datakit/logs:
                        type: string
                      inputConf:
                        type: string
  scope: Namespaced
  names:
    plural: datakits
    singular: datakit
    kind: Datakit
    shortNames:
    - dk

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: datakit
  namespace: datakit

---

apiVersion: v1
kind: Service
metadata:
  name: datakit-service
  namespace: datakit
spec:
  selector:
    app: daemonset-datakit
  ports:
    - protocol: TCP
      port: 9529
      targetPort: 9529

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: datakit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: datakit
subjects:
- kind: ServiceAccount
  name: datakit
  namespace: datakit

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: daemonset-datakit
  name: datakit
  namespace: datakit
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: daemonset-datakit
  template:
    metadata:
      labels:
        app: daemonset-datakit
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: ENV_K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: ENV_DATAWAY
          value: https://openway.guance.com?token=tkn_83b339919xxxxxxxxxc052101
        - name: ENV_GLOBAL_TAGS
          value: host=__datakit_hostname,host_ip=__datakit_ip
        - name: ENV_DEFAULT_ENABLED_INPUTS
          value: self,cpu,disk,diskio,mem,swap,system,hostobject,net,host_processes,container
        - name: ENV_ENABLE_ELECTION
          value: enable
        - name: ENV_LOG
          value: stdout
        - name: ENV_HTTP_LISTEN
          value: 0.0.0.0:9529
        - name: HOST_PROC
          value: /rootfs/proc
        - name: HOST_SYS
          value: /rootfs/sys
        - name: HOST_ETC
          value: /rootfs/etc
        - name: HOST_VAR
          value: /rootfs/var
        - name: HOST_RUN
          value: /rootfs/run
        - name: HOST_DEV
          value: /rootfs/dev
        - name: HOST_ROOT
          value: /rootfs
        # # ---iploc-start
        #- name: ENV_IPDB
        #  value: iploc
        # # ---iploc-end
        image: pubrepo.jiagouyun.com/datakit/datakit:1.9.2
        imagePullPolicy: Always
        name: datakit
        ports:
        - containerPort: 9529
          hostPort: 9529
          name: port
          protocol: TCP
        resources:
          requests:
            cpu: "200m"
            memory: "128Mi"
          limits:
            cpu: "2000m"
            memory: "4Gi"
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /usr/local/datakit/cache
          name: cache
          readOnly: false
        - mountPath: /rootfs
          name: rootfs
        - mountPath: /var/run
          name: run
        - mountPath: /sys/kernel/debug
          name: debugfs
        # # ---iploc-start
        - mountPath: /usr/local/datakit/data/ipdb/iploc/
          name: datakit-ipdb
        # # ---iploc-end
        - mountPath: /usr/local/datakit/conf.d/db/mysql.conf
          name: datakit-conf
          subPath: mysql.conf
          readOnly: true
        - mountPath: /usr/local/datakit/conf.d/db/redis.conf
          name: datakit-conf
          subPath: redis.conf
          readOnly: true
        - mountPath: /usr/local/datakit/conf.d/db/openes.conf
          name: datakit-conf
          subPath: elasticsearch.conf
          readOnly: false
        - mountPath: /usr/local/datakit/conf.d/influxdb/influxdb.conf
          name: datakit-conf
          subPath: influxdb.conf
        - mountPath: /usr/local/datakit/conf.d/ddtrace/ddtrace.conf
          name: datakit-conf
          subPath: ddtrace.conf
        workingDir: /usr/local/datakit
      # # ---iploc-start
      initContainers:
      - args:
        - tar -xf /opt/iploc.tar.gz -C /usr/local/datakit/data/ipdb/iploc/
        command:
        - bash
        - -c
        image: pubrepo.jiagouyun.com/datakit/iploc:1.0
        imagePullPolicy: IfNotPresent
        name: init-volume
        resources: {}
        volumeMounts:
        - mountPath: /usr/local/datakit/data/ipdb/iploc/
          name: datakit-ipdb
      # # ---iploc-end
      hostIPC: true
      hostPID: true
      restartPolicy: Always
      serviceAccount: datakit
      serviceAccountName: datakit
      tolerations:
      - operator: Exists
      volumes:
      - configMap:
          name: datakit-conf
        name: datakit-conf
      - hostPath:
          path: /root/datakit_cache
        name: cache
      - hostPath:
          path: /
        name: rootfs
      - hostPath:
          path: /var/run
        name: run
      - hostPath:
          path: /sys/kernel/debug
        name: debugfs
      # # ---iploc-start
      - emptyDir: {}
        name: datakit-ipdb
      # # ---iploc-end
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: datakit-conf
  namespace: datakit
data:
    mysql.conf: |-
        [[inputs.mysql]]
          host = "xxxxxxxxxxxxxxx"      ##修改对应MySQL连接地址
          user = "ste3"                 ##修改MySQL用户名
          pass = ""                     ##修改MySQL密码
          port = 3306
          # sock = "<SOCK>"
          # charset = "utf8"

          ## @param connect_timeout - number - optional - default: 10s
          # connect_timeout = "10s"

          ## Deprecated
          # service = "<SERVICE>"

          interval = "10s"

          ## @param inno_db
          innodb = true

          ## table_schema
          tables = []

          ## user
          users = []

          ## 开启数据库性能指标采集
          # dbm = false

          # [inputs.mysql.log]
          # #required, glob logfiles
          # files = ["/var/log/mysql/*.log"]

          ## glob filteer
          #ignore = [""]

          ## optional encodings:
          ##    "utf-8", "utf-16le", "utf-16le", "gbk", "gb18030" or ""
          #character_encoding = ""

          ## The pattern should be a regexp. Note the use of '''this regexp'''
          ## regexp link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
          #multiline_match = '''^(# Time|\d{4}-\d{2}-\d{2}|\d{6}\s+\d{2}:\d{2}:\d{2}).*'''

          ## grok pipeline script path
          #pipeline = "mysql.p"

          # [[inputs.mysql.custom_queries]]
          #   sql = "SELECT foo, COUNT(*) FROM table.events GROUP BY foo"
          #   metric = "xxxx"
          #   tagKeys = ["column1", "column1"]
          #   fieldKeys = ["column3", "column1"]
          
          ## 监控指标配置
          [inputs.mysql.dbm_metric]
            enabled = true
          
          ## 监控采样配置
          [inputs.mysql.dbm_sample]
            enabled = true  

          [inputs.mysql.tags]
            # some_tag = "some_value"
            # more_tag = "some_other_value"

    redis.conf: |-
        [[inputs.redis]]
          host = "r-6cxxxxxx4.redis.rds.ops.ste3.com"            ## 修改Redis连接地址
          port = 6379                                                   
          # unix_socket_path = "/var/run/redis/redis.sock"
          # 配置多个db，配置了dbs，db也会放入采集列表。dbs=[]或者不配置则会采集redis中所有非空的db
          # dbs=[]
          # username = "<USERNAME>"
           password = ""                                        ## 修改Redis密码

          ## @param connect_timeout - number - optional - default: 10s
          # connect_timeout = "10s"

          ## @param service - string - optional
          # service = "<SERVICE>"

          ## @param interval - number - optional - default: 15
          interval = "15s"

          ## @param keys - list of strings - optional
          ## The length is 1 for strings.
          ## The length is zero for keys that have a type other than list, set, hash, or sorted set.
          #
          # keys = ["KEY_1", "KEY_PATTERN"]

          ## @param warn_on_missing_keys - boolean - optional - default: true
          ## If you provide a list of 'keys', set this to true to have the Agent log a warning
          ## when keys are missing.
          #
          # warn_on_missing_keys = true

          ## @param slow_log - boolean - optional - default: false
          slow_log = true

          ## @param slowlog-max-len - integer - optional - default: 128
          slowlog-max-len = 128

          ## @param command_stats - boolean - optional - default: false
          ## Collect INFO COMMANDSTATS output as metrics.
          # command_stats = false

          # [inputs.redis.log]
          # #required, glob logfiles
          # files = ["/var/log/redis/*.log"]

          ## glob filteer
          #ignore = [""]

          ## grok pipeline script path
          #pipeline = "redis.p"

          ## optional encodings:
          ##    "utf-8", "utf-16le", "utf-16le", "gbk", "gb18030" or ""
          #character_encoding = ""

          ## The pattern should be a regexp. Note the use of '''this regexp'''
          ## regexp link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
          #match = '''^\S.*'''

          [inputs.redis.tags]
          # some_tag = "some_value"
          # more_tag = "some_other_value"

    openes.conf: |-
        [[inputs.elasticsearch]]
          ## Elasticsearch服务器配置
          # 支持Basic认证:
          # servers = ["http://user:pass@localhost:9200"]
          servers = ["http://guance:xxxxx@opensearch-cluster-client.middleware:9200"]    ## 修改相关配置


          ## 采集间隔
          # 单位 "ns", "us" (or "µs"), "ms", "s", "m", "h"
          interval = "60s"

          ## HTTP超时设置
          #http_timeout = "5s"

          ## 默认local是开启的，只采集当前Node自身指标，如果需要采集集群所有Node，需要将local设置为false
          local = false

          ## 设置为true可以采集cluster health
          cluster_health = true

          ## cluster health level 设置，indices (默认) 和 cluster
          # cluster_health_level = "indices"

          ## 设置为true时可以采集cluster stats.
          cluster_stats = true

          ## 只从master Node获取cluster_stats，这个前提是需要设置 local = true
          cluster_stats_only_from_master = true

          ## 需要采集的Indices, 默认为 _all
          indices_include = ["_all"]

          ## indices级别，可取值："shards", "cluster", "indices"
          indices_level = "indices"

          ## node_stats可支持配置选项有"indices", "os", "process", "jvm", "thread_pool", "fs", "transport", "http", "breaker"
          # 默认是所有
          # node_stats = ["jvm", "http"]

          ## HTTP Basic Authentication 用户名和密码
          # username = ""
          # password = ""

          ## TLS Config
          tls_open = false
          # tls_ca = "/etc/telegraf/ca.pem"
          # tls_cert = "/etc/telegraf/cert.pem"
          # tls_key = "/etc/telegraf/key.pem"
          ## Use TLS but skip chain & host verification
          # insecure_skip_verify = false

          # [inputs.elasticsearch.log]
          # files = []
          # #grok pipeline script path
          # pipeline = "elasticsearch.p"

          [inputs.elasticsearch.tags]
            app="openes"
            # some_tag = "some_value"
            # more_tag = "some_other_value"
            
    influxdb.conf: |-
        # {"version": "1.6.997_new-release", "desc": "do NOT edit this line"}

        [[inputs.influxdb]]
          url = "http://localhost:8086/debug/vars"                   ## 修改相关配置

          ## (optional) collect interval, default is 10 seconds
          interval = '10s'
          
          ## Username and password to send using HTTP Basic Authentication.
          # username = ""                                            ## 修改相关配置
          # password = ""                                            ## 修改相关配置

          ## http request & header timeout
          timeout = "5s"

          ## Set true to enable election
          election = true

          ## (Optional) TLS connection config
          # [inputs.influxdb.tlsconf]
          # ca_certs = ["/path/to/ca.pem"]
          # cert = "/path/to/cert.pem"
          # cert_key = "/path/to/key.pem"
          ## Use TLS but skip chain & host verification
          # insecure_skip_verify = false

          # [inputs.influxdb.log]
          # files = []
          # #grok pipeline script path
          # pipeline = "influxdb.p"

          [inputs.influxdb.tags]
            # some_tag = "some_value"
            # more_tag = "some_other_value"

    ddtrace.conf: |-
        # {"version": "1.6.997_new-release", "desc": "do NOT edit this line"}

        [[inputs.ddtrace]]
          ## DDTrace Agent endpoints register by version respectively.
          ## Endpoints can be skipped listen by remove them from the list.
          ## NOTE: DO NOT EDIT.
          endpoints = ["/v0.3/traces", "/v0.4/traces", "/v0.5/traces"]

          ## customer_tags is a list of keys contains keys set by client code like span.SetTag(key, value)
          ## that want to send to data center. Those keys set by client code will take precedence over
          ## keys in [inputs.ddtrace.tags]. DOT(.) IN KEY WILL BE REPLACED BY DASH(_) WHEN SENDING.
          # customer_tags = ["key1", "key2", ...]

          ## Keep rare tracing resources list switch.
          ## If some resources are rare enough(not presend in 1 hour), those resource will always send
          ## to data center and do not consider samplers and filters.
          # keep_rare_resource = false

          ## By default every error presents in span will be send to data center and omit any filters or
          ## sampler. If you want to get rid of some error status, you can set the error status list here.
          # omit_err_status = ["404"]

          ## Ignore tracing resources map like service:[resources...].
          ## The service name is the full service name in current application.
          ## The resource list is regular expressions uses to block resource names.
          ## If you want to block some resources universally under all services, you can set the
          ## service name as "*". Note: double quotes "" cannot be omitted.
          # [inputs.ddtrace.close_resource]
            # service1 = ["resource1", "resource2", ...]
            # service2 = ["resource1", "resource2", ...]
            # "*" = ["close_resource_under_all_services"]
            # ...

          ## Sampler config uses to set global sampling strategy.
          ## sampling_rate used to set global sampling rate.
          # [inputs.ddtrace.sampler]
            # sampling_rate = 1.0

          # [inputs.ddtrace.tags]
            # key1 = "value1"
            # key2 = "value2"
            # ...

          ## Threads config controls how many goroutines an agent cloud start to handle HTTP request.
          ## buffer is the size of jobs' buffering of worker channel.
          ## threads is the total number fo goroutines at running time.
          # [inputs.ddtrace.threads]
            # buffer = 100
            # threads = 8

          ## Storage config a local storage space in hard dirver to cache trace data.
          ## path is the local file path used to cache data.
          ## capacity is total space size(MB) used to store data.
          # [inputs.ddtrace.storage]
            # path = "./ddtrace_storage"
            # capacity = 5120
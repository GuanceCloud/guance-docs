apiVersion: v1
kind: Namespace
metadata:
  name: datakit
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: datakit
rules:
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterroles
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/proxy
  - namespaces
  - pods
  - pods/log
  - events
  - services
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - statefulsets
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - guance.com
  resources:
  - datakits
  verbs:
  - get
  - list
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: datakit
  namespace: datakit

---

apiVersion: v1
kind: Service
metadata:
  name: datakit-service
  namespace: datakit
spec:
  selector:
    app: daemonset-datakit
  ports:
    - protocol: TCP
      port: 9529
      targetPort: 9529

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: datakit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: datakit
subjects:
- kind: ServiceAccount
  name: datakit
  namespace: datakit

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: daemonset-datakit
  name: datakit
  namespace: datakit
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: daemonset-datakit
  template:
    metadata:
      labels:
        app: daemonset-datakit
      annotations:
        datakit/logs: |
          [
            {
              "disable": true
            }
          ]
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: ENV_K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: ENV_DATAWAY
          value: https://openway.guance.com?token=tkn_a624f78a0e224183805848c1949d4374 ## 此处填上 dataway 真实地址
        - name: ENV_GLOBAL_TAGS
          value: host=__datakit_hostname,host_ip=__datakit_ip
        - name: ENV_DEFAULT_ENABLED_INPUTS
          value: cpu,disk,diskio,mem,swap,system,hostobject,net,host_processes,container
        - name: ENV_ENABLE_ELECTION
          value: enable
        - name: ENV_LOG
          value: /var/log/datakit/log
        - name: ENV_HTTP_LISTEN
          value: 0.0.0.0:9529
        - name: ENV_LOG_LEVEL
          value: debug
        - name: ENV_INPUT_DISK_EXCLUDE_DEVICE
          value: /dev/sda1
        - name: ENV_INPUT_DISK_EXTRA_DEVICE
          value : 10.100.14.144:/nfsdata            ## 修改成实际的nfs 目录
        - name: ENV_IPDB
          value: iploc
        image: pubrepo.jiagouyun.com/datakit/datakit:1.4.14     ## 修改成最新镜像版本
        imagePullPolicy: Always
        name: datakit
        ports:
        - containerPort: 9529
          hostPort: 9529
          name: port
          protocol: TCP
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /var/run
          name: run
        - mountPath: /var/lib
          name: lib
        - mountPath: /var/log
          name: log
        - mountPath: /usr/lib
          name: usrlib
        - mountPath: /etc
          name: etc
        - mountPath: /rootfs
          name: rootfs
        - mountPath: /sys/kernel/debug
          name: debugfs
        - mountPath: /usr/local/datakit/data
          name: datakit-ipdb
     #   - mountPath: /usr/local/datakit/conf.d/host/disk.conf
      #    name: datakit-conf
      #    subPath: disk.conf
        #- mountPath: /usr/local/datakit/conf.d/db/mysql.conf
        #  name: datakit-conf
        #  subPath: mysql.conf
        #  readOnly: true
        #- mountPath: /usr/local/datakit/conf.d/db/redis.conf
        #  name: datakit-conf
        #  subPath: redis.conf
        #  readOnly: true
        # - mountPath: /usr/local/datakit/conf.d/db/elasticsearch.conf
        #   name: datakit-conf
        #   subPath: elasticsearch.conf
        #   readOnly: false
        # - mountPath: /usr/local/datakit/conf.d/db/tdengine.conf
        #   name: datakit-conf
        #   subPath: tdengine.conf
        #   readOnly: false
        # - mountPath: /usr/local/datakit/conf.d/ddtrace/ddtrace.conf
        #   name: datakit-conf
        #   subPath: ddtrace.conf
        #   readOnly: false
        workingDir: /usr/local/datakit
      initContainers:
        - name: init-volume
          image: "pubrepo.jiagouyun.com/datakit/iploc:1.0"
          imagePullPolicy: IfNotPresent
          command: ["bash", "-c"]
          args:
            - tar -xf /opt/iploc.tar.gz -C /usr/local/datakit/data
          volumeMounts:
            - name: datakit-ipdb
              mountPath: /usr/local/datakit/data
      hostIPC: true
      hostPID: true
      restartPolicy: Always
      serviceAccount: datakit
      serviceAccountName: datakit
      tolerations:
      - operator: Exists
      volumes:
      - configMap:
          name: datakit-conf
        name: datakit-conf
      - hostPath:
          path: /var/run
        name: run
      - hostPath:
          path: /var/lib
        name: lib
      - hostPath:
          path: /var/log
        name: log
      - hostPath:
          path: /usr/lib
        name: usrlib
      - hostPath:
          path: /etc
        name: etc
      - hostPath:
          path: /
        name: rootfs
      - hostPath:
          path: /sys/kernel/debug
        name: debugfs
      - hostPath:
          path: /usr/local/datakit/data
        name: datakit-ipdb
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: datakit-conf
  namespace: datakit
data:
    mysql.conf: |-
        [[inputs.mysql]]
          host = "rm-6cq9acv52i8aa6123.mysql.rds.ops.ste3.com"
          user = "ste3"
          pass = "Test1234"
          port = 3306
          # sock = "<SOCK>"
          # charset = "utf8"

          ## @param connect_timeout - number - optional - default: 10s
          # connect_timeout = "10s"

          ## Deprecated
          # service = "<SERVICE>"

          interval = "10s"

          ## @param inno_db
          innodb = true

          ## table_schema
          tables = []

          ## user
          users = []

          ## 开启数据库性能指标采集
          # dbm = false

          # [inputs.mysql.log]
          # #required, glob logfiles
          # files = ["/var/log/mysql/*.log"]

          ## glob filteer
          #ignore = [""]

          ## optional encodings:
          ##    "utf-8", "utf-16le", "utf-16le", "gbk", "gb18030" or ""
          #character_encoding = ""

          ## The pattern should be a regexp. Note the use of '''this regexp'''
          ## regexp link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
          #multiline_match = '''^(# Time|\d{4}-\d{2}-\d{2}|\d{6}\s+\d{2}:\d{2}:\d{2}).*'''

          ## grok pipeline script path
          #pipeline = "mysql.p"

          # [[inputs.mysql.custom_queries]]
          #   sql = "SELECT foo, COUNT(*) FROM table.events GROUP BY foo"
          #   metric = "xxxx"
          #   tagKeys = ["column1", "column1"]
          #   fieldKeys = ["column3", "column1"]
          
          ## 监控指标配置
          [inputs.mysql.dbm_metric]
            enabled = true
          
          ## 监控采样配置
          [inputs.mysql.dbm_sample]
            enabled = true  

          [inputs.mysql.tags]
            # some_tag = "some_value"
            # more_tag = "some_other_value"

    redis.conf: |-
        [[inputs.redis]]
          host = "r-6cq2279babc65984.redis.rds.ops.ste3.com"
          port = 6379
          # unix_socket_path = "/var/run/redis/redis.sock"
          # 配置多个db，配置了dbs，db也会放入采集列表。dbs=[]或者不配置则会采集redis中所有非空的db
          # dbs=[]
          # username = "<USERNAME>"
           password = "Test1234"

          ## @param connect_timeout - number - optional - default: 10s
          # connect_timeout = "10s"

          ## @param service - string - optional
          # service = "<SERVICE>"

          ## @param interval - number - optional - default: 15
          interval = "15s"

          ## @param keys - list of strings - optional
          ## The length is 1 for strings.
          ## The length is zero for keys that have a type other than list, set, hash, or sorted set.
          #
          # keys = ["KEY_1", "KEY_PATTERN"]

          ## @param warn_on_missing_keys - boolean - optional - default: true
          ## If you provide a list of 'keys', set this to true to have the Agent log a warning
          ## when keys are missing.
          #
          # warn_on_missing_keys = true

          ## @param slow_log - boolean - optional - default: false
          slow_log = true

          ## @param slowlog-max-len - integer - optional - default: 128
          slowlog-max-len = 128

          ## @param command_stats - boolean - optional - default: false
          ## Collect INFO COMMANDSTATS output as metrics.
          # command_stats = false

          # [inputs.redis.log]
          # #required, glob logfiles
          # files = ["/var/log/redis/*.log"]

          ## glob filteer
          #ignore = [""]

          ## grok pipeline script path
          #pipeline = "redis.p"

          ## optional encodings:
          ##    "utf-8", "utf-16le", "utf-16le", "gbk", "gb18030" or ""
          #character_encoding = ""

          ## The pattern should be a regexp. Note the use of '''this regexp'''
          ## regexp link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
          #match = '''^\S.*'''

          [inputs.redis.tags]
          # some_tag = "some_value"
          # more_tag = "some_other_value"

    elasticsearch.conf: |-
        [[inputs.elasticsearch]]
          ## Elasticsearch服务器配置
          # 支持Basic认证:
          # servers = ["http://user:pass@localhost:9200"]
          servers = ["http://copriwolf:sayHi2Elastic@172.21.12.116:9200"]

          ## 采集间隔
          # 单位 "ns", "us" (or "µs"), "ms", "s", "m", "h"
          interval = "10s"

          ## HTTP超时设置
          http_timeout = "5s"

          ## 发行版本: elasticsearch, opendistro
          distribution = "elasticsearch"

          ## 默认local是开启的，只采集当前Node自身指标，如果需要采集集群所有Node，需要将local设置为false
          local = true

          ## 设置为true可以采集cluster health
          cluster_health = true

          ## cluster health level 设置，indices (默认) 和 cluster
          # cluster_health_level = "indices"

          ## 设置为true时可以采集cluster stats.
          cluster_stats = true

          ## 只从master Node获取cluster_stats，这个前提是需要设置 local = true
          cluster_stats_only_from_master = true

          ## 需要采集的Indices, 默认为 _all
          indices_include = ["_all"]

          ## indices级别，可取值："shards", "cluster", "indices"
          indices_level = "shards"

          ## node_stats可支持配置选项有"indices", "os", "process", "jvm", "thread_pool", "fs", "transport", "http", "breaker"
          # 默认是所有
          # node_stats = ["jvm", "http"]

          ## HTTP Basic Authentication 用户名和密码
          # username = "test"
          # password = "123456"

          ## TLS Config
          tls_open = false
          # tls_ca = "/etc/telegraf/ca.pem"
          # tls_cert = "/etc/telegraf/cert.pem"
          # tls_key = "/etc/telegraf/key.pem"
          ## Use TLS but skip chain & host verification
          # insecure_skip_verify = false

          # [inputs.elasticsearch.log]
          # files = []
          # #grok pipeline script path
          # pipeline = "elasticsearch.p"

          [inputs.elasticsearch.tags]
            # some_tag = "some_value"
            # more_tag = "some_other_value"
    tdengine.conf: |-
        # {"version": "1.4.3", "desc": "do NOT edit this line"}

        [[inputs.tdengine]]
          ## adapter config (Required)
          adapter_endpoint = "http://taos-tdengine.middleware:6041"
          user = "zhuyun"
          password = "jfdlEGFH2143!"

          ## add tag (optional)
           [inputs.tdengine.tags]
            app="hcs"
            # some_tag = "some_value"
            # more_tag = "some_other_value"

    ddtrace.conf: |-
        [[inputs.ddtrace]]
          ## DDTrace Agent endpoints register by version respectively.
          ## Endpoints can be skipped listen by remove them from the list.
          ## Default value set as below. DO NOT MODIFY THESE ENDPOINTS if not necessary.
          endpoints = ["/v0.3/traces", "/v0.4/traces", "/v0.5/traces"]

          ## customer_tags is a list of keys contains keys set by client code like span.SetTag(key, value)
          ## that want to send to data center. Those keys set by client code will take precedence over
          ## keys in [inputs.ddtrace.tags]. DOT(.) IN KEY WILL BE REPLACED BY DASH(_) WHEN SENDING.
          # customer_tags = ["key1", "key2", ...]

          ## Keep rare tracing resources list switch.
          ## If some resources are rare enough(not presend in 1 hour), those resource will always send
          ## to data center and do not consider samplers and filters.
          # keep_rare_resource = false

          ## Ignore tracing resources map like service:[resources...].
          ## The service name is the full service name in current application.
          ## The resource list is regular expressions uses to block resource names.
          # [inputs.ddtrace.close_resource]
            # service1 = ["resource1", "resource2", ...]
            # service2 = ["resource1", "resource2", ...]
            # ...

          ## Sampler config uses to set global sampling strategy.
          ## priority uses to set tracing data propagation level, the valid values are -1, 0, 1
          ##   -1: always reject any tracing data send to datakit
          ##    0: accept tracing data and calculate with sampling_rate
          ##    1: always send to data center and do not consider sampling_rate
          ## sampling_rate used to set global sampling rate
          # [inputs.ddtrace.sampler]
            # priority = 0
            # sampling_rate = 1.0

          # [inputs.ddtrace.tags]
            # key1 = "value1"
            # key2 = "value2"
            # ...

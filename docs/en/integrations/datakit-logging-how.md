---
skip: 'not-searchable-on-index-page'
title: 'DataKit Log Processing Overview'
---

This article introduces how DataKit processes logs. In [another document](datakit-logging.md), we introduced how DataKit collects logs. These two documents can be read together, and we hope that everyone will have a more comprehensive understanding of the entire log processing.

Core issues:

- Why is the configuration for log collection so complex?
- How are log data processed?

## Why is the log collection configuration so complex? {#why}

From [this document](datakit-logging.md), we know that because log sources are diverse, the configuration methods for logs are also varied. We need to organize them here for better understanding.

During the log collection process, DataKit has two main categories of collection methods: active and passive.

- Active Collection
    - Directly collect [disk file logs](logging.md)
    - Collect logs generated by [containers](container.md)

- Passive Collection
    - Inject logs into DataKit through [HTTP](logstreaming.md), [TCP/UDP](logging.md#socket), and [Websocket](logfwd.md)

In these different forms of log collection methods, they all need to address one core issue: **How should DataKit handle these logs next?**

Breaking down this core issue further, it can be divided into the following sub-questions:

- Determine what `source` is: All subsequent log processing depends on this field (there is an additional `service` field, but if not specified, its value will be set the same as source).
- How to configure Pipeline: Although it's not mandatory, it is widely applied.
- Additional Tag Configuration: It's also not mandatory, but in certain situations, it serves a special function.
- How to split multi-line logs: Need to inform DataKit how target logs are separated into individual log entries (DataKit defaults to treating every line starting with non-whitespace characters as a new log entry).
- Whether there are special ignore strategies: Not all data collected by DataKit needs to be processed; under certain conditions, you can choose not to collect them (even though they meet the collection criteria).
- Other distinctive configurations: Such as filtering color characters, text encoding handling, etc.

Currently, there are several ways to inform DataKit how to handle collected logs:

- [Log collector](logging.md) conf configuration

In the log collector, [through conf configuration](logging.md#config), specify the list of files to collect (or which TCP/UDP port to read log streams from). In conf, multiple settings such as source/Pipeline/multi-line splitting/additional tag addition can be configured.

If the data is sent to DataKit in TCP/UDP form, logging.conf must still be used to configure subsequent log processing because these protocols are not convenient for attaching extra descriptive information; they only transmit simple log stream data.

This form of log collection is the easiest to understand.

- [Container collector](container.md) conf configuration

Currently, the container collector's conf can only make very basic configurations for logs (based on container/Pod image names), and cannot configure subsequent log processing (such as Pipeline/source settings), because this conf targets **all logs on the current host**, and in container environments, logs on a single host are varied, making it impossible to categorize and configure each individually.

- Inform DataKit how to configure log processing via the request

Through HTTP requests to DataKit's [Log Streaming](logstreaming.md) service, include various request parameters in the request to inform Datakit how to handle received log data.

- Apply specific annotations on the object being collected (e.g., containers/Pods) to inform DataKit how to process their generated logs

As mentioned earlier, configuring log collection solely in the container collector conf is too coarse-grained and not conducive to fine-tuning, but [annotations or labels can be added on containers/Pods](container-log.md#logging-with-annotation-or-label), and DataKit will **actively discover these annotations**, thereby knowing how to process logs from each container/Pod.

### Priority Explanation {#priority}

In general, the priority of annotations on containers/Pods is the highest, overriding settings in conf/Env; Env has medium priority, overriding configurations in conf; configurations in conf have the lowest priority, and settings in Env or annotations can always override them.

> There are currently no direct Env variables related to log collection/processing, but relevant environment variables may be added in the future.

For example, in container.conf, assume we exclude the image named 'my_test' from log collection:

```toml
container_exclude_log = ["image:my_test*"]
```

In this case, DataKit will not collect any containers or Pods matching this image name wildcard. However, if the corresponding Pod has been annotated accordingly:

```yaml
apiVersion: apps/v1
kind: Pod
metadata:
  name: test-app
  annotations:
    datakit/logs: |   # <----------
      [
        {
          "source": "my-testing-app",
          "pipeline": "test.p",
        }
      ]

spec:
   containers:
   - name : mytest
     image: my_test:1.2.3
```

Even though we excluded all images matching `my_test.*` in container.conf, since this Pod carries specific annotations (`datakit/logs`), DataKit will still collect the Pod's logs and can configure settings like Pipeline.

## How are log data processed? {#how-logging-processed}

In DataKit, logs currently go through the following stages of processing (listed in order):

- Collection Stage

After reading (receiving) logs from external sources, the collection stage performs basic processing. This includes log segmentation (splitting large chunks of text into multiple independent raw logs), encoding/decoding (uniform conversion to UTF8 encoding), removing some interfering color characters, etc.

- Single Log Segmentation

If the corresponding log has Pipeline segmentation configured, then each log (including multi-line logs) will be segmented through the Pipeline. The Pipeline mainly consists of two steps:

1. Grok/JSON segmentation: Through Grok/JSON, single Raw logs are segmented into structured data.
1. Fine-tuning the extracted fields: For example, [completing IP information](../pipeline/use-pipeline/pipeline-built-in-function.md#fn-geoip), [log desensitization](../pipeline/use-pipeline/pipeline-built-in-function.md#fn-cover), etc.

- Blacklist (Filter)

[Filter is a set of filters](../datakit/datakit-filter.md) that receives a set of structured data and determines whether to discard it based on certain logical judgments. Filters are centrally issued (DataKit actively pulls) a set of logical operation rules, generally in the following form:

``` not-set
{ source = 'datakit' AND bar IN [ 1, 2, 3] }
```

If the center configures a log blacklist, assuming 10 out of 100 segmented logs satisfy these conditions (i.e., source is `datakit`, and the value of the `bar` field appears in the subsequent list), then these 10 logs will not be reported to Guance and will be silently discarded. In [DataKit Monitor](../datakit/datakit-monitor.md), you can see statistics about discarded logs.

- Report to Guance

After going through the above steps, the log data is finally reported to Guance, and you can view the log data on the log viewing page.

Under normal circumstances, from the generation of logs, if collection is successful, it takes about 30 seconds for the data to appear on the page. During this time, DataKit itself reports data at most once every 10 seconds, and the center also undergoes a series of processing before final storage.

## Extended Reading {#more-readings}

- [DataKit Log Collection Overview](datakit-logging.md)
- [How to Debug Pipelines](../pipeline/use-pipeline/pipeline-quick-start.md#debug)
- [Line Protocol Blacklist Filter](../datakit/datakit-filter.md)
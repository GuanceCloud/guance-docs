# Pipeline Data Processing for Different Categories

[:octicons-beaker-24: Experimental](index.md#experimental)

---

Since DataKit 1.4.0, the built-in Pipeline feature can be used to directly manipulate data collected by DataKit, supporting all [data types](../../datakit/apis.md#category).

<!-- markdownlint-disable MD046 -->
???+ attention

    - The Pipeline applies to all data and is currently in the experimental phase. There is no guarantee that backward incompatible changes will not be made to the mechanism or behavior.
    - Even data reported via the [DataKit API](../../datakit/apis.md) supports Pipeline processing.
    - Using Pipeline to process existing collected data (especially non-log data) may significantly alter the existing data structure, leading to abnormal behavior in Guance.
    - Before applying a Pipeline, please confirm that the data processing meets expectations using the [Pipeline Debugging Tool](pipeline-quick-start.md#debug).
<!-- markdownlint-enable -->

The Pipeline can perform the following operations on data collected by DataKit:

- Add, delete, modify the values or data types of fields and tags
- Convert fields to tags
- Modify the name of a measurement
- Mark data for discard ([drop()](pipeline-built-in-function.md#fn-drop))
- Terminate the execution of the Pipeline script ([exit()](pipeline-built-in-function.md#fn-exit))
- ...

## Input Data Structure {#input-data-struct}

All categories of data are encapsulated into a Point structure before being processed by the Pipeline script. The structure can be viewed as follows:

``` not-set
struct Point {
   Name:      str          # Equivalent to the measurement name for Metrics (time series) data, source for Logging (log) data,
                              # source for Network data, class for Object/CustomObject (object) data ...
   Tags:      map[str]str  # Stores all tags of the data; for non-time series data, the distinction between tags and fields is less clear
   Fields:    map[str]any  # Stores all fields (metrics for time series data)
   Time:      int64        # Represents the timestamp of the data, typically the time when the data was generated, in nanoseconds
   DropFlag:  bool         # Indicates whether the data should be discarded
}
```

For example, an Nginx log entry collected by the log collector would be represented as follows when input into the Pipeline script:

``` not-set
Point {
    Name: "nginx"
    Tags: map[str]str {
        "host": "your_hostname"
    },
    Fields: map[str]any {
        "message": "127.0.0.1 - - [12/Jan/2023:11:51:38 +0800] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.81.0\""
    },
    Time: 1673495498000123456,
    DropFlag: false,
}
```

Note:

- The `name` can be modified using the `set_measurement()` function.

- In the Point's tags/fields map, **no key will appear simultaneously in both tags and fields**;

- You can read the corresponding key value from the Point's tags/fields map in the Pipeline using custom identifiers or the `get_key()` function; modifying the value of keys in Tags or Fields requires other built-in functions such as `add_key`; `_` can be considered an alias for the key `message`.

- After the script runs, if there is a key named `time` in the Point's tags/fields map, it will be deleted; if its value is of type int64, it will be assigned to the Point's time and then deleted. If `time` is a string, you can attempt to convert it to int64 using the `default_time()` function.

- You can use the `drop()` function to mark the input Point for discard; after the script executes, this data will not be uploaded.

## Storage, Indexing, and Matching of Pipeline Scripts {#script-store-index-match}

### Script Storage and Indexing {#store-and-index}

Currently, Pipeline scripts are divided into four namespaces with decreasing indexing priority, as shown in the table below:

| Namespace | Directory | Supported Data Categories | Description |
| - | -  | - | - |
| `remote`  | *[DataKit Installation Directory]/pipeline_remote*                  | CO, E, L, M, N, O, P, R, S, T | Scripts managed by the Guance console            |
| `confd`   | *[DataKit Installation Directory]/pipeline_confd*                    | CO, E, L, M, N, O, P, R, S, T | Scripts managed by Confd                |
| `gitrepo` | *[DataKit Installation Directory]/pipeline_gitrepos/[repo-name]*  | CO, E, L, M, N, O, P, R, S, T | Scripts managed by Git                  |
| `default` | *[DataKit Installation Directory]/pipeline*                         | CO, E, L, M, N, O, P, R, S, T | Scripts generated by DataKit or user-written |

Note:

- Do not modify the automatically generated default scripts under the *pipeline* directory. Any modifications will be overwritten upon DataKit startup.
- It is recommended to add local scripts for specific data categories under the *pipeline/[category]/* directory.
- Except for the *pipeline* directory, do not make any modifications to other script directories (*remote*, *confd*, *gitrepo*).

When DataKit selects the appropriate Pipeline, the indexing priority of scripts within these four namespaces decreases in order. For example, when looking for *metric/cpu.p*, DataKit will search in the following order:

1. `pipeline_remote/metric/cpu.p`
2. `pipeline_confd/metric/cpu.p`
3. `gitrepo/<repo-name>/metric/cpu.p`
4. `pipeline/metric/cpu.p`

> Note: `<repo-name>` depends on your Git repository name.

We create indexes for each category of scripts separately. This functionality does not allow the `use()` function to reference scripts across namespaces. The implementation of Pipeline script storage and indexing is illustrated in the diagram below, where higher-priority namespace scripts override lower-priority ones during indexing:

![script-index](img/pipeline-script-index.drawio.png)

All four sources of Pipeline directories store Pipeline scripts in the following manner:

``` not-set
├── pattern   <-- Directory specifically for custom patterns
├── apache.p
├── consul.p
├── sqlserver.p        <--- All top-level directory Pipelines default to logs for historical compatibility
├── tomcat.p
├── other.p
├── custom_object      <--- Directory specifically for custom object Pipelines
│   └── some-object.p
├── keyevent           <--- Directory specifically for event Pipelines
│   └── some-event.p
├── logging            <--- Directory specifically for log Pipelines
│   └── nginx.p
├── metric             <--- Directory specifically for time series metric Pipelines
│   └── cpu.p
├── network            <--- Directory specifically for network metric Pipelines
│   └── ebpf.p
├── object             <--- Directory specifically for object Pipelines
│   └── HOST.p
├── rum                <--- Directory specifically for RUM Pipelines
│   └── error.p
├── security           <--- Directory specifically for Security Check Pipelines
│   └── scheck.p
└── tracing            <--- Directory specifically for APM Pipelines
    └── service_a.p
```

### Data and Script Matching Strategy {#match}

The data and script name matching strategy consists of four rules, evaluated from highest (Rule 4) to lowest (Rule 1) priority. If a higher-priority rule is satisfied, lower-priority rules are not applied:

1. Generate a data characteristic string based on the input data, append the Pipeline script file extension `.p`, and find the corresponding category script.
2. Use the default script set for all data of this category in the Guance console.
3. Use the data-to-script mapping relationship defined in the Guance console.
4. Use the script specified in the collector configuration file.

All data and script matching strategies depend on the data characteristic string; the generation strategy varies by data category:

1. Generate the data characteristic string using specific point tags/fields:
   - For Tracing and Profiling data in APM:
       - Use the value of `service` in tags/fields to generate the data characteristic string. For example, if `service` is `service-a`, the generated string is `service-a`, corresponding to the script name `service-a.p`, which is then searched under the *Tracing/Profiling* script index.
   - For Security data in Security Check:
       - Use the value of `category` in tags/fields to generate the data characteristic string. For example, if `category` is `system`, the generated string is `system`, corresponding to the script name `system.p`.

2. Generate the data characteristic string using specific point tags/fields and point name:
   - For RUM data in RUM:
     - Use the value of `app_id` in tags/fields and the point name to generate the data characteristic string. For example, if the point name is `action`, the generated string is `<app_id>_action`, corresponding to the script name `<app_id>_action.p`.

3. Generate the data characteristic string using the point name:
   - For Logging, Metric, Network, Object, etc.:
     - Use the point name to generate the data characteristic string. For example, for the `cpu` time series metric, the generated string is `cpu`, corresponding to the script `cpu.p`. For object data with class `HOST`, the generated string is `HOST`, corresponding to the script `HOST.p`.

## Pipeline Processing Examples {#examples}

> Example scripts are for reference only; customize them according to your needs.

### Processing Time Series Data {#M}

The following example demonstrates how to modify tags and fields using Pipeline. Through DQL, we know the fields of a CPU metric are as follows:

```shell
dql > M::cpu{host='u'} LIMIT 1
-----------------[ r1.cpu.s1 ]-----------------
core_temperature 76
             cpu 'cpu-total'
            host 'u'
            time 2022-04-25 12:32:55 +0800 CST
     usage_guest 0
usage_guest_nice 0
      usage_idle 81.399796
    usage_iowait 0.624681
       usage_irq 0
      usage_nice 1.695563
   usage_softirq 0.191229
     usage_steal 0
    usage_system 5.239674
     usage_total 18.600204
      usage_user 10.849057
---------
```

Write the following Pipeline script:

```python
# file pipeline/metric/cpu.p

set_tag(script, "metric::cpu.p")
set_tag(host2, host)
usage_guest = 100.1
```

After restarting DataKit, new data is collected, and through DQL, we get the modified CPU metric:

```shell
dql > M::cpu{host='u'}[20s] LIMIT 1
-----------------[ r1.cpu.s1 ]-----------------
core_temperature 54.250000
             cpu 'cpu-total'
            host 'u'
           host2 'u'                        <--- New tag
          script 'metric::cpu.p'            <--- New tag
            time 2022-05-31 12:49:15 +0800 CST
     usage_guest 100.100000                 <--- Modified field value
usage_guest_nice 0
      usage_idle 94.251269
    usage_iowait 0.012690
       usage_irq 0
      usage_nice 0
   usage_softirq 0.012690
     usage_steal 0
    usage_system 2.106599
     usage_total 5.748731
      usage_user 3.616751
---------
```

### Processing Object Data {#O}

The following Pipeline example demonstrates how to discard (filter) data. Taking the Nginx process on the current host as an example, the Nginx process list is as follows:

```shell
$ ps axuwf | grep  nginx
root        1278  0.0  0.0  55288  1496 ?        Ss   10:10   0:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;
www-data    1279  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1280  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1281  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1282  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1283  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1284  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1286  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
www-data    1287  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
```

Through DQL, we know the fields of a specific process metric are as follows:

```shell
dql > O::host_processes:(host, class, process_name, cmdline, pid) {host='u', pid=1278}
-----------------[ r1.host_processes.s1 ]-----------------
       class 'host_processes'
     cmdline 'nginx: master process /usr/sbin/nginx -g daemon on; master_process on;'
        host 'u'
         pid 1278
process_name 'nginx'
        time 2022-05-31 14:19:15 +0800 CST
---------
```

Write the following Pipeline script:

```python
if process_name == "nginx" {
    drop()  # drop() marks the data for discard and continues running the pl
    exit()  # exit() terminates the Pipeline execution
}
```

After restarting DataKit, the corresponding Nginx process objects will no longer be collected (the central object has an expiration policy, so wait 5~10 minutes for the original Nginx object to expire automatically).
Here is the translation of the provided text into English:

# Data Processing for Each Category

[:octicons-beaker-24: Experimental](index.md#experimental)

---

Since DataKit 1.4.0, the built-in Pipeline feature can directly manipulate DataKit's collected data, supporting all [current data types](../../datakit/apis.md#category).

<!-- markdownlint-disable MD046 -->
???+ warning

    - The application of Pipeline to all data is currently in the experimental stage, and there is no guarantee that the mechanism or behavior will not be adjusted to be incompatible later.
    - Even data reported through the [DataKit API](../../datakit/apis.md) also supports Pipeline processing.
    - Using Pipeline to process existing collected data (especially non-log data) is very likely to damage the existing data structure, causing the data to behave abnormally on the Observation Cloud.
    - Before applying Pipeline, please use the [Pipeline debugging tool](pipeline-quick-start.md#debug) to confirm whether the data processing meets expectations.
<!-- markdownlint-enable -->

Pipeline can perform the following operations on DataKit's collected data:

- Add, delete, modify the values or data types of fields and tags.
- Change fields to tags.
- Modify the name of the metric set.
- Mark to discard the current data ([drop()](pipeline-built-in-function.md#fn-drop)).
- Terminate the execution of the Pipeline script ([exit()](pipeline-built-in-function.md#fn-exit)).
- ...

## Input Data Structure {#input-data-struct}

All categories of data will be encapsulated into the Point structure before being processed by the Pipeline script, which can be considered as:

``` not-set
struct Point {
   Name:      str          # Equivalent to the name of the metric set for Metric (time series) data, the source for Logging (log) data,
                              # the source for Network data, the class for Object/CustomObject (object) data, etc.
   Tags:      map[str]str  # Stores all data tags, for non-time series category data, the boundary between tags and fields is vague
   Fields:    map[str]any  # Stores all data fields (metrics for time series category data)
   Time:      int64        # As the data time, usually interpreted as the data generation timestamp, in nanoseconds
   DropFlag:  bool         # Marks whether the data should be discarded
}
```

Take an Nginx log data as an example. After being collected by the log collector, the generated data as the input of the Pipeline script is roughly:

``` not-set
Point {
    Name: "nginx"
    Tags: map[str]str {
        "host": "your_hostname"
    },
    Fields: map[str]any {
        "message": "127.0.0.1 - - [12/Jan/2023:11:51:38 +0800] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.81.0\""
    },
    Time: 1673495498000123456,
    DropFlag: false,
}
```

**Notes**:

- The `name` can be modified through the function `set_measurement()`.

- In the point's tags/fields map, **any key cannot and will not appear in both tags and fields at the same time**.

- You can read the value of the corresponding key in the point's tags/fields map through a custom identifier or the function `get_key()` in the Pipeline; however, modifying the value of the keys in Tags or Fields requires other built-in functions, such as `add_key` and others; where `_` can be considered an alias for the `message` key.

- After the script is executed, if there is a key named `time` in the point's tags/fields map, it will be deleted; when its value is of int64 type, it will be assigned to the point's time and then deleted. If the time is a string, you can try to use the function `default_time()` to convert it to int64.

- You can use the `drop()` function to mark the input Point as to be discarded, and the data will not be uploaded after the script execution is completed.

## Pipeline Script Storage, Indexing and Matching {#script-store-index-match}

### Script Storage and Indexing {#store-and-index}

Currently, Pipeline scripts are divided into four namespaces according to the source, with decreasing index priority as shown in the table below:

| Namespace | Directory | Supported Data Categories | Description |
| - | -  | - | - |
| `remote`  | *[DataKit Installation Directory]/pipeline_remote* | CO, E, L, M, N, O, P, R, S, T | Scripts managed by the Observation Cloud console |
| `confd`   | *[DataKit Installation Directory]/pipeline_cond* | CO, E, L, M, N, O, P, R, S, T | Scripts managed by Confd |
| `gitrepo` | *[DataKit Installation Directory]/pipeline_gitrepos/[repo-name]* | CO, E, L, M, N, O, P, R, S, T | Scripts managed by Git |
| `default` | *[DataKit Installation Directory]/pipeline* | CO, E, L, M, N, O, P, R, S, T | Scripts generated by DataKit or written by the user |

**Note:**

- Do not modify the automatically generated default scripts in the *pipeline* directory, as they will be overwritten after DataKit starts;
- It is recommended to add local scripts for corresponding data categories in the *pipeline/[category]/* directory;
- Do not make any modifications to other script directories (*remote*, *confd*, *gitrepo*) except for the *pipeline* directory.

When DataKit selects the corresponding Pipeline, the index priority of scripts in these four namespaces is decreasing. Taking the `cpu` metric set as an example, when *metric/cpu.p* is needed, the order of DataKit's search is as follows:

1. `pipeline_remote/metric/cpu.p`
2. `pipeline_confd/metric/cpu.p`
3. `gitrepo/<repo-name>/metric/cpu.p`
4. `pipeline/metric/cpu.p`

**Note**: The `<repo-name>` here depends on the name of your git repository.

We will create an index for scripts in each data category, and this feature will not cause the `use()` function to reference scripts across namespaces; the implementation of Pipeline script storage and script indexing is shown in the figure below. When establishing the script index, scripts in high-priority namespaces will cover those in low-priority ones:

![script-index](img/pipeline-script-index.drawio.png)

All four sources of Pipeline directories are stored in the following manner for Pipeline scripts:

``` not-set
├── pattern   <-- Directory specifically for storing custom patterns
├── apache.p
├── consul.p
├── sqlserver.p        <--- All top-level Pipeline scripts under the directory default to logs for compatibility with historical settings
├── tomcat.p
├── other.p
├── custom_object      <--- Directory specifically for storing pipelines in resource directories
│   └── some-object.p
├── keyevent           <--- Directory specifically for storing pipelines for events
│   └── some-event.p
├── logging            <--- Directory specifically for storing pipelines for logs
│   └── nginx.p
├── metric             <--- Directory specifically for storing pipelines for time series metrics
│   └── cpu.p
├── network            <--- Directory specifically for storing pipelines for network metrics
│   └── ebpf.p
├── object             <--- Directory specifically for storing pipelines for objects
│   └── HOST.p
├── rum                <--- Directory specifically for storing pipelines for RUM
│   └── error.p
├── security           <--- Directory specifically for storing pipelines for scheck
│   └── scheck.p
└── tracing            <--- Directory specifically for storing pipelines for APM
    └── service_a.p
```

### Data and Script Matching Strategy {#match}

There are four matching strategies for data and script names, which will be judged from the 4th (highest priority) to the 1st, and if a high-priority strategy is met, the low-priority strategy will not be executed:

1. Generate a data feature string based on the input data, add the Pipeline script file extension `.p`, and look for the corresponding category script.
2. The default script set for all data in this category in the Observation Cloud console.
3. The mapping relationship between data and scripts set in the Observation Cloud console.
4. The script specified in the collector configuration file.



All data and script matching strategies rely on the data's feature string; the generation strategy for the data feature string varies for different categories of data:

1. Use specific point tags/fields to generate the data feature string:
   - For APM's Tracing and Profiling category data:
     - The data feature string is generated using the value of `service` in **tags/fields**. For example, if DataKit collects a piece of data with the `service` value `service-a`, it will generate `service-a`, corresponding to the script name `service-a.p`, and then search under the *Tracing/Profiling* category script index.
   - For Scheck's Security category data feature string:
     - The data feature string is generated using the value of `category` in **tags/fields**. For example, if DataKit receives a Security data piece with the `category` value `system`, it will generate `system`, corresponding to the script name `system.p`.

2. Use specific point tags/fields and point name to generate the data feature string:
   - For RUM's RUM category data:
     - The data feature string is generated using the value of `app_id` in **tags/fields** and the **point name** value. Taking `action` as an example for the point name value, it generates `<app_id>_action`, corresponding to the script name `<app_id>_action.p`.

3. Use the point name to generate the data feature string:
   - For Logging/Metric/Network/Object/... and all other categories:
     - All use the **point name** to generate the data feature string. Taking the time series metric set `cpu` as an example, it generates `cpu`, corresponding to the script `cpu.p`; for object data where the class is `HOST`, it generates `HOST`, corresponding to the script `HOST.p`.

## Pipeline Processing Examples {#examples}

Example scripts are for reference only; please write according to your specific needs.

### Processing Time Series Data {#M}

The following example shows how to modify tags and fields through Pipeline. By using DQL, we can see the fields of a CPU metric set as follows:

```shell
dql > M::cpu{host='u'} LIMIT 1
-----------------[ r1.cpu.s1 ]-----------------
core_temperature 76
             cpu 'cpu-total'
            host 'u'
            time 2022-04-25 12:32:55 +0800 CST
     usage_guest 0
usage_guest_nice 0
      usage_idle 81.399796
    usage_iowait 0.624681
       usage_irq 0
      usage_nice 1.695563
   usage_softirq 0.191229
     usage_steal 0
    usage_system 5.239674
     usage_total 18.600204
      usage_user 10.849057
---------
```

Write the following Pipeline script:

```python
# file pipeline/metric/cpu.p

set_tag(script, "metric::cpu.p")
set_tag(host2, host)
usage_guest = 100.1
```

After restarting DataKit, the newly collected data, when viewed through DQL, will show the modified CPU metric set as follows:

```shell
dql > M::cpu{host='u'}[20s] LIMIT 1
-----------------[ r1.cpu.s1 ]-----------------
core_temperature 54.250000
             cpu 'cpu-total'
            host 'u'
           host2 'u'                        <--- Newly added tag
          script 'metric::cpu.p'            <--- Newly added tag
            time 2022-05-31 12:49:15 +0800 CST
     usage_guest 100.100000                 <--- Overwritten specific field value
usage_guest_nice 0
      usage_idle 94.251269
    usage_iowait 0.012690
       usage_irq 0
      usage_nice 0
   usage_softirq 0.012690
     usage_steal 0
    usage_system 2.106599
     usage_total 5.748731
      usage_user 3.616751
---------
```

### Processing Object Data {#O}

The following Pipeline example shows how to discard (filter) data. Taking the Nginx process as an example, the current Nginx process list on the host is as follows:

```shell
$ ps axuwf | grep nginx
root        1278  0.0  0.0  55288  1496 ?        Ss   10:10   0:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;
www-data    1279  0.0  0.0  55856  5212 ?        S    10:10   0:00  \_ nginx: worker process
...
```

Through DQL, we can know that the fields of a specific process metric set are as follows:

```shell
dql > O::host_processes:(host, class, process_name, cmdline, pid) {host='u', pid=1278}
-----------------[ r1.host_processes.s1 ]-----------------
       class 'host_processes'
     cmdline 'nginx: master process /usr/sbin/nginx -g daemon on; master_process on;'
        host 'u'
         pid 1278
process_name 'nginx'
        time 2022-05-31 14:19:15 +0800 CST
---------
```

Write the following Pipeline script:

```python
if process_name == "nginx" {
    drop()  # The drop() function marks the data as to be discarded and continues to run the script after execution
    exit()  # The exit() function can be used to terminate the Pipeline run
}
```

After restarting DataKit, the corresponding NGINX process object will no longer be collected (the central object has an expiration policy, which requires waiting for 5 to 10 minutes to let the original NGINX object expire automatically).

# Best Practices for Full-Process Observability of E-commerce Orders

---

## Introduction

To meet the needs of rapid iteration and traffic surges, e-commerce systems often use microservices for development and deployment. Performance bottlenecks in a microservice can directly impact the customer shopping experience, especially when payment anomalies or order cancellations occur. We need to observe the entire order chain, track the number of successfully paid orders, abnormal orders, and canceled orders in real-time. These metrics are very helpful for analyzing business bottlenecks. This best practice is based on a distributed e-commerce platform using Java, combined with <<< custom_key.brand_name >>> to monitor the number of successfully paid orders from an order perspective and analyze the reasons for unsuccessful payments in real-time.

## Prerequisites
### Install Datakit

- [Install Datakit](/datakit/datakit-install/)
## Data Ingestion

The way to ingest order data into <<< custom_key.brand_name >>> is through logs. Microservices output log files to cloud servers, and DataKit is installed on these servers to enable log collection by specifying the path to the log files. To parse the order number, user, and order status from the log files, you need to write a Pipeline to split the log file content accordingly.

### Enable Input

1. Enable ddtrace

```shell
cd /usr/local/datakit/conf.d/ddtrace
cp ddtrace.conf.sample ddtrace.conf  
```

2. Write Pipeline

```shell
cd /usr/local/datakit/pipeline
vi log_book_order.p
```

Here **%{DATA:username}** is the user, **%{DATA:order_no}** is the order number, and **%{DATA:order_status}** is the order status.

```toml
#2021-12-22 10:09:53.443 [http-nio-7001-exec-7] INFO  c.d.s.b.s.i.OrderServiceImpl - [createOrder,164] - ecs009-book-order 7547183777837932733 2227975860088333788 test d6a3337d-ff82-4b00-9b4d-c07fb00c0cfb - User:test has placed an order, Order No: d6a3337d-ff82-4b00-9b4d-c07fb00c0cfb


grok(_, "%{TIMESTAMP_ISO8601:time} %{NOTSPACE:thread_name} %{LOGLEVEL:status}%{SPACE}%{NOTSPACE:class_name} - \\[%{NOTSPACE:method_name},%{NUMBER:line}\\] - %{DATA:service1} %{DATA:trace_id} %{DATA:span_id} %{DATA:username} %{DATA:order_no} %{DATA:order_status} - %{GREEDYDATA:msg}")

default_time(time)

```

3. Enable Logging Plugin, copy sample file

```shell
cd /usr/local/datakit/conf.d/log
cp logging.conf.sample log_book_order.conf
```
Modify the `log_book_order.conf` file, specify the log files and the Pipeline created in the previous step. Set the source to `log_book_order` for easy use in views.

```toml
[[inputs.logging]]
  ## required
  logfiles = [
    "/usr/local/df-demo/book-shop/logs/order/log.log"    
  ]

  ## glob filter
  ignore = [""]

  ## your logging source, if it's empty, use 'default'
  source = "log_book_order"

  ## add service tag, if it's empty, use $source.

  service = "book-store"

  ## grok pipeline script path
  pipeline = "log_book_order.p"

  ## optional status:
  ##   "emerg","alert","critical","error","warning","info","debug","OK"
  ignore_status = []

  ## optional encodings:
  ##    "utf-8", "utf-16le", "utf-16le", "gbk", "gb18030" or ""
  character_encoding = ""

  ## The pattern should be a regexp. Note the use of '''this regexp'''
  ## regexp link: https://golang.org/pkg/regexp/syntax/#hdr-Syntax
  multiline_match = '''^\d{4}-\d{2}-\d{2}'''

  ## removes ANSI escape codes from text strings
  remove_ansi_escape_codes = false

  [inputs.logging.tags]
    app = "book-shop"
  # some_tag = "some_value"
  # more_tag = "some_other_value"

```

4. Restart DataKit 

```shell
systemctl restart datakit
```
### Ingesting E-commerce Data

Project source code: [book-store](https://github.com/devdcores/BookStoreApp-Distributed-Application).

The logs processed by the Pipeline are generated by microservices, so the user, order number, and order status need to be output to the logs. In this example, the logging tool used is Logback. To output business data via Logback, you need to use the MDC mechanism, which involves putting the user, order number, and order status into MDC before logging, and then outputting them in the `logback-spring.xml` PATTERN. This requires modifying the `bookstore-order-service` microservice code.

1. Create a Slice

Create a slice to add `userName`, `orderNo`, and `orderStatus` to MDC, and remove them when the request ends.

```java
@Component
@Aspect
public class LogAop {
    private static final String USER_NAME = "userName";
    private static final String ORDER_NO = "orderNo";
    private static final String ORDER_STATUS = "orderStatus";

    @Pointcut("execution(public * com.devd.spring.bookstoreorderservice.controller..*.*(..))")
    public void controllerCall() {
    }

    @Before("controllerCall()")
    public void logInfoBefore(JoinPoint jp) throws UnsupportedEncodingException {
        MDC.put(USER_NAME, "");
        MDC.put(ORDER_NO, "");
        MDC.put(ORDER_STATUS, "");
    }

    @AfterReturning(returning = "req", pointcut = "controllerCall()")
    public void logInfoAfter(JoinPoint jp, Object req) throws Exception {
        MDC.remove(USER_NAME);
        MDC.remove(ORDER_NO);
        MDC.remove(ORDER_STATUS);
    }

}
```

2. Write Order Data to Logs

Output logs after a successful order placement.

![1641289006.png](../images/order-1.png)

3. Configure `logback-spring.xml`

```xml
<property name="CONSOLE_LOG_PATTERN" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{20} - [%method,%line] - %X{dd.service} %X{dd.trace_id} %X{dd.span_id} %X{userName} %X{orderNo} %X{orderStatus} - %msg%n" />
```

### Packaging and Deployment

- Frontend packaging, generates a build directory

```shell
cd bookstore-frontend-react-app
yarn build
```

- Backend packaging, generates:

`bookstore-account-service-0.0.1-SNAPSHOT.jar`,
`bookstore-payment-service-0.0.1-SNAPSHOT.jar`,
`bookstore-api-gateway-service-0.0.1-SNAPSHOT.jar`,
`bookstore-billing-service-0.0.1-SNAPSHOT.jar`,
`bookstore-catalog-service-0.0.1-SNAPSHOT.jar`,
`bookstore-eureka-discovery-service-0.0.1-SNAPSHOT.jar`,
`bookstore-order-service-0.0.1-SNAPSHOT.jar`

```shell
mvn clean install -DskipTests
```

### Enable RUM

- Log in to [<<< custom_key.brand_name >>>](https://<<< custom_key.studio_main_site >>>/)

Click on 【User Analysis】 -> 【Create】 to input `book-shop`, select Web, and copy the JS.

![image.png](../images/order-2.png)

- Copy the build directory to the server

Open `index.html`, paste the copied JS into the `<head>` section, and modify the `datakitOrigin` value to the address of the DataKit deployed on the current cloud server. The value of `allowedDDtracingOrgins` is the Gateway address.

![image.png](../images/order-3.png)

- Install Nginx, deploy the web project

```
server {
        listen       80;
        location / {            
            proxy_set_header   Host    $host:$server_port;
            proxy_set_header   X-Real-IP   $remote_addr;
            proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
			      root   /usr/local/df-demo/book-shop/build;
            index  index.html index.htm;
        }
}

```

### Enable APM

To collect Trace data, use `/usr/local/datakit/data/dd-java-agent.jar`.

```shell
java -jar bookstore-eureka-discovery-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-gateway \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-api-gateway-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-account \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-account-service-0.0.1-SNAPSHOT.jar  
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-order \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-order-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-billing \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-billing-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-payment \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-payment-service-0.0.1-SNAPSHOT.jar
 
 
java  -javaagent:/usr/local/datakit/data/dd-java-agent.jar \
 -Ddd.service.name=book-catalog \
 -Ddd.env=dev  \
 -Ddd.agent.port=9529   \
 -jar bookstore-catalog-service-0.0.1-SNAPSHOT.jar 
```

### Order Monitoring Dashboard

Log in to [<<< custom_key.brand_name >>>](https://<<< custom_key.studio_main_site >>>/), go to 【Use Cases】->【Create New Dashboard】->【Create Blank Dashboard】, input “Order Monitoring Dashboard”, and click 【Confirm】.

![1641360132(1).png](../images/order-4.png)

Click on the dashboard created in the previous step, click 【Edit】, drag a “Time Series Chart”, set the chart title to “Number of Orders Placed”. In the Time Series Chart, select “Logs”, then choose `log_book_order`. `log_book_order` is the source value from `log_book_order.conf`, then select `order_no`, and sort by “Count_distinct_by”. Filter conditions select “order_status” with the value “Placed”. This Time Series Chart will count the number of placed orders. Finally, click “+” -> Transformation Function -> cumsum, converting the order count into a cumulative sum over time.

![1642059188(1).png](../images/order-5.png)

Drag another “Time Series Chart”, set the chart title to “Number of Paid Orders”. In the Time Series Chart, select “Logs”, then choose `log_book_order`. `log_book_order` is the source value from `log_book_order.conf`, then select `order_no`, and sort by “Count_distinct_by”. Filter conditions select “order_status” with the value “Paid”. This Time Series Chart will count the number of paid orders. Finally, click “+” -> Transformation Function -> Cumsum, converting the order count into a cumulative sum over time.

![1642059204(1).png](../images/order-6.png)

Drag another “Time Series Chart”, set the chart title to “Number of Abnormal Orders”. In the Time Series Chart, select “Logs”, then choose `log_book_order`. `log_book_order` is the source value from `log_book_order.conf`, then select `order_no`, and sort by “Count_distinct_by”. Filter conditions select “order_status” with the value “Payment Anomaly”. This Time Series Chart will count the number of orders with payment anomalies. Finally, click “+” -> Transformation Function -> Cumsum, converting the order count into a cumulative sum over time.

![1642059222(1).png](../images/order-7.png)
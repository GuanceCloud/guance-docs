---
icon: zy/logs
---
# Logs
---

<video controls="controls" poster="https://static.<<< custom_key.brand_main_domain >>>/dataflux/help/video/log.png" >
      <source id="mp4" src="https://static.<<< custom_key.brand_main_domain >>>/dataflux/help/video/log.mp4" type="video/mp4">
</video>

In modern infrastructure, thousands of log events can be generated every minute. These logs follow specific formats, usually containing timestamps and are generated by servers. They are output to different files such as system logs, application logs, and security logs. However, these logs are currently stored in a distributed manner across various servers, leading to the need to log into each server separately to review logs when a system failure occurs, in order to determine the cause of the failure. This process increases the complexity of troubleshooting.

Facing such a large volume of data, you need to decide which logs should be sent to a log management solution and which should be archived. Filtering logs before sending them might result in missing critical information or inadvertently deleting valuable data.

To improve the efficiency of fault diagnosis and gain a comprehensive understanding of the system's status, avoiding passive responses during emergencies, centralized management of logs along with centralized search and correlation analysis capabilities becomes crucial.

<<< custom_key.brand_name >>>, through its powerful log collection feature, allows you to unify log data reporting to <<< custom_key.brand_name >>> workspaces. This way, you can centrally store, audit, monitor, alert, analyze, and export collected log data, thereby simplifying the log management process.

By doing this, <<< custom_key.brand_name >>> helps you avoid potential issues caused by filtering logs before sending them, ensuring all critical information is properly handled and analyzed.


## Features


<div class="grid cards" markdown>

- [:material-clipboard-text-search:{ .lg .middle } __Query & Analysis__](explorer.md)

    ---
    
    Automatically identifies log status, quickly filters and correlates logs, aggregates similar text, helping to rapidly discover and analyze anomalies, accelerating troubleshooting

- [:material-book-arrow-down-outline:{ .lg .middle } __Pipelines__](../pipeline/index.md)

    ---

    Parses the textual content of logs, converting it into structured data, including extracting timestamps, status, and specific fields as labels

- [:fontawesome-brands-atlassian:{ .lg .middle } __Generate Metrics__](generate-metrics.md)

    ---

    Generates new metric data based on existing data within the current workspace, facilitating the design and implementation of new technical metrics according to needs

- [:material-calendar-text-outline:{ .lg .middle } __Log Indexing__](./multi-index/index.md)

    ---

    Archives log data that meets certain criteria in different indexes and selects data storage policies for log indexes

- [:material-filter-multiple:{ .lg .middle } __Log Blacklist__](../management/overall-blacklist.md)  

    ---

    Customizes filtering rules for log collection; log data that meets the criteria will not be reported to <<< custom_key.brand_name >>>, helping to save on log data storage costs

- [:material-clipboard-check-multiple-outline:{ .lg .middle } __Data Forwarding__](../management/backup/index.md)
    
    ---

    Saves log, trace, and user access data to <<< custom_key.brand_name >>> object storage or forwards it to external storage, managing forwarded data flexibly

- [:material-database-check:{ .lg .middle } __Data Access__](../management/logdata-access.md)

    ---

    By setting role access permissions and data masking rules, you can more finely control access to log data while properly handling sensitive information
      
</div>
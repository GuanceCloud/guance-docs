# Preface
---

## History of Computer System Monitoring and Observability Development

​        There is an emerging concept developed in the era of cloud computing called observability, a word coming from the theoretical system of Cybernetics, which originated from the book CYBERNETICS Or Control and Communication in the Animal and the Machine published by Nobert Wiener in 1948. Observability in Cybernetics refers to the degree to which a system can infer its internal state from its external output, and observability and controllability of a system are mathematically dual concepts. Observability was first conceptualized by Rudolf Kalman, a Hungarian-born engineer, for linear dynamic systems. He believed a system is observable if all internal states can be output to the output signal in terms of a signal flow diagram.

​        This is not a new concept. Before it entered the field of computer software, we actually monitored the overall system stability in this way. In fact, there is a misunderstanding, if we want to complete the monitoring of a computer system, then the monitoring target is the data generated by the computer, and the monitoring premise is that the object is able to produce observable indicators and other data. Therefore, the value and meaning of monitoring will become less on condition that less data is observed in front of less indicators. According to the cybernetic concept of duality, this means that we are also less able to control the system. For example, if we can only monitor whether a server is normal, then we can only judge the state of this server, and completely cannot observe the state of the operating system on it. If we monitor the metrics of the operating system, then we can only judge the state of the operating system instead of the state of the applications installed on this operating system. If we have to monitor the applications, then we need to guarantee that each application itself is observable, otherwise we may only be able to determine whether the application is running from the perspective of the operating system. So monitoring is an action, and its prerequisite is that the object being monitored should have observability, and more observable data means we can better control the whole system.

​        Why do requirements for observability become higher and more urgent today? Let's look at the history of computers and the history of monitoring softwares. Most of the earliest computers were actually stand-alone computers with no network concept. At that time, there were many tools and software at the operating system level to ensure that we could know and observe the operating status of the operating system. For Windows users, the most famous one is Task Manager, while Linux also has a bunch of commands like Top and PS to help us know the running status of the operating system. Some of the applications also have the form of logs that output text to record the operation of the application in order to facilitate troubleshooting, such as event correlation in Windows and Syslog in Linux. In this period we understand the system and control the system through some functions of the operating system and the application.

​        With the progress of the times, computers entered the era of LAN. This time C/S architecture (Client/Server) appeared, and a computer in the LAN became a server; the server, as the name suggests, is the machine that serves others. Correspondingly, various business needs are achieved as client conduct data exchange with this server. Early distributed systems began to emerge, whose demand focused on high availability, because once the server was unavailable, then the client was unable to work as well. Later, it was found that requests from different clients could be handled by different servers, and as the number of connected clients increased, the complexity of the business increased, and the concept of clustering emerged. At that time, people put servers together and connected them in series through switches, routers and other network devices, and the need for monitoring changed. Zabbix, for example, uses a C/S architecture to collect observable data from the operating system and view it uniformly, including Alert based on a threshold (i.e., a value greater or less than a certain value). However, due to the fact that Zabbix was born at a time when the overall computer performance was not strong and the data collected from the system was small, it was not possible to collect more complete data, so the granularity of Zabbix collection was coarse. And for performance reasons, log data was usually transmitted to the Server side of Zabbix as a signal after the Client side (Zabbix agent) made a judgment, for example, that the log keywords contained certain content. As a result, Zabbix has gained a lot of popularity as a simple monitoring software that was open source and free, and still enjoys a large number of users today. However, the simple observable metrics collection inherent to the system and the scripting of signals were its biggest limitations. At the time Zabbix became popular, there were a number of similar types of software, some of which may have focused on networking due to differences in processing power, while others may have focused on special software, such as database-specific monitoring software, which was essentially a computer power decision. Meanwhile, Splunk quietly came out, whose purpose was to collect log data from the cluster, and eventually unified processing and unified analysis, this amount of data was very impressive. Thanks to its storage structure and algorithms, Splunk has been regarded as a database. Compared to Zabbix, it could complete the collection of data and analyze and process massive amounts of data.

​        As time marched on and technology evolved, the birth of the Internet brought about a unified client product, the browser (originally from Netscape).And a unified form of content presentation came into being, the Web, which utilized the HTML language (a type of text), of which the browser was precisely the HTML parser, to turn text into visual web pages. From the beginning, Netscape has been providing a small programming language (JavaScript) that made this static text protocol dynamic. In the process a concept called B/S, namely Brower/Server emerged. That is, the browser interacts with the server as a unified client. Thanks to the rise of the Internet and the increasing number of users, there are a large number of websites based on Web technology. At that time people created their own websites, one end of which was placed on a server, with the browser acting as a unified client end to access the services provided by the server. The initial form of websites was to connect personal computers to the Internet, which was usually only possible for universities with Internet addresses, and as the Internet continued to grow, telecom operators provided a unified way to connect servers to the Internet, namely IDC (Internet Data Center, opposed to the traditional large-scale storage of servers, connecting various offices with dedicated lines called data centers). Data centers were initially used by countries, banks and telecommunication companies to satisfy their internal business platforms, but later Internet companies joined them to provide application services to the world. Servers on the Internet became more and more numerous and systems became more complex, and some companies such as NetEase began to offer web hosting-like services that allowed users to quickly have a WebSite without having to go to an IDC to host servers (the precursor to cloud computing). At this time a great company Google appeared, applying the fast-growing WebSite with a search engine to crawl (also a client) to scan the entire Internet site and then index its content, in order to provide a way for people to quickly find the content they want to access. At the same time, a large number of Internet applications were created, from instant messaging and novel sites to Internet games. Correspondingly the complexity of the systems and the size of the server clusters behind them grew. A programmer created the search engine technology ElasticSearch to help his wife search recipes faster and set up a company for this purpose. Only that eventually the company did not become the next Google but a rival with Splunk, launching ELK solutions (ElasticSearch, Logstash, Kibana ).  It competed with Splunk for massive log data collection and management. At this moment, with the improvement of computer power and technology development, the processing of massive logs has become possible, and ELK as open source software has become a very popular choice. While Splunk as commercial software is also growing rapidly, ELK is jointly regarded as an efficient solution for log-based monitoring. Large-scale Internet applications gave rise to CDN technology, which distributes user-accessed content in a cached manner to servers in different physical locations to accelerate end-user access. Meanwhile a new way of monitoring began to emerge, which was dial-up testing, the provider of a website or Internet service. This ensures that their website or service is monitored, it is necessary to simulate themselves as a client to access their website or service to guarantee that the website is healthy and free of anomalies, but also to analyze the access acceleration of the website in different regions to be sure that the CDN is working properly.

​         In 2007, Apple released the great product iPhone, the birth of which completely changed and accelerated the development of the Internet. Google and Android came to the scene later. Along with the rapid development of wireless communication technology, mankind has rapidly entered the era of mobile Internet. The mobile Internet brought two changes. On the one hand, Apps, a unified client on the mobile operating system quietly emerged (Program refers to the PC applications and App for mobile applications) due to Steve Jobs' pursuit of the ultimate experience, coupled with the computing power of mobile devices; on the other hand, the change is even more impressive. A large number of devices around the world to access the Internet, a large number of users, a large number of visits. A large number of users, a large number of accesses. At the same time, a way to manage large numbers of servers on a large scale was invented, and Vmware was the first to introduce a virtualization solution, which started out as a solution for adapting individual computers to different operating systems and evolved into a cluster of physical servers partitioned in software to create more virtual machines to improve server utilization. In 2006, Amazon began using this technology to quietly launch an Internet service, AWS (Amazon Web Service), which initially used a large number of unused servers in Amazon's data centers as an e-commerce site to provide a large-scale hosting service for websites, a service that today is known as cloud computing. As games have become Internet games, there are Internet video sites, national and even global e-commerce, taxi, delivery and other Internet services, enterprise-class software has also started to provide services in the form of Paas, SaaS through the Internet plus browser or mobile client (App). In order to meet the various services of the Internet and the different application forms brought by mobile devices, a large number of new forms of databases, message queues, and middleware were created, including NoSQL, which is actually a series of databases to meet the needs of specific scenarios, and a single relational data no longer meets the needs of customers (the history of databases is also long, so I won't expand). With the emergence of a large number of users, each user in the process of using different Internet applications and a large amount of data, people in order to analyze and process these data, the concept of big data also came out. In recent years, with the demand of users for service enhancement and iteration, as well as the popularity of Internet push updates, the development and testing of Internet companies have become more and more agile, and the traditional way of developing an application that requires a lot of testing to release a version is becoming more and more difficult to be received, and the concept of Devops emerged. Especially in recent years, in order to further make applications more agile and easier to manage, container technology and the concept of cloud-native to adapt to container technology emerged (cloud-native is the collective name of ecological software built on the container orchestration framework Kubernetes). Back to the main topic of the article, at this time, in order to further improve application performance, people proposed a concept of APM (Application Performance Moniter), which aims to unify the execution of server-side and client-side code through data collection, not only to solve the problem of failure, but also to improve the performance of the application, such as New Relic, Dynatrace, and AppDynamic have launched the corresponding APM services. But this also brings a problem, because the original cybernetic description, a complex Internet IT services need to be observed in order to have complete control of the system, so a large number of monitoring products, from infrastructure, cloud, cloud native, database, middleware, big data, dialing, security; we need to observe the object from the original server to virtual machines to containers, we need to We need to observe more database middleware, we need to observe cloud services like AWS, even different applications deployed on different cloud vendors, the old Zabbix has been completely unable to take on the monitoring of so many objects that need to be observed, in the open source world in order to solve the problem of so many massive data observation, the emergence of time-series database based monitoring software, such as Prometheus, Telegraf+InfluxDB, APM, ZipKin, Jaeger, Pinpoint, Skywalking, etc. If we want to observe a complete Internet system, we need a large number of various forms of open source monitoring products for combined use, and in the commercial world, a unified platform DataDog emerged as a SaaS-based all-inclusive observable monitoring service, and is now the world's highest market capitalization IT monitoring management vendor, and ELK is not just a logging platform, but has also introduced features including ELK APM. New Relic, Dynatrace, and AppDynamic have also begun to position themselves as more than just single APM vendors, offering complete observable capabilities. The emergence of OpenTelemetry also signaled the industry's recognition of the need to turn system observability into a unified standard and specification, proposing the concept of the three pillars of observability, Metric, Log, Trace, with the goal of driving more applications or services to follow this specification and provide the appropriate observability capabilities.

​        This is basically a brief history of the development of computer monitoring and observability, but it may not be clear what observability is and how it differs from monitoring. In fact, observability emphasizes the need for servers, cloud services and applications themselves to actively provide the three pillars of observability in some form, for example, Prometheus proposed a concept called Exporter, calling on all middleware and applications can actively expose the Metric to the monitoring software. Then at the same time it needed to have software to support the reading of the observable data provided and then further performance analysis and monitoring. The latest open source database, Tidb, opens its own observability interface by default, while the older MySQL does not. This again shows how DataDog powerful is. On the one hand, it provides DataDog Agent, which can easily and professionally implement the observability interface for a large number of systems, so that systems that do not have observability can quickly have observability; on the other hand, it is also equipped with corresponding services that can handle large amounts of observable data, visualize these observable data and provide analysis and alerting functions. Functions of analysis and alerting are also available. The open source world is not so easy, for example, Prometheus and Telegraf have a large number of immature and even security-hazardous hobbyist-developed open source Exporter, and most software engineers would not complete the complete adaptation of observability for their own systems. So compared to traditional monitoring software or products, what can be called observability has two elements: the ability to make the object to be monitored observable (collecting its metrics, logs and code links), and the ability to store, process and analyze these massive amounts of real-time data.

​        As you can see, the development of this particular product is inextricably linked to the development of computers themselves and the development of the Internet. What will the future look like? I can only say that the human system will become more and more complex, we will face more Internet devices access (IOT technology under the Internet of Things and industrial Internet), and there will be more new cloud technology, data technology, and these devices and new technology would also be equipped with observability, and monitoring products that can monitor and manage them. I believe that monitoring and observability will continue to evolve in order to secure such a complexly built system.

​        **Guance**, an observability platform in cloud era launched by Zhuyun Technology, was created in response to historical trends and user needs.
